{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moonshot  AI Knowledge Graph Cypher Query neo4j Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline\n",
    "1. Configuration\n",
    "2. Helper Functions\n",
    "3. Prompts\n",
    "4. Running the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install openai\n",
    "# pip install neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from string import Template\n",
    "import json\n",
    "from neo4j import GraphDatabase\n",
    "import glob\n",
    "from timeit import default_timer as timer\n",
    "from dotenv import load_dotenv\n",
    "from time import sleep\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain.schema import Document\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI API configuration\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_key = os.getenv(\"api_key_azure\")\n",
    "openai.api_base = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "openai.api_version = os.getenv(\"api_version\")\n",
    "openai_deployment = \"sdgi-gpt-35-turbo-16k\"\n",
    "\n",
    "# openai.api_key = os.getenv(\"OPENAI_KEY\")\n",
    "\n",
    "\n",
    "# print(openai.api_key)\n",
    "# print(openai.api_base)\n",
    "# print(openai.api_version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neo4j configuration & constraints\n",
    "neo4j_url = os.getenv(\"NEO4J_CONNECTION_URL\")\n",
    "neo4j_user = os.getenv(\"NEO4J_USER\")\n",
    "neo4j_password = os.getenv(\"NEO4J_PASSWORD\")\n",
    "# print(f\" neo4j_url == {neo4j_url}\")\n",
    "# print(f\" neo4j_user == {neo4j_user}\")\n",
    "# print(f\" neo4j_password == {neo4j_password}\")\n",
    "\n",
    "gds = GraphDatabase.driver(neo4j_url, auth=(neo4j_user, neo4j_password))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to call the OpenAI API\n",
    "def process_gpt(file_prompt, system_msg):\n",
    "    # sleep(35)\n",
    "    completion = openai.chat.completions.create(\n",
    "                    model=openai_deployment,\n",
    "                    max_tokens=15000,\n",
    "                    temperature=0,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": system_msg},\n",
    "                        {\"role\": \"user\", \"content\": file_prompt},\n",
    "                    ]\n",
    "                )\n",
    "    nlp_results = completion.choices[0].message.content\n",
    "    # print(f\"NLP Result:::  {nlp_results}\")\n",
    "    sleep(8)\n",
    "    return nlp_results\n",
    "\n",
    "\n",
    "# Function to take folder of files and a prompt template, and return a json-object of all the entities and relationships\n",
    "def extract_entities_relationships(folder, prompt_template):\n",
    "    start = timer()\n",
    "    files = glob.glob(f\"./Data/{folder}/*\")\n",
    "    system_msg = \"You are a helpful IT-project and account management expert who extracts information from documents.\"\n",
    "    print(f\"Running pipeline for {len(files)} files in {folder} folder\")\n",
    "    results = []\n",
    "    for i, file in enumerate(files):\n",
    "        print(f\"Extracting entities and relationships for {file}\")\n",
    "        try:\n",
    "            with open(file, \"r\") as f:\n",
    "                text = f.read().rstrip()\n",
    "                prompt = Template(prompt_template).substitute(ctext=text)\n",
    "                result = process_gpt(prompt, system_msg=system_msg)\n",
    "                results.append(json.loads(result))\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file}: {e}\")\n",
    "    end = timer()\n",
    "    print(f\"Pipeline completed in {end-start} seconds\")\n",
    "    return results\n",
    "\n",
    "# Function to take a json-object of entitites and relationships and generate cypher query for creating those entities\n",
    "def generate_cypher(json_obj):\n",
    "    e_statements = []\n",
    "    r_statements = []\n",
    "\n",
    "    e_label_map = {}\n",
    "\n",
    "    # print(f\" generating cypher for {json_obj}\")\n",
    "    # loop through our json object\n",
    "    for i, obj in enumerate(json_obj):\n",
    "        print(f\"Generating cypher for file {i+1} of {len(json_obj)}\")\n",
    "        for entity in obj[\"entities\"]:\n",
    "            label = entity[\"label\"]\n",
    "            id = entity[\"id\"]\n",
    "            id = id.replace(\"-\", \"\").replace(\"_\", \"\")\n",
    "            properties = {k: v for k, v in entity.items() if k not in [\"label\", \"id\"]}\n",
    "\n",
    "            cypher = f'MERGE (n:{label} {{id: \"{id}\"}})'\n",
    "            if properties:\n",
    "                props_str = \", \".join(\n",
    "                    [f'n.{key} = \"{val}\"' for key, val in properties.items()]\n",
    "                )\n",
    "                cypher += f\" ON CREATE SET {props_str}\"\n",
    "            e_statements.append(cypher)\n",
    "            e_label_map[id] = label\n",
    "\n",
    "        for rs in obj[\"relationships\"]:\n",
    "            src_id, rs_type, tgt_id = rs.split(\"|\")\n",
    "            src_id = src_id.replace(\"-\", \"\").replace(\"_\", \"\")\n",
    "            tgt_id = tgt_id.replace(\"-\", \"\").replace(\"_\", \"\")\n",
    "\n",
    "            src_label = e_label_map[src_id]\n",
    "            tgt_label = e_label_map[tgt_id]\n",
    "\n",
    "            cypher = f'MERGE (a:{src_label} {{id: \"{src_id}\"}}) MERGE (b:{tgt_label} {{id: \"{tgt_id}\"}}) MERGE (a)-[:{rs_type}]->(b)'\n",
    "            r_statements.append(cypher)\n",
    "\n",
    "    with open(\"cyphers.txt\", \"w\") as outfile:\n",
    "        outfile.write(\"\\n\".join(e_statements + r_statements))\n",
    "\n",
    "    return e_statements + r_statements\n",
    "\n",
    "\n",
    "# Final function to bring all the steps together\n",
    "def ingestion_pipeline(folders):\n",
    "    # Extrating the entites and relationships from each folder, append into one json_object\n",
    "    entities_relationships = []\n",
    "    for key, value in folders.items():\n",
    "        entities_relationships.extend(extract_entities_relationships(key, value))\n",
    "\n",
    "    # Generate and execute cypher statements\n",
    "    cypher_statements = generate_cypher(entities_relationships)\n",
    "    for i, stmt in enumerate(cypher_statements):\n",
    "        print(f\"Executing cypher statement {i+1} of {len(cypher_statements)}\")\n",
    "        try:\n",
    "            gds.execute_query(stmt)\n",
    "        except Exception as e:\n",
    "            with open(\"failed_statements.txt\", \"w\") as f:\n",
    "                f.write(f\"{stmt} - Exception: {e}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Defining Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt_template = \"\"\"\n",
    "From the Brief below, extract the following Entities & relationships described in the mentioned format \n",
    "0. ALWAYS FINISH THE OUTPUT. Never send partial responses\n",
    "1. First, look for  Entity types in the text and generate as comma-separated format similar to entity type.\n",
    "   `id` property of each entity must be alphanumeric and must be unique among the entities. You will be referring this property to define the relationship between entities. Do not create new entity types that aren't mentioned below. Document must be summarized and stored inside Country entity under `summary` property. You will have to generate as many entities as needed as per the types below:\n",
    "    Entity Types:\n",
    "    label:'Entity',id:string,name:string;summary:string \n",
    "2. Next generate each relationships as triples of head, relationship and tail. To refer the head and tail entity, use their respective `id` property. \n",
    "   Relationship property should be mentioned within brackets as comma-separated. \n",
    "   You will have to generate as many relationships as needed as defined below:\n",
    "    Relationship types:\n",
    "    Entity|RELATIONSHIP_TYPE|Entity \n",
    "3. The output should look like :\n",
    "{\n",
    "    \"entities\": [{\"label\":\"Entity\",\"id\":string,\"name\":string,\"summary\":string}],\n",
    "    \"relationships\": [\"Entityid|RELATIONSHIP_TYPE|AnotherEntityid\"]\n",
    "}\n",
    "Entity, RELATIONSHIP_TYPE and AnotherEntityid are to be generate by you based on the brief.\n",
    "Case Sheet:\n",
    "$ctext\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Running the pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running pipeline for 1 files in cleaned_text_manually folder\n",
      "Extracting entities and relationships for ./Data/cleaned_text_manually/AFG-CPD-2014-EN.txt\n",
      "Error processing ./Data/cleaned_text_manually/AFG-CPD-2014-EN.txt: Error code: 400 - {'error': {'message': \"This model's maximum context length is 16384 tokens. However, you requested 23962 tokens (8962 in the messages, 15000 in the completion). Please reduce the length of the messages or completion.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "Pipeline completed in 1.1828097130000002 seconds\n"
     ]
    }
   ],
   "source": [
    "countries = {\n",
    "    \"cleaned_text_manually\": prompt_template,\n",
    "}\n",
    "\n",
    "ingestion_pipeline(countries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the knowledge graph in a RAG application\n",
    "from langchain.chains import GraphCypherQAChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.graphs import Neo4jGraph\n",
    "import os\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] =  os.getenv(\"OPENAI_KEY\")\n",
    " \n",
    "\n",
    "graph = Neo4jGraph(\n",
    "    url=neo4j_url,\n",
    "    username=neo4j_user,\n",
    "    password=neo4j_password\n",
    ")\n",
    "\n",
    "graph.refresh_schema()\n",
    "\n",
    "cypher_chain = GraphCypherQAChain.from_llm(\n",
    "    graph=graph,\n",
    "    cypher_llm=ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\"),\n",
    "    qa_llm=ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-16k\"),\n",
    "    validate_cypher=True, # Validate relationship directions\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (e1:Entity)-[r:TRANSFORMS]->(e2:Entity)\n",
      "WHERE e2.name = \"Transformation Decade\"\n",
      "RETURN e1.summary, r.summary\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, but I don't have any information on the Transformation Decade or what caused it to start.\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cypher_chain.run(\"When did the Transformation Decade start and what even caused it to start?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### . Token limit each file  (optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/davidoluyalegbenga/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "nltk.download('punkt')  # Download NLTK tokenizer data if not already downloaded\n",
    "\n",
    "# Function to split file into subfiles based on sentences\n",
    "def split_file_sentences(filename, sentence_limit=5):\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "        sentences = nltk.sent_tokenize(content)  # Tokenize into sentences\n",
    "        \n",
    "        num_sentences = len(sentences)\n",
    "        num_files = (num_sentences // sentence_limit) + 1\n",
    "        \n",
    "        for i in range(num_files):\n",
    "            start = i * sentence_limit\n",
    "            end = (i + 1) * sentence_limit\n",
    "            sub_sentences = sentences[start:end]  # Extract sentences for subfile\n",
    "            \n",
    "            # Create subfile name (e.g., original_filename_part1.txt, original_filename_part2.txt, ...)\n",
    "            subfile_name = f\"{os.path.splitext(filename)[0]}_part{i + 1}.txt\"\n",
    "            \n",
    "            with open(subfile_name, 'w', encoding='utf-8') as subfile:\n",
    "                subfile.write('\\n'.join(sub_sentences))  # Write sentences to subfile\n",
    "        \n",
    "        os.remove(filename)  # Remove the original file after splitting\n",
    "\n",
    "# Directory containing the .txt files\n",
    "directory = 'Data/cleaned_text_manually'\n",
    "\n",
    "# Loop through each file in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        split_file_sentences(file_path)\n",
    "\n",
    "\n",
    "\n",
    "# import os\n",
    "# import nltk\n",
    "# from nltk.tokenize import word_tokenize\n",
    "\n",
    "# nltk.download('punkt')  # Download NLTK tokenizer data if not already downloaded\n",
    "\n",
    "# # Function to read a file, tokenize its content, and split it into subfiles\n",
    "# def split_file(filename, token_limit=500):\n",
    "#     with open(filename, 'r', encoding='utf-8') as file:\n",
    "#         content = file.read()\n",
    "#         tokens = word_tokenize(content)  # Tokenize the content\n",
    "        \n",
    "#         num_tokens = len(tokens)\n",
    "#         num_files = (num_tokens // token_limit) + 1\n",
    "        \n",
    "#         for i in range(num_files):\n",
    "#             start = i * token_limit\n",
    "#             end = (i + 1) * token_limit\n",
    "#             sub_tokens = tokens[start:end]  # Extract tokens for subfile\n",
    "            \n",
    "#             # Create subfile name (e.g., original_filename_part1.txt, original_filename_part2.txt, ...)\n",
    "#             subfile_name = f\"{os.path.splitext(filename)[0]}_part{i + 1}.txt\"\n",
    "            \n",
    "#             with open(subfile_name, 'w', encoding='utf-8') as subfile:\n",
    "#                 subfile.write(' '.join(sub_tokens))  # Write tokens to subfile\n",
    "        \n",
    "#         os.remove(filename)  # Remove the original file after splitting\n",
    "\n",
    "# # Directory containing the .txt files\n",
    "# directory = 'Data/cleaned_text_manually'\n",
    "\n",
    "# # Loop through each file in the directory\n",
    "# for filename in os.listdir(directory):\n",
    "#     if filename.endswith(\".txt\"):\n",
    "#         file_path = os.path.join(directory, filename)\n",
    "#         split_file(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 517 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting joblib\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "\u001b[K     |████████████████████████████████| 302 kB 3.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: regex>=2021.8.3 in /Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages (from nltk) (4.66.1)\n",
      "Collecting click\n",
      "  Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 2.2 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: joblib, click, nltk\n",
      "Successfully installed click-8.1.7 joblib-1.3.2 nltk-3.8.1\n",
      "\u001b[33mWARNING: You are using pip version 21.1.3; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the '/Users/davidoluyalegbenga/.pyenv/versions/3.9.6/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
