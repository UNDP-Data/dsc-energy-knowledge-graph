{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "vM5VDCJyAWnC",
      "metadata": {
        "id": "vM5VDCJyAWnC"
      },
      "source": [
        "# Improve ChatGPT with Knowledge Graphs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "uWz0SMuX5FFp",
      "metadata": {
        "id": "uWz0SMuX5FFp"
      },
      "outputs": [],
      "source": [
        "# pip install -q openai langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "c4a39cc6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# %reset -f\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "66162a41",
      "metadata": {},
      "outputs": [],
      "source": [
        "# pip install python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "829cb1c9",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "from string import Template\n",
        "import json\n",
        "from neo4j import GraphDatabase\n",
        "import glob\n",
        "from timeit import default_timer as timer\n",
        "from dotenv import load_dotenv\n",
        "from time import sleep\n",
        "from langchain.text_splitter import TokenTextSplitter\n",
        "from langchain.schema import Document\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "beb23ada",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "XpvHDkzNnFLj",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpvHDkzNnFLj",
        "outputId": "3d634edc-dbba-4294-8dcc-2ea998e7928e"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "\n",
        "os.environ[\"OPENAI_API_TYPE\"] = \"azure\"\n",
        "os.environ[\"OPENAI_API_VERSION\"] = os.getenv(\"api_version\")\n",
        "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
        "os.environ[\"AZURE_OPENAI_API_KEY\"] = os.getenv(\"api_key_azure\")\n",
        "\n",
        "# os.environ['OPENAI_API_KEY'] = os.getenv(\"OPENAI_KEY\")\n",
        "# openai.api_key = os.getenv(\"api_key_azure\")\n",
        "# openai.api_version = os.getenv(\"api_version\")\n",
        "openai_deployment = \"sdgi-gpt-35-turbo-16k\" \n",
        "# print(os.getenv(\"api_key_azure\"))\n",
        "# print(\"=====\")\n",
        "# print(os.getenv(\"api_version\"))\n",
        "# completion = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\",\n",
        "#                                           temperature=0,\n",
        "#                                           messages=[{\"role\": \"user\",\n",
        "#                                                      \"content\": question}])\n",
        "# print(completion[\"choices\"][0][\"message\"][\"content\"])\n",
        "\n",
        "# completion = openai.chat.completions.create(\n",
        "#                     model=openai_deployment,\n",
        "#                     max_tokens=15000,\n",
        "#                     temperature=0,\n",
        "#                     messages=[\n",
        "#                         {\"role\": \"user\", \"content\": question}\n",
        "#                     ]\n",
        "#                 )\n",
        "# nlp_results = completion.choices[0].message.content\n",
        "# print(nlp_results)\n",
        "\n",
        "# print(os.environ['OPENAI_API_BASE'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88a9f2e3-c729-455a-a338-2f83776c1d4c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88a9f2e3-c729-455a-a338-2f83776c1d4c",
        "outputId": "f6c9f4d8-387b-4ffc-e200-0c35ed449d8a"
      },
      "outputs": [],
      "source": [
        "# from langchain.llms import OpenAI\n",
        "from langchain.indexes import GraphIndexCreator\n",
        "from langchain.chains import GraphQAChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "# from langchain.llms import AzureChatOpenAI\n",
        "from langchain.chat_models import AzureChatOpenAI\n",
        "import networkx as nx\n",
        "import concurrent.futures\n",
        "\n",
        "def custom_sentence_tokenize(text, max_words=250):\n",
        "    words = text.split()\n",
        "    sentences = []\n",
        "    current_sentence = []\n",
        "\n",
        "    for word in words:\n",
        "        current_sentence.append(word)\n",
        "        if len(current_sentence) >= max_words:\n",
        "            sentences.append(' '.join(current_sentence))\n",
        "            current_sentence = []\n",
        "\n",
        "    if current_sentence:\n",
        "        sentences.append(' '.join(current_sentence))\n",
        "\n",
        "    return sentences\n",
        "\n",
        " \n",
        "def createGML(text, filename):\n",
        "     \n",
        "    sentences = custom_sentence_tokenize(text)\n",
        "    print(f\"********* {len(sentences)}\")\n",
        "\n",
        "    try:\n",
        "        for idx, sentence in enumerate(sentences):\n",
        "            # Create a graph for each sentence\n",
        "            index_creator = GraphIndexCreator(llm=AzureChatOpenAI(deployment_name=openai_deployment, temperature=0))\n",
        "            graph = index_creator.from_text(sentence)\n",
        "            # print(f\" filename=== {filename} and idx=== {idx} \")\n",
        "            # Write each graph to a GML file with a unique filename\n",
        "            filenamex = f\"Models/GML/{filename}_{idx + 1}.gml\"  # Unique filename based on index\n",
        "            graph.write_to_gml(filenamex)\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating GML file {e}\")    \n",
        "\n",
        "\n",
        "def process_file(file):\n",
        "    try:\n",
        "        with open(file, \"r\") as f:\n",
        "            text = f.read().rstrip()\n",
        "            filename = file.split(\"/\")[-1].replace(\".txt\", \"\")\n",
        "            createGML(text, filename)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {file}: {e}\")\n",
        "\n",
        "def process_files_concurrently(files):\n",
        "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "        executor.map(process_file, files)\n",
        "\n",
        "folder=\"./Data/cleaned_text_manually/\"\n",
        "files = glob.glob(f\"{folder}*\")\n",
        "\n",
        "process_files_concurrently(files)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# print(f\"Running pipeline for {len(files)} files in /Data/cleaned_text_manually/ folder\")\n",
        "\n",
        "# for i, file in enumerate(files):\n",
        "#         # print(f\"Extracting entities and relationships for {file}\")\n",
        "#         try:\n",
        "#             with open(file, \"r\") as f:\n",
        "#                  text = f.read().rstrip()\n",
        "#                 #  filename = file.split(\"/\")[-1] \n",
        "#                  filename = file.split(\"/\")[-1].replace(\".txt\", \"\")  # Extracting just the filename without the path and extension\n",
        "#                  createGML(text, filename)\n",
        "#         except Exception as e:\n",
        "#             print(f\"Error processing {file}: {e}\")\n",
        "\n",
        "\n",
        "# # # Create a dictionary to represent the knowledge graph\n",
        "# graph_data = {\n",
        "#     \"nodes\": list(set(triplet[0] for triplet in triples + [triplet[1] for triplet in triples])),\n",
        "#     \"edges\": [{\"source\": triplet[0], \"target\": triplet[1], \"label\": triplet[2]} for triplet in triples]\n",
        "# }\n",
        "\n",
        "# # Convert graph data to JSON format\n",
        "# graph_json = json.dumps(graph_data, indent=4)\n",
        "\n",
        "# # Display or use the JSON data as needed\n",
        "# print(graph_json)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "887a3e86",
      "metadata": {},
      "outputs": [],
      "source": [
        "folder=\"./Models/GML/\"\n",
        "\n",
        "#comibine them\n",
        "# Get all .gml files in the folder\n",
        "filesGml = glob.glob(f\"{folder}*.gml\")\n",
        "\n",
        "# Initialize an empty list to hold graphs\n",
        "graphs = []\n",
        "print(f\"Combine graphs {filesGml}\")\n",
        "# Read each .gml file and append the graphs to the list\n",
        "for file in filesGml:\n",
        "    print(f\"file=== {file}\")\n",
        "    G = nx.read_gml(file)\n",
        "    graphs.append(G)\n",
        "# Combine all the graphs\n",
        "combined_graph = nx.compose_all(graphs)\n",
        "nx.write_gml(combined_graph, 'combined_graph_model_v1.gml')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed06c43d",
      "metadata": {},
      "source": [
        "### Save Graph \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "0388bb7f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# graph.write_to_gml(\"Models/GML/AFG-CPD-2014-EN.gml\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6888973",
      "metadata": {},
      "source": [
        "### Use Saved Graph\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "id": "KZIs4N5S8e8S",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "id": "KZIs4N5S8e8S",
        "outputId": "af6ba50c-e5f6-40be-adaf-07911dab948f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new GraphQAChain chain...\u001b[0m\n",
            "Entities Extracted:\n",
            "\u001b[32;1m\u001b[1;3mParis Agreement\u001b[0m\n",
            "Full Context:\n",
            "\u001b[32;1m\u001b[1;3m\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"I don't know.\""
            ]
          },
          "execution_count": 136,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.indexes.graph import NetworkxEntityGraph\n",
        "loaded_graph = NetworkxEntityGraph.from_gml(\"moonshot_AI_graph_model_v1.gml\")\n",
        "\n",
        "prompt = \"Use the following knowledge triplets to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\n{context}\\n\\nQuestion: {question}\\nHelpful Answer:\"\n",
        "prompt_entity=\"Extract all entities from the following text. As a guideline, a proper noun is generally capitalized. You should definitely extract all names,places, Dates and Times, Numbers, Organizations, Products and Brands, Events, Roles and Positions, Keywords and Topics, Email Addresses and URLs, References to External Entities, Emotional Tone, Quantities and Units, Codes and Identifiers, Languages, Social Media Handles, Currencies..\\n\\nReturn the output as a single comma-separated list, or NONE if there is nothing of note to return.\\n\\nEXAMPLE\\ni'm trying to improve Langchain's interfaces, the UX, its integrations with various products the user might want ... a lot of stuff.\\nOutput: Langchain\\nEND OF EXAMPLE\\n\\nEXAMPLE\\ni'm trying to improve Langchain's interfaces, the UX, its integrations with various products the user might want ... a lot of stuff. I'm working with Sam.\\nOutput: Langchain, Sam\\nEND OF EXAMPLE\\n\\nBegin!\\n\\n{input}\\nOutput:\"\n",
        "question = f\"\"\"\n",
        "What is the annual capacity addition of renewable energy needed to achieve the targets set in the Paris Agreement?\n",
        "\"\"\"\n",
        "chain = GraphQAChain.from_llm(AzureChatOpenAI(temperature=0, deployment_name= openai_deployment), graph=loaded_graph, verbose=True,\n",
        "qa_prompt=PromptTemplate(input_variables=['context', 'question'], template=prompt),\n",
        "entity_prompt=PromptTemplate(input_variables=['input'], template=prompt_entity)\n",
        ")\n",
        "chain.run(question) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c095dcd",
      "metadata": {},
      "source": [
        "### Merge Saved Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "436bd154",
      "metadata": {},
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "\n",
        "# Read the first GML file\n",
        "G1 = nx.read_gml('moonshot1.gml')\n",
        "\n",
        "# Read the second GML file\n",
        "G2 = nx.read_gml('moonshot2.gml')\n",
        "\n",
        "# Read the third GML file\n",
        "G3 = nx.read_gml('moonshot3.gml')\n",
        "\n",
        "# Read the third GML file\n",
        "G4 = nx.read_gml('moonshot3.gml')\n",
        "\n",
        "combined_graph = nx.compose_all([G1, G2, G3, G4])\n",
        "\n",
        "# Combine the graphs\n",
        "# combined_graph = nx.compose(G3, G4)\n",
        "\n",
        "# Write the combined graph to a new GML file\n",
        "nx.write_gml(combined_graph, 'combined_graph.gml')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
