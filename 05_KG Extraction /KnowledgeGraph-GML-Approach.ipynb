{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "vM5VDCJyAWnC",
      "metadata": {
        "id": "vM5VDCJyAWnC"
      },
      "source": [
        "# Improve ChatGPT with Knowledge Graphs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "uWz0SMuX5FFp",
      "metadata": {
        "id": "uWz0SMuX5FFp"
      },
      "outputs": [],
      "source": [
        "# pip install -q openai langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "c4a39cc6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# %reset -f\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "66162a41",
      "metadata": {},
      "outputs": [],
      "source": [
        "# pip install python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "829cb1c9",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "from string import Template\n",
        "import json\n",
        "from neo4j import GraphDatabase\n",
        "import glob\n",
        "from timeit import default_timer as timer\n",
        "from dotenv import load_dotenv\n",
        "from time import sleep\n",
        "from langchain.text_splitter import TokenTextSplitter\n",
        "from langchain.schema import Document\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "beb23ada",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "XpvHDkzNnFLj",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpvHDkzNnFLj",
        "outputId": "3d634edc-dbba-4294-8dcc-2ea998e7928e"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "\n",
        "os.environ[\"OPENAI_API_TYPE\"] = \"azure\"\n",
        "os.environ[\"OPENAI_API_VERSION\"] = os.getenv(\"api_version\")\n",
        "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
        "os.environ[\"AZURE_OPENAI_API_KEY\"] = os.getenv(\"api_key_azure\")\n",
        "\n",
        "# os.environ['OPENAI_API_KEY'] = os.getenv(\"OPENAI_KEY\")\n",
        "# openai.api_key = os.getenv(\"api_key_azure\")\n",
        "# openai.api_version = os.getenv(\"api_version\")\n",
        "openai_deployment = \"sdgi-gpt-35-turbo-16k\" \n",
        "# print(os.getenv(\"api_key_azure\"))\n",
        "# print(\"=====\")\n",
        "# print(os.getenv(\"api_version\"))\n",
        "# completion = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\",\n",
        "#                                           temperature=0,\n",
        "#                                           messages=[{\"role\": \"user\",\n",
        "#                                                      \"content\": question}])\n",
        "# print(completion[\"choices\"][0][\"message\"][\"content\"])\n",
        "\n",
        "# completion = openai.chat.completions.create(\n",
        "#                     model=openai_deployment,\n",
        "#                     max_tokens=15000,\n",
        "#                     temperature=0,\n",
        "#                     messages=[\n",
        "#                         {\"role\": \"user\", \"content\": question}\n",
        "#                     ]\n",
        "#                 )\n",
        "# nlp_results = completion.choices[0].message.content\n",
        "# print(nlp_results)\n",
        "\n",
        "# print(os.environ['OPENAI_API_BASE'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "88a9f2e3-c729-455a-a338-2f83776c1d4c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88a9f2e3-c729-455a-a338-2f83776c1d4c",
        "outputId": "f6c9f4d8-387b-4ffc-e200-0c35ed449d8a"
      },
      "outputs": [],
      "source": [
        "# from langchain.llms import OpenAI\n",
        "from langchain.indexes import GraphIndexCreator\n",
        "from langchain.chains import GraphQAChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "# from langchain.llms import AzureChatOpenAI\n",
        "from langchain.chat_models import AzureChatOpenAI\n",
        "import networkx as nx\n",
        "import concurrent.futures\n",
        "from tqdm import tqdm  # Import tqdm for the progress bar\n",
        "\n",
        "\n",
        "def custom_sentence_tokenize(text, max_words=250):\n",
        "    words = text.split()\n",
        "    sentences = []\n",
        "    current_sentence = []\n",
        "\n",
        "    for word in words:\n",
        "        current_sentence.append(word)\n",
        "        if len(current_sentence) >= max_words:\n",
        "            sentences.append(' '.join(current_sentence))\n",
        "            current_sentence = []\n",
        "\n",
        "    if current_sentence:\n",
        "        sentences.append(' '.join(current_sentence))\n",
        "\n",
        "    return sentences\n",
        "\n",
        " \n",
        "def createGML(text, filename):\n",
        "     \n",
        "    sentences = custom_sentence_tokenize(text)\n",
        "    print(f\"********* {len(sentences)}\")\n",
        "    sleep(8)\n",
        "    try:\n",
        "        for idx, sentence in enumerate(sentences):\n",
        "            # Create a graph for each sentence\n",
        "            prompt_extractor= \"You are a networked intelligence helping a human track knowledge triples about all relevant people, things, concepts, names,places, Dates and Times, Numbers, Organizations, Products and Brands, Events, Roles and Positions, Keywords and Topics, Email Addresses and URLs, References to External Entities, Emotional Tone, Quantities and Units, Codes and Identifiers, Languages, Social Media Handles, Currencies etc. and integrating them with your knowledge stored within your weights as well as that stored in a knowledge graph. Extract all of the knowledge triples from the text. A knowledge triple is a clause that contains a subject, a predicate, and an object. The subject is the entity being described, the predicate is the property of the subject that is being described, and the object is the value of the property.\\n\\nEXAMPLE\\nIt's a state in the US. It's also the number 1 producer of gold in the US.\\n\\nOutput: (Nevada, is a, state)<|>(Nevada, is in, US)<|>(Nevada, is the number 1 producer of, gold)\\nEND OF EXAMPLE\\n\\nEXAMPLE\\nI'm going to the store.\\n\\nOutput: NONE\\nEND OF EXAMPLE\\n\\nEXAMPLE\\nOh huh. I know Descartes likes to drive antique scooters and play the mandolin.\\nOutput: (Descartes, likes to drive, antique scooters)<|>(Descartes, plays, mandolin)\\nEND OF EXAMPLE\\n\\nEXAMPLE\\n{text}Output:\"\n",
        "            index_creator = GraphIndexCreator(llm=AzureChatOpenAI(deployment_name=openai_deployment, temperature=0))\n",
        "            graph = index_creator.from_text(sentence, prompt=PromptTemplate(input_variables=['text'], template=prompt_extractor))\n",
        "            # print(f\" filename=== {filename} and idx=== {idx} \")\n",
        "            # Write each graph to a GML file with a unique filename\n",
        "            filenamex = f\"Models/GML/{filename}_{idx + 1}.gml\"  # Unique filename based on index\n",
        "            graph.write_to_gml(filenamex)\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating GML file {e}\")    \n",
        "\n",
        "\n",
        "def process_file(file):\n",
        "    try:\n",
        "        with open(file, \"r\") as f:\n",
        "            text = f.read().rstrip()\n",
        "            filename = file.split(\"/\")[-1].replace(\".txt\", \"\")\n",
        "            createGML(text, filename)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {file}: {e}\")\n",
        "\n",
        "# def process_files_concurrently(files):\n",
        "#     with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "#         executor.map(process_file, files)\n",
        "\n",
        "\n",
        "def process_files_concurrently(files):\n",
        "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "        # Use tqdm to create a progress bar\n",
        "        with tqdm(total=len(files)) as pbar:\n",
        "            # Define a function to update the progress bar after each task completion\n",
        "            def update(*_):\n",
        "                pbar.update()\n",
        "            \n",
        "            # Submit tasks to the ThreadPoolExecutor\n",
        "            futures = [executor.submit(process_file, file) for file in files]\n",
        "            \n",
        "            # Add the update function to the completion of each future\n",
        "            for future in concurrent.futures.as_completed(futures):\n",
        "                future.add_done_callback(update)\n",
        "\n",
        "folder=\"./Data/cleaned_text_manually/\"\n",
        "files = glob.glob(f\"{folder}*\")\n",
        "\n",
        "# process_files_concurrently(files)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# print(f\"Running pipeline for {len(files)} files in /Data/cleaned_text_manually/ folder\")\n",
        "\n",
        "# for i, file in enumerate(files):\n",
        "#         # print(f\"Extracting entities and relationships for {file}\")\n",
        "#         try:\n",
        "#             with open(file, \"r\") as f:\n",
        "#                  text = f.read().rstrip()\n",
        "#                 #  filename = file.split(\"/\")[-1] \n",
        "#                  filename = file.split(\"/\")[-1].replace(\".txt\", \"\")  # Extracting just the filename without the path and extension\n",
        "#                  createGML(text, filename)\n",
        "#         except Exception as e:\n",
        "#             print(f\"Error processing {file}: {e}\")\n",
        "\n",
        "\n",
        "# # # Create a dictionary to represent the knowledge graph\n",
        "# graph_data = {\n",
        "#     \"nodes\": list(set(triplet[0] for triplet in triples + [triplet[1] for triplet in triples])),\n",
        "#     \"edges\": [{\"source\": triplet[0], \"target\": triplet[1], \"label\": triplet[2]} for triplet in triples]\n",
        "# }\n",
        "\n",
        "# # Convert graph data to JSON format\n",
        "# graph_json = json.dumps(graph_data, indent=4)\n",
        "\n",
        "# # Display or use the JSON data as needed\n",
        "# print(graph_json)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "887a3e86",
      "metadata": {},
      "outputs": [],
      "source": [
        "folder=\"./Models/GML/\"\n",
        "\n",
        "#comibine them\n",
        "# Get all .gml files in the folder\n",
        "filesGml = glob.glob(f\"{folder}*.gml\")\n",
        "\n",
        "# Initialize an empty list to hold graphs\n",
        "graphs = []\n",
        "print(f\"Combine graphs {filesGml}\")\n",
        "# Read each .gml file and append the graphs to the list\n",
        "for file in filesGml:\n",
        "    print(f\"file=== {file}\")\n",
        "    G = nx.read_gml(file)\n",
        "    graphs.append(G)\n",
        "# Combine all the graphs\n",
        "combined_graph = nx.compose_all(graphs)\n",
        "# nx.write_gml(combined_graph, 'combined_graph_model_v.gml')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed06c43d",
      "metadata": {},
      "source": [
        "### Save Graph \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "0388bb7f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# graph.write_to_gml(\"Models/GML/AFG-CPD-2014-EN.gml\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6888973",
      "metadata": {},
      "source": [
        "### Use Saved Graph\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "KZIs4N5S8e8S",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "id": "KZIs4N5S8e8S",
        "outputId": "af6ba50c-e5f6-40be-adaf-07911dab948f"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Based on the provided knowledge base, here is the information related to Afghanistan's energy sector from 2014 to 2022:\n",
              "\n",
              "1. Total Energy Supply Growth:\n",
              "- Total energy supply in 2014: 157,182 TJ\n",
              "- Total energy supply in 2019: 187,519 TJ\n",
              "- Growth in total energy supply from 2014 to 2019: +19.3%\n",
              "\n",
              "2. Renewable Energy Share:\n",
              "- Renewable energy share in 2014: 20%\n",
              "- Renewable energy share in 2019: 19%\n",
              "\n",
              "3. Utilization of Renewable Energy:\n",
              "- Afghanistan utilizes solar resources.\n",
              "- Afghanistan utilizes wind resources.\n",
              "- Afghanistan utilizes hydro resources.\n",
              "\n",
              "4. Energy Self-Sufficiency:\n",
              "- Energy self-sufficiency in 2014: 41%\n",
              "- Energy self-sufficiency in 2019: 43%\n",
              "\n",
              "5. Trade Dynamics:\n",
              "- Primary energy imports in 2014: 103,880 TJ\n",
              "- Primary energy imports in 2019: 136,405 TJ\n",
              "- Primary energy exports in 2014: 8,172 TJ\n",
              "- Primary energy exports in 2019: 21,807 TJ\n",
              "- Net primary energy trade in 2014: -95,708 TJ\n",
              "- Net primary energy trade in 2019: -114,598 TJ\n",
              "- Imports as a percentage of total energy supply: 66%\n",
              "- Exports as a percentage of primary energy production: 13%\n",
              "\n",
              "6. Alignment with Sustainable Development Goals:\n",
              "- Access to electricity indicator in 2019: 75.6%\n",
              "- Access to clean cooking indicator in 2019: 74.3%\n",
              "- Renewable energy indicator in 2019: 74.5%\n",
              "\n",
              "Please note that the knowledge base does not provide specific data for each year from 2014 to 2022. The available information is limited to 2014 and 2019."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from langchain.indexes.graph import NetworkxEntityGraph\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "loaded_graph = NetworkxEntityGraph.from_gml(\"moonshot_AI_graph_model_v1.gml\")\n",
        "\n",
        "# prompt =  \"Ignore previous output. Use the following knowledge triplets  to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\n{context}\\n\\nQuestion: {question}\\nHelpful Answer:\"\n",
        "\n",
        "prompt =  \"Use the following knowledge triplets to answer the question at the end. If you don't know the answer, look out for potential factors in the knowledge triplets else just say I don't know based on my knowledge base, don't try to make up an answer. If a term like a Continent is used e.g Africa, Asia, replace the continent with all african countries available in the knowledge triplets. E.g Nigeria, South Africa and Egypt are under Africa. In your answer, Always refer to knowledge triplets as knowledge base.\\n\\n{context}\\n\\nQuestion: {question}\\nHelpful Answer:\"\n",
        "prompt_entity=\"Extract all entities from the following text. As a guideline, a proper noun is generally capitalized. You should definitely extract all names,places, Dates and Times, Numbers, Organizations, Products and Brands, Events, Roles and Positions, Keywords and Topics, Email Addresses and URLs, References to External Entities, Emotional Tone, Quantities and Units, Codes and Identifiers, Languages, Social Media Handles, Currencies..\\n\\nReturn the output as a single comma-separated list, or NONE if there is nothing of note to return.\\n\\nEXAMPLE\\ni'm trying to improve Langchain's interfaces, the UX, its integrations with various products the user might want ... a lot of stuff.\\nOutput: Langchain\\nEND OF EXAMPLE\\n\\nEXAMPLE\\ni'm trying to improve Langchain's interfaces, the UX, its integrations with various products the user might want ... a lot of stuff. I'm working with Sam.\\nOutput: Langchain, Sam\\nEND OF EXAMPLE\\n\\nBegin!\\n\\n{input}\\nOutput:\"\n",
        "question = f\"\"\"\n",
        "I'm looking for Afghanistan's energy sector information from 2014 to 2022. Specifically in the country's Total Energy Supply growth over the years and how the renewable energy share has changed? Levels of renewable energy utilization like solar, hydro/marine, and wind? Additionally, I'd like to know data on energy self-sufficiency and trade dynamics, such as imports and exports? Could you provide data or observations on how the energy sector aligns with Afghanistan's Sustainable Development Goals, particularly in terms of access to electricity access, clean cooking?\n",
        "\"\"\"\n",
        "chain = GraphQAChain.from_llm(AzureChatOpenAI(temperature=0, deployment_name= openai_deployment), graph=loaded_graph, verbose=False,\n",
        "qa_prompt=PromptTemplate(input_variables=['context', 'question'], template=prompt),\n",
        "entity_prompt=PromptTemplate(input_variables=['input'], template=prompt_entity)\n",
        ")\n",
        "response = chain.run(question) \n",
        "\n",
        "\n",
        "# IPython.display.HTML(response)\n",
        "display(Markdown(response))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
