{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "vM5VDCJyAWnC",
      "metadata": {
        "id": "vM5VDCJyAWnC"
      },
      "source": [
        "# Improve ChatGPT with Knowledge Graphs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "uWz0SMuX5FFp",
      "metadata": {
        "id": "uWz0SMuX5FFp"
      },
      "outputs": [],
      "source": [
        "# pip install -q openai langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "66162a41",
      "metadata": {},
      "outputs": [],
      "source": [
        "# pip install python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "829cb1c9",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "from string import Template\n",
        "import json\n",
        "from neo4j import GraphDatabase\n",
        "import glob\n",
        "from timeit import default_timer as timer\n",
        "from dotenv import load_dotenv\n",
        "from time import sleep\n",
        "from langchain.text_splitter import TokenTextSplitter\n",
        "from langchain.schema import Document\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "beb23ada",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "XpvHDkzNnFLj",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpvHDkzNnFLj",
        "outputId": "3d634edc-dbba-4294-8dcc-2ea998e7928e"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "\n",
        "os.environ[\"OPENAI_API_TYPE\"] = \"azure\"\n",
        "os.environ[\"OPENAI_API_VERSION\"] = os.getenv(\"api_version\")\n",
        "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
        "os.environ[\"AZURE_OPENAI_API_KEY\"] = os.getenv(\"api_key_azure\")\n",
        "\n",
        "# os.environ['OPENAI_API_KEY'] = os.getenv(\"OPENAI_KEY\")\n",
        "# openai.api_key = os.getenv(\"api_key_azure\")\n",
        "# openai.api_version = os.getenv(\"api_version\")\n",
        "openai_deployment = \"sdgi-gpt-35-turbo-16k\" \n",
        "# print(os.getenv(\"api_key_azure\"))\n",
        "# print(\"=====\")\n",
        "# print(os.getenv(\"api_version\"))\n",
        "# completion = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\",\n",
        "#                                           temperature=0,\n",
        "#                                           messages=[{\"role\": \"user\",\n",
        "#                                                      \"content\": question}])\n",
        "# print(completion[\"choices\"][0][\"message\"][\"content\"])\n",
        "\n",
        "# completion = openai.chat.completions.create(\n",
        "#                     model=openai_deployment,\n",
        "#                     max_tokens=15000,\n",
        "#                     temperature=0,\n",
        "#                     messages=[\n",
        "#                         {\"role\": \"user\", \"content\": question}\n",
        "#                     ]\n",
        "#                 )\n",
        "# nlp_results = completion.choices[0].message.content\n",
        "# print(nlp_results)\n",
        "\n",
        "# print(os.environ['OPENAI_API_BASE'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "88a9f2e3-c729-455a-a338-2f83776c1d4c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88a9f2e3-c729-455a-a338-2f83776c1d4c",
        "outputId": "f6c9f4d8-387b-4ffc-e200-0c35ed449d8a"
      },
      "outputs": [],
      "source": [
        "# from langchain.llms import OpenAI\n",
        "from langchain.indexes import GraphIndexCreator\n",
        "from langchain.chains import GraphQAChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "# from langchain.llms import AzureChatOpenAI\n",
        "from langchain.chat_models import AzureChatOpenAI\n",
        "import networkx as nx\n",
        "import concurrent.futures\n",
        "from tqdm import tqdm  # Import tqdm for the progress bar\n",
        "\n",
        "\n",
        "def custom_sentence_tokenize(text, max_words=250):\n",
        "    words = text.split()\n",
        "    sentences = []\n",
        "    current_sentence = []\n",
        "\n",
        "    for word in words:\n",
        "        current_sentence.append(word)\n",
        "        if len(current_sentence) >= max_words:\n",
        "            sentences.append(' '.join(current_sentence))\n",
        "            current_sentence = []\n",
        "\n",
        "    if current_sentence:\n",
        "        sentences.append(' '.join(current_sentence))\n",
        "\n",
        "    return sentences\n",
        "\n",
        " \n",
        "def createGML(text, filename):\n",
        "     \n",
        "    sentences = custom_sentence_tokenize(text)\n",
        "    print(f\"********* {len(sentences)}\")\n",
        "    sleep(8)\n",
        "    try:\n",
        "        for idx, sentence in enumerate(sentences):\n",
        "            # Create a graph for each sentence\n",
        "            prompt_extractor= \"You are a networked intelligence helping a human track knowledge triples about all relevant people, things, concepts, names,places, Dates and Times, Numbers, Organizations, Products and Brands, Events, Roles and Positions, Keywords and Topics, Email Addresses and URLs, References to External Entities, Emotional Tone, Quantities and Units, Codes and Identifiers, Languages, Social Media Handles, Currencies etc. and integrating them with your knowledge stored within your weights as well as that stored in a knowledge graph. Extract all of the knowledge triples from the text. A knowledge triple is a clause that contains a subject, a predicate, and an object. The subject is the entity being described, the predicate is the property of the subject that is being described, and the object is the value of the property.\\n\\nEXAMPLE\\nIt's a state in the US. It's also the number 1 producer of gold in the US.\\n\\nOutput: (Nevada, is a, state)<|>(Nevada, is in, US)<|>(Nevada, is the number 1 producer of, gold)\\nEND OF EXAMPLE\\n\\nEXAMPLE\\nI'm going to the store.\\n\\nOutput: NONE\\nEND OF EXAMPLE\\n\\nEXAMPLE\\nOh huh. I know Descartes likes to drive antique scooters and play the mandolin.\\nOutput: (Descartes, likes to drive, antique scooters)<|>(Descartes, plays, mandolin)\\nEND OF EXAMPLE\\n\\nEXAMPLE\\n{text}Output:\"\n",
        "            index_creator = GraphIndexCreator(llm=AzureChatOpenAI(deployment_name=openai_deployment, temperature=0))\n",
        "            graph = index_creator.from_text(sentence, prompt=PromptTemplate(input_variables=['text'], template=prompt_extractor))\n",
        "            # print(f\" filename=== {filename} and idx=== {idx} \")\n",
        "            # Write each graph to a GML file with a unique filename\n",
        "            filenamex = f\"Models/GML/{filename}_{idx + 1}.gml\"  # Unique filename based on index\n",
        "            graph.write_to_gml(filenamex)\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating GML file {e}\")    \n",
        "\n",
        "\n",
        "def process_file(file):\n",
        "    try:\n",
        "        with open(file, \"r\") as f:\n",
        "            text = f.read().rstrip()\n",
        "            filename = file.split(\"/\")[-1].replace(\".txt\", \"\")\n",
        "            createGML(text, filename)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {file}: {e}\")\n",
        "\n",
        "# def process_files_concurrently(files):\n",
        "#     with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "#         executor.map(process_file, files)\n",
        "\n",
        "\n",
        "def process_files_concurrently(files):\n",
        "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "        # Use tqdm to create a progress bar\n",
        "        with tqdm(total=len(files)) as pbar:\n",
        "            # Define a function to update the progress bar after each task completion\n",
        "            def update(*_):\n",
        "                pbar.update()\n",
        "            \n",
        "            # Submit tasks to the ThreadPoolExecutor\n",
        "            futures = [executor.submit(process_file, file) for file in files]\n",
        "            \n",
        "            # Add the update function to the completion of each future\n",
        "            for future in concurrent.futures.as_completed(futures):\n",
        "                future.add_done_callback(update)\n",
        "\n",
        "folder=\"./Data/cleaned_text_manually/\"\n",
        "files = glob.glob(f\"{folder}*\")\n",
        "\n",
        "# process_files_concurrently(files)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# print(f\"Running pipeline for {len(files)} files in /Data/cleaned_text_manually/ folder\")\n",
        "\n",
        "# for i, file in enumerate(files):\n",
        "#         # print(f\"Extracting entities and relationships for {file}\")\n",
        "#         try:\n",
        "#             with open(file, \"r\") as f:\n",
        "#                  text = f.read().rstrip()\n",
        "#                 #  filename = file.split(\"/\")[-1] \n",
        "#                  filename = file.split(\"/\")[-1].replace(\".txt\", \"\")  # Extracting just the filename without the path and extension\n",
        "#                  createGML(text, filename)\n",
        "#         except Exception as e:\n",
        "#             print(f\"Error processing {file}: {e}\")\n",
        "\n",
        "\n",
        "# # # Create a dictionary to represent the knowledge graph\n",
        "# graph_data = {\n",
        "#     \"nodes\": list(set(triplet[0] for triplet in triples + [triplet[1] for triplet in triples])),\n",
        "#     \"edges\": [{\"source\": triplet[0], \"target\": triplet[1], \"label\": triplet[2]} for triplet in triples]\n",
        "# }\n",
        "\n",
        "# # Convert graph data to JSON format\n",
        "# graph_json = json.dumps(graph_data, indent=4)\n",
        "\n",
        "# # Display or use the JSON data as needed\n",
        "# print(graph_json)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "887a3e86",
      "metadata": {},
      "outputs": [],
      "source": [
        "folder=\"./Models/GML/\"\n",
        "\n",
        "#comibine them\n",
        "# Get all .gml files in the folder\n",
        "filesGml = glob.glob(f\"{folder}*.gml\")\n",
        "\n",
        "# Initialize an empty list to hold graphs\n",
        "graphs = []\n",
        "print(f\"Combine graphs {filesGml}\")\n",
        "# Read each .gml file and append the graphs to the list\n",
        "for file in filesGml:\n",
        "    print(f\"file=== {file}\")\n",
        "    G = nx.read_gml(file)\n",
        "    graphs.append(G)\n",
        "# Combine all the graphs\n",
        "combined_graph = nx.compose_all(graphs)\n",
        "# nx.write_gml(combined_graph, 'combined_graph_model_v.gml')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed06c43d",
      "metadata": {},
      "source": [
        "### Save Graph \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "0388bb7f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# graph.write_to_gml(\"Models/GML/AFG-CPD-2014-EN.gml\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6888973",
      "metadata": {},
      "source": [
        "### Use Saved Graph\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "KZIs4N5S8e8S",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "id": "KZIs4N5S8e8S",
        "outputId": "af6ba50c-e5f6-40be-adaf-07911dab948f"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Based on the given knowledge triplets, we can gather the following information:\n",
              "\n",
              "1. Afghanistan Renewable Energy Policy aims to provide thrust and direction to the REN sector.\n",
              "2. Afghanistan Renewable Energy Policy provides a ‘portfolio approach’ integrating technologies and market structures.\n",
              "3. 2013 is a year.\n",
              "4. 2017 is a date.\n",
              "5. 2017 is 7.1.1 Access to electricity (% population).\n",
              "6. 2017 is PM2.5 (μm/m3).\n",
              "7. 2017 is 2 000.\n",
              "8. 2017 is 100.\n",
              "9. 2017 is 20.\n",
              "10. 2017 is 80.\n",
              "11. 2017 is 2020.\n",
              "12. 2017 is 2018.\n",
              "13. 2017 is a number.\n",
              "14. 2017 is a year.\n",
              "\n",
              "Based on this information, we can see that the Afghanistan Renewable Energy Policy exists and aims to provide direction to the renewable energy sector. It follows a portfolio approach and integrates various technologies and market structures.\n",
              "\n",
              "However, the knowledge triplets do not provide specific details about the policy objectives, guiding principles, barriers to implementation, or the legal and regulatory framework. There is also no information about short-term, medium-term, and long-term implementation strategies or institutional frameworks involving inter-ministerial coordination and stakeholder involvement.\n",
              "\n",
              "Therefore, based on the given knowledge base, we cannot provide a detailed answer to the question regarding the policy objectives, guiding principles, barriers, legal and regulatory framework, or implementation strategies."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from langchain.indexes.graph import NetworkxEntityGraph\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "loaded_graph = NetworkxEntityGraph.from_gml(\"moonshot_AI_graph_model_v1.gml\")\n",
        "\n",
        "# prompt =  \"Ignore previous output. Use the following knowledge triplets  to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\n{context}\\n\\nQuestion: {question}\\nHelpful Answer:\"\n",
        "\n",
        "prompt =  \"Use the following knowledge triplets to answer the question at the end. If you don't know the answer, look out for potential factors in the knowledge triplets else just say I don't know based on my knowledge base, don't try to make up an answer. If a term like a Continent is used e.g Africa, Asia, replace the continent with all african countries available in the knowledge triplets. E.g Nigeria, South Africa and Egypt are under Africa. In your answer, Always refer to knowledge triplets as knowledge base.\\n\\n{context}\\n\\nQuestion: {question}\\nHelpful Answer:\"\n",
        "prompt_entity=\"Extract all entities from the following text. As a guideline, a proper noun is generally capitalized. You should definitely extract all names,places, Dates and Times, Numbers, Organizations, Products and Brands, Events, Roles and Positions, Keywords and Topics, Email Addresses and URLs, References to External Entities, Emotional Tone, Quantities and Units, Codes and Identifiers, Languages, Social Media Handles, Currencies..\\n\\nReturn the output as a single comma-separated list, or NONE if there is nothing of note to return.\\n\\nEXAMPLE\\ni'm trying to improve Langchain's interfaces, the UX, its integrations with various products the user might want ... a lot of stuff.\\nOutput: Langchain\\nEND OF EXAMPLE\\n\\nEXAMPLE\\ni'm trying to improve Langchain's interfaces, the UX, its integrations with various products the user might want ... a lot of stuff. I'm working with Sam.\\nOutput: Langchain, Sam\\nEND OF EXAMPLE\\n\\nBegin!\\n\\n{input}\\nOutput:\"\n",
        "question = f\"\"\"\n",
        "I'm interested in Afghanistan Renewable Energy Policy, especially in the rural area in 2013, also the future focus from 2017 to 2027. I'd like to learn more about the policy objectives and guiding principles, but also what barriers have been identified to the implementation of renewable rural energy policies? What is the legal and regulatory framework underpinning this policy? Can you break into short-term, medium-term, and long-term implementation strategies as well as institutional frameworks involving inter-ministerial coordination and stakeholder involvement?\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "chain = GraphQAChain.from_llm(AzureChatOpenAI(temperature=0, deployment_name= openai_deployment), graph=loaded_graph, verbose=False,\n",
        "qa_prompt=PromptTemplate(input_variables=['context', 'question'], template=prompt),\n",
        "entity_prompt=PromptTemplate(input_variables=['input'], template=prompt_entity)\n",
        ")\n",
        "response = chain.run(question) \n",
        "\n",
        "\n",
        "# IPython.display.HTML(response)\n",
        "display(Markdown(response))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
