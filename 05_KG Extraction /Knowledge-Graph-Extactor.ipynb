{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moonshot  AI Knowledge Graph -> Graph DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline\n",
    "1. Configuration\n",
    "2. Helper Functions\n",
    "3. Prompts\n",
    "4. Running the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install openai\n",
    "# pip install neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from string import Template\n",
    "import json\n",
    "from neo4j import GraphDatabase\n",
    "import glob\n",
    "from timeit import default_timer as timer\n",
    "from dotenv import load_dotenv\n",
    "from time import sleep\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain.schema import Document\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI API configuration\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_key = os.getenv(\"api_key_azure\")\n",
    "openai.api_base = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "openai.api_version = os.getenv(\"api_version\")\n",
    "openai_deployment = \"sdgi-gpt-35-turbo-16k\"\n",
    "\n",
    "# openai.api_key = os.getenv(\"OPENAI_KEY\")\n",
    "\n",
    "\n",
    "# print(openai.api_key)\n",
    "# print(openai.api_base)\n",
    "# print(openai.api_version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neo4j configuration & constraints\n",
    "neo4j_url = os.getenv(\"NEO4J_CONNECTION_URL\")\n",
    "neo4j_user = os.getenv(\"NEO4J_USER\")\n",
    "neo4j_password = os.getenv(\"NEO4J_PASSWORD\")\n",
    "# print(f\" neo4j_url == {neo4j_url}\")\n",
    "# print(f\" neo4j_user == {neo4j_user}\")\n",
    "# print(f\" neo4j_password == {neo4j_password}\")\n",
    "\n",
    "gds = GraphDatabase.driver(neo4j_url, auth=(neo4j_user, neo4j_password))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to call the OpenAI API\n",
    "def process_gpt(file_prompt, system_msg):\n",
    "    # sleep(35)\n",
    "    completion = openai.chat.completions.create(\n",
    "                    model=openai_deployment,\n",
    "                    max_tokens=15000,\n",
    "                    temperature=0,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": system_msg},\n",
    "                        {\"role\": \"user\", \"content\": file_prompt},\n",
    "                    ]\n",
    "                )\n",
    "    nlp_results = completion.choices[0].message.content\n",
    "    # print(f\"NLP Result:::  {nlp_results}\")\n",
    "    sleep(8)\n",
    "    return nlp_results\n",
    "\n",
    "\n",
    "# Function to take folder of files and a prompt template, and return a json-object of all the entities and relationships\n",
    "def extract_entities_relationships(folder, prompt_template):\n",
    "    start = timer()\n",
    "    files = glob.glob(f\"./Data/{folder}/*\")\n",
    "    system_msg = \"You are a helpful IT-project and account management expert who extracts information from documents.\"\n",
    "    print(f\"Running pipeline for {len(files)} files in {folder} folder\")\n",
    "    results = []\n",
    "    for i, file in enumerate(files):\n",
    "        print(f\"Extracting entities and relationships for {file}\")\n",
    "        try:\n",
    "            with open(file, \"r\") as f:\n",
    "                text = f.read().rstrip()\n",
    "                prompt = Template(prompt_template).substitute(ctext=text)\n",
    "                result = process_gpt(prompt, system_msg=system_msg)\n",
    "                results.append(json.loads(result))\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file}: {e}\")\n",
    "    end = timer()\n",
    "    print(f\"Pipeline completed in {end-start} seconds\")\n",
    "    return results\n",
    "\n",
    "# Function to take a json-object of entitites and relationships and generate cypher query for creating those entities\n",
    "def generate_cypher(json_obj):\n",
    "    e_statements = []\n",
    "    r_statements = []\n",
    "\n",
    "    e_label_map = {}\n",
    "\n",
    "    # print(f\" generating cypher for {json_obj}\")\n",
    "    # loop through our json object\n",
    "    for i, obj in enumerate(json_obj):\n",
    "        print(f\"Generating cypher for file {i+1} of {len(json_obj)}\")\n",
    "        for entity in obj[\"entities\"]:\n",
    "            label = entity[\"label\"]\n",
    "            id = entity[\"id\"]\n",
    "            id = id.replace(\"-\", \"\").replace(\"_\", \"\")\n",
    "            properties = {k: v for k, v in entity.items() if k not in [\"label\", \"id\"]}\n",
    "\n",
    "            cypher = f'MERGE (n:{label} {{id: \"{id}\"}})'\n",
    "            if properties:\n",
    "                props_str = \", \".join(\n",
    "                    [f'n.{key} = \"{val}\"' for key, val in properties.items()]\n",
    "                )\n",
    "                cypher += f\" ON CREATE SET {props_str}\"\n",
    "            e_statements.append(cypher)\n",
    "            e_label_map[id] = label\n",
    "\n",
    "        for rs in obj[\"relationships\"]:\n",
    "            src_id, rs_type, tgt_id = rs.split(\"|\")\n",
    "            src_id = src_id.replace(\"-\", \"\").replace(\"_\", \"\")\n",
    "            tgt_id = tgt_id.replace(\"-\", \"\").replace(\"_\", \"\")\n",
    "\n",
    "            src_label = e_label_map[src_id]\n",
    "            tgt_label = e_label_map[tgt_id]\n",
    "\n",
    "            cypher = f'MERGE (a:{src_label} {{id: \"{src_id}\"}}) MERGE (b:{tgt_label} {{id: \"{tgt_id}\"}}) MERGE (a)-[:{rs_type}]->(b)'\n",
    "            r_statements.append(cypher)\n",
    "\n",
    "    with open(\"cyphers.txt\", \"w\") as outfile:\n",
    "        outfile.write(\"\\n\".join(e_statements + r_statements))\n",
    "\n",
    "    return e_statements + r_statements\n",
    "\n",
    "\n",
    "# Final function to bring all the steps together\n",
    "def ingestion_pipeline(folders):\n",
    "    # Extrating the entites and relationships from each folder, append into one json_object\n",
    "    entities_relationships = []\n",
    "    for key, value in folders.items():\n",
    "        entities_relationships.extend(extract_entities_relationships(key, value))\n",
    "\n",
    "    # Generate and execute cypher statements\n",
    "    cypher_statements = generate_cypher(entities_relationships)\n",
    "    for i, stmt in enumerate(cypher_statements):\n",
    "        print(f\"Executing cypher statement {i+1} of {len(cypher_statements)}\")\n",
    "        try:\n",
    "            gds.execute_query(stmt)\n",
    "        except Exception as e:\n",
    "            with open(\"failed_statements.txt\", \"w\") as f:\n",
    "                f.write(f\"{stmt} - Exception: {e}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Defining Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt_template = \"\"\"\n",
    "From the Brief below, extract the following Entities & relationships described in the mentioned format \n",
    "0. ALWAYS FINISH THE OUTPUT. Never send partial responses\n",
    "1. First, look for  Entity types in the text and generate as comma-separated format similar to entity type.\n",
    "   `id` property of each entity must be alphanumeric and must be unique among the entities. You will be referring this property to define the relationship between entities. Do not create new entity types that aren't mentioned below. Document must be summarized and stored inside Country entity under `summary` property. You will have to generate as many entities as needed as per the types below:\n",
    "    Entity Types:\n",
    "    label:'Entity',id:string,name:string;summary:string \n",
    "2. Next generate each relationships as triples of head, relationship and tail. To refer the head and tail entity, use their respective `id` property. \n",
    "   Relationship property should be mentioned within brackets as comma-separated. \n",
    "   You will have to generate as many relationships as needed as defined below:\n",
    "    Relationship types:\n",
    "    Entity|RELATIONSHIP_TYPE|Entity \n",
    "3. The output should look like :\n",
    "{\n",
    "    \"entities\": [{\"label\":\"Entity\",\"id\":string,\"name\":string,\"summary\":string}],\n",
    "    \"relationships\": [\"Entityid|RELATIONSHIP_TYPE|AnotherEntityid\"]\n",
    "}\n",
    "Entity, RELATIONSHIP_TYPE and AnotherEntityid are to be generate by you based on the brief.\n",
    "Case Sheet:\n",
    "$ctext\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Running the pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running pipeline for 54 files in cleaned_text_manually folder\n",
      "Extracting entities and relationships for ./Data/cleaned_text_manually/AFG-CPD-2014-EN_part16.txt\n",
      "Extracting entities and relationships for ./Data/cleaned_text_manually/AFG-CPD-2014-EN_part17.txt\n",
      "Extracting entities and relationships for ./Data/cleaned_text_manually/AFG-CPD-2014-EN_part15.txt\n",
      "Extracting entities and relationships for ./Data/cleaned_text_manually/AFG-CPD-2014-EN_part29.txt\n",
      "Extracting entities and relationships for ./Data/cleaned_text_manually/AFG-CPD-2014-EN_part28.txt\n",
      "Extracting entities and relationships for ./Data/cleaned_text_manually/AFG-CPD-2014-EN_part14.txt\n",
      "Extracting entities and relationships for ./Data/cleaned_text_manually/AFG-CPD-2014-EN_part38.txt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPStatusError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/_base_client.py:912\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/_base_client.py?line=910'>911</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/_base_client.py?line=911'>912</a>\u001b[0m     response\u001b[39m.\u001b[39;49mraise_for_status()\n\u001b[1;32m    <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/_base_client.py?line=912'>913</a>\u001b[0m \u001b[39mexcept\u001b[39;00m httpx\u001b[39m.\u001b[39mHTTPStatusError \u001b[39mas\u001b[39;00m err:  \u001b[39m# thrown on 4xx and 5xx status code\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.6/lib/python3.9/site-packages/httpx/_models.py:758\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/httpx/_models.py?line=756'>757</a>\u001b[0m message \u001b[39m=\u001b[39m message\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m, error_type\u001b[39m=\u001b[39merror_type)\n\u001b[0;32m--> <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/httpx/_models.py?line=757'>758</a>\u001b[0m \u001b[39mraise\u001b[39;00m HTTPStatusError(message, request\u001b[39m=\u001b[39mrequest, response\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n",
      "\u001b[0;31mHTTPStatusError\u001b[0m: Client error '429 Too Many Requests' for url 'https://undp-ngd-openai-datafutures-dev-2.openai.azure.com//openai/deployments/sdgi-gpt-35-turbo-16k/chat/completions?api-version=2023-09-01-preview'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m countries \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcleaned_text_manually\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt_template,\n\u001b[1;32m      3\u001b[0m }\n\u001b[0;32m----> 5\u001b[0m \u001b[43mingestion_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcountries\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 88\u001b[0m, in \u001b[0;36mingestion_pipeline\u001b[0;34m(folders)\u001b[0m\n\u001b[1;32m     86\u001b[0m entities_relationships \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m folders\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m---> 88\u001b[0m     entities_relationships\u001b[38;5;241m.\u001b[39mextend(\u001b[43mextract_entities_relationships\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# Generate and execute cypher statements\u001b[39;00m\n\u001b[1;32m     91\u001b[0m cypher_statements \u001b[38;5;241m=\u001b[39m generate_cypher(entities_relationships)\n",
      "Cell \u001b[0;32mIn[7], line 32\u001b[0m, in \u001b[0;36mextract_entities_relationships\u001b[0;34m(folder, prompt_template)\u001b[0m\n\u001b[1;32m     30\u001b[0m         text \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39mrstrip()\n\u001b[1;32m     31\u001b[0m         prompt \u001b[38;5;241m=\u001b[39m Template(prompt_template)\u001b[38;5;241m.\u001b[39msubstitute(ctext\u001b[38;5;241m=\u001b[39mtext)\n\u001b[0;32m---> 32\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_gpt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem_msg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msystem_msg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(json\u001b[38;5;241m.\u001b[39mloads(result))\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m, in \u001b[0;36mprocess_gpt\u001b[0;34m(file_prompt, system_msg)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_gpt\u001b[39m(file_prompt, system_msg):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# sleep(35)\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     completion \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopenai_deployment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem_msg\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_prompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     nlp_results \u001b[38;5;241m=\u001b[39m completion\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# print(f\"NLP Result:::  {nlp_results}\")\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/_utils/_utils.py:303\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/_utils/_utils.py?line=300'>301</a>\u001b[0m             msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMissing required argument: \u001b[39m\u001b[39m{\u001b[39;00mquote(missing[\u001b[39m0\u001b[39m])\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/_utils/_utils.py?line=301'>302</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/_utils/_utils.py?line=302'>303</a>\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/resources/chat/completions.py:598\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/resources/chat/completions.py?line=550'>551</a>\u001b[0m \u001b[39m@required_args\u001b[39m([\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m], [\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/resources/chat/completions.py?line=551'>552</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/resources/chat/completions.py?line=552'>553</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/resources/chat/completions.py?line=595'>596</a>\u001b[0m     timeout: \u001b[39mfloat\u001b[39m \u001b[39m|\u001b[39m httpx\u001b[39m.\u001b[39mTimeout \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m|\u001b[39m NotGiven \u001b[39m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/resources/chat/completions.py?line=596'>597</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ChatCompletion \u001b[39m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/resources/chat/completions.py?line=597'>598</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_post(\n\u001b[1;32m    <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/resources/chat/completions.py?line=598'>599</a>\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39m/chat/completions\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/resources/chat/completions.py?line=599'>600</a>\u001b[0m         body\u001b[39m=\u001b[39;49mmaybe_transform(\n\u001b[1;32m    <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/resources/chat/completions.py?line=600'>601</a>\u001b[0m             {\n\u001b[1;32m    <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/resources/chat/completions.py?line=601'>602</a>\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmessages\u001b[39;49m\u001b[39m\"\u001b[39;49m: messages,\n\u001b[1;32m    <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/resources/chat/completions.py?line=602'>603</a>\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m\"\u001b[39;49m: model,\n\u001b[1;32m    <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/resources/chat/completions.py?line=603'>604</a>\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfrequency_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: frequency_penalty,\n\u001b[1;32m    <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/resources/chat/completions.py?line=604'>605</a>\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfunction_call\u001b[39;49m\u001b[39m\"\u001b[39;49m: function_call,\n\u001b[1;32m    <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/resources/chat/completions.py?line=605'>606</a>\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfunctions\u001b[39;49m\u001b[39m\"\u001b[39;49m: functions,\n\u001b[1;32m    <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/resources/chat/completions.py?line=606'>607</a>\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mlogit_bias\u001b[39;49m\u001b[39m\"\u001b[39;49m: logit_bias,\n\u001b[1;32m    <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/resources/chat/completions.py?line=607'>608</a>\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmax_tokens\u001b[39;49m\u001b[39m\"\u001b[39;49m: max_tokens,\n\u001b[1;32m    <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/resources/chat/completions.py?line=608'>609</a>\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mn\u001b[39;49m\u001b[39m\"\u001b[39;49m: n,\n\u001b[1;32m    <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/resources/chat/completions.py?line=609'>610</a>\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mpresence_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: presence_penalty,\n\u001b[1;32m    <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/resources/chat/completions.py?line=610'>611</a>\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mresponse_format\u001b[39;49m\u001b[39m\"\u001b[39;49m: response_format,\n\u001b[1;32m    <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/resources/chat/completions.py?line=611'>612</a>\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mseed\u001b[39;49m\u001b[39m\"\u001b[39;49m: seed,\n\u001b[1;32m    <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/resources/chat/completions.py?line=612'>613</a>\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstop\u001b[39;49m\u001b[39m\"\u001b[39;49m: stop,\n\u001b[1;32m    <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/resources/chat/completions.py?line=613'>614</a>\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstream\u001b[39;49m\u001b[39m\"\u001b[39;49m: stream,\n\u001b[1;32m    <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/resources/chat/completions.py?line=614'>615</a>\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtemperature\u001b[39;49m\u001b[39m\"\u001b[39;49m: temperature,\n\u001b[1;32m    <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/resources/chat/completions.py?line=615'>616</a>\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtool_choice\u001b[39;49m\u001b[39m\"\u001b[39;49m: tool_choice,\n\u001b[1;32m    <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/resources/chat/completions.py?line=616'>617</a>\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtools\u001b[39;49m\u001b[39m\"\u001b[39;49m: tools,\n\u001b[1;32m    <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/resources/chat/completions.py?line=617'>618</a>\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtop_p\u001b[39;49m\u001b[39m\"\u001b[39;49m: top_p,\n\u001b[1;32m    <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/resources/chat/completions.py?line=618'>619</a>\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m: user,\n\u001b[1;32m    <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/resources/chat/completions.py?line=619'>620</a>\u001b[0m             },\n\u001b[1;32m    <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/resources/chat/completions.py?line=620'>621</a>\u001b[0m             completion_create_params\u001b[39m.\u001b[39;49mCompletionCreateParams,\n\u001b[1;32m    <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/resources/chat/completions.py?line=621'>622</a>\u001b[0m         ),\n\u001b[1;32m    <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/resources/chat/completions.py?line=622'>623</a>\u001b[0m         options\u001b[39m=\u001b[39;49mmake_request_options(\n\u001b[1;32m    <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/resources/chat/completions.py?line=623'>624</a>\u001b[0m             extra_headers\u001b[39m=\u001b[39;49mextra_headers, extra_query\u001b[39m=\u001b[39;49mextra_query, extra_body\u001b[39m=\u001b[39;49mextra_body, timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[1;32m    <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/resources/chat/completions.py?line=624'>625</a>\u001b[0m         ),\n\u001b[1;32m    <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/resources/chat/completions.py?line=625'>626</a>\u001b[0m         cast_to\u001b[39m=\u001b[39;49mChatCompletion,\n\u001b[1;32m    <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/resources/chat/completions.py?line=626'>627</a>\u001b[0m         stream\u001b[39m=\u001b[39;49mstream \u001b[39mor\u001b[39;49;00m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/resources/chat/completions.py?line=627'>628</a>\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mStream[ChatCompletionChunk],\n\u001b[1;32m    <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/resources/chat/completions.py?line=628'>629</a>\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/_base_client.py:1088\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/_base_client.py?line=1073'>1074</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpost\u001b[39m(\n\u001b[1;32m   <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/_base_client.py?line=1074'>1075</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/_base_client.py?line=1075'>1076</a>\u001b[0m     path: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/_base_client.py?line=1082'>1083</a>\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/_base_client.py?line=1083'>1084</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[1;32m   <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/_base_client.py?line=1084'>1085</a>\u001b[0m     opts \u001b[39m=\u001b[39m FinalRequestOptions\u001b[39m.\u001b[39mconstruct(\n\u001b[1;32m   <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/_base_client.py?line=1085'>1086</a>\u001b[0m         method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m, url\u001b[39m=\u001b[39mpath, json_data\u001b[39m=\u001b[39mbody, files\u001b[39m=\u001b[39mto_httpx_files(files), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions\n\u001b[1;32m   <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/_base_client.py?line=1086'>1087</a>\u001b[0m     )\n\u001b[0;32m-> <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/_base_client.py?line=1087'>1088</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(ResponseT, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(cast_to, opts, stream\u001b[39m=\u001b[39;49mstream, stream_cls\u001b[39m=\u001b[39;49mstream_cls))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/_base_client.py:853\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/_base_client.py?line=843'>844</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/_base_client.py?line=844'>845</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/_base_client.py?line=845'>846</a>\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/_base_client.py?line=850'>851</a>\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/_base_client.py?line=851'>852</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[0;32m--> <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/_base_client.py?line=852'>853</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[1;32m    <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/_base_client.py?line=853'>854</a>\u001b[0m         cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[1;32m    <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/_base_client.py?line=854'>855</a>\u001b[0m         options\u001b[39m=\u001b[39;49moptions,\n\u001b[1;32m    <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/_base_client.py?line=855'>856</a>\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/_base_client.py?line=856'>857</a>\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m    <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/_base_client.py?line=857'>858</a>\u001b[0m         remaining_retries\u001b[39m=\u001b[39;49mremaining_retries,\n\u001b[1;32m    <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/_base_client.py?line=858'>859</a>\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/_base_client.py:916\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/_base_client.py?line=913'>914</a>\u001b[0m \u001b[39mif\u001b[39;00m retries \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_retry(err\u001b[39m.\u001b[39mresponse):\n\u001b[1;32m    <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/_base_client.py?line=914'>915</a>\u001b[0m     err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mclose()\n\u001b[0;32m--> <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/_base_client.py?line=915'>916</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_retry_request(\n\u001b[1;32m    <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/_base_client.py?line=916'>917</a>\u001b[0m         options,\n\u001b[1;32m    <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/_base_client.py?line=917'>918</a>\u001b[0m         cast_to,\n\u001b[1;32m    <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/_base_client.py?line=918'>919</a>\u001b[0m         retries,\n\u001b[1;32m    <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/_base_client.py?line=919'>920</a>\u001b[0m         err\u001b[39m.\u001b[39;49mresponse\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/_base_client.py?line=920'>921</a>\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/_base_client.py?line=921'>922</a>\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m    <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/_base_client.py?line=922'>923</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/_base_client.py?line=924'>925</a>\u001b[0m \u001b[39m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/_base_client.py?line=925'>926</a>\u001b[0m \u001b[39m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/_base_client.py?line=926'>927</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/_base_client.py:956\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/_base_client.py?line=951'>952</a>\u001b[0m log\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mRetrying request to \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m in \u001b[39m\u001b[39m%f\u001b[39;00m\u001b[39m seconds\u001b[39m\u001b[39m\"\u001b[39m, options\u001b[39m.\u001b[39murl, timeout)\n\u001b[1;32m    <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/_base_client.py?line=953'>954</a>\u001b[0m \u001b[39m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/_base_client.py?line=954'>955</a>\u001b[0m \u001b[39m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/_base_client.py?line=955'>956</a>\u001b[0m time\u001b[39m.\u001b[39;49msleep(timeout)\n\u001b[1;32m    <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/_base_client.py?line=957'>958</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_request(\n\u001b[1;32m    <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/_base_client.py?line=958'>959</a>\u001b[0m     options\u001b[39m=\u001b[39moptions,\n\u001b[1;32m    <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/_base_client.py?line=959'>960</a>\u001b[0m     cast_to\u001b[39m=\u001b[39mcast_to,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/_base_client.py?line=962'>963</a>\u001b[0m     stream_cls\u001b[39m=\u001b[39mstream_cls,\n\u001b[1;32m    <a href='file:///Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages/openai/_base_client.py?line=963'>964</a>\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "countries = {\n",
    "    \"cleaned_text_manually\": prompt_template,\n",
    "}\n",
    "\n",
    "ingestion_pipeline(countries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the knowledge graph in a RAG application\n",
    "from langchain.chains import GraphCypherQAChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.graphs import Neo4jGraph\n",
    "import os\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] =  os.getenv(\"OPENAI_KEY\")\n",
    " \n",
    "\n",
    "graph = Neo4jGraph(\n",
    "    url=neo4j_url,\n",
    "    username=neo4j_user,\n",
    "    password=neo4j_password\n",
    ")\n",
    "\n",
    "graph.refresh_schema()\n",
    "\n",
    "cypher_chain = GraphCypherQAChain.from_llm(\n",
    "    graph=graph,\n",
    "    cypher_llm=ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\"),\n",
    "    qa_llm=ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-16k\"),\n",
    "    validate_cypher=True, # Validate relationship directions\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (e1:Entity)-[r:TRANSFORMS]->(e2:Entity)\n",
      "WHERE e2.name = \"Transformation Decade\"\n",
      "RETURN e1.summary, r.summary\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, but I don't have any information on the Transformation Decade or what caused it to start.\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cypher_chain.run(\"When did the Transformation Decade start and what even caused it to start?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### . Token limit each file  (optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/davidoluyalegbenga/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "nltk.download('punkt')  # Download NLTK tokenizer data if not already downloaded\n",
    "\n",
    "# Function to split file into subfiles based on sentences\n",
    "def split_file_sentences(filename, sentence_limit=5):\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "        sentences = nltk.sent_tokenize(content)  # Tokenize into sentences\n",
    "        \n",
    "        num_sentences = len(sentences)\n",
    "        num_files = (num_sentences // sentence_limit) + 1\n",
    "        \n",
    "        for i in range(num_files):\n",
    "            start = i * sentence_limit\n",
    "            end = (i + 1) * sentence_limit\n",
    "            sub_sentences = sentences[start:end]  # Extract sentences for subfile\n",
    "            \n",
    "            # Create subfile name (e.g., original_filename_part1.txt, original_filename_part2.txt, ...)\n",
    "            subfile_name = f\"{os.path.splitext(filename)[0]}_part{i + 1}.txt\"\n",
    "            \n",
    "            with open(subfile_name, 'w', encoding='utf-8') as subfile:\n",
    "                subfile.write('\\n'.join(sub_sentences))  # Write sentences to subfile\n",
    "        \n",
    "        os.remove(filename)  # Remove the original file after splitting\n",
    "\n",
    "# Directory containing the .txt files\n",
    "directory = 'Data/cleaned_text_manually'\n",
    "\n",
    "# Loop through each file in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        split_file_sentences(file_path)\n",
    "\n",
    "\n",
    "\n",
    "# import os\n",
    "# import nltk\n",
    "# from nltk.tokenize import word_tokenize\n",
    "\n",
    "# nltk.download('punkt')  # Download NLTK tokenizer data if not already downloaded\n",
    "\n",
    "# # Function to read a file, tokenize its content, and split it into subfiles\n",
    "# def split_file(filename, token_limit=500):\n",
    "#     with open(filename, 'r', encoding='utf-8') as file:\n",
    "#         content = file.read()\n",
    "#         tokens = word_tokenize(content)  # Tokenize the content\n",
    "        \n",
    "#         num_tokens = len(tokens)\n",
    "#         num_files = (num_tokens // token_limit) + 1\n",
    "        \n",
    "#         for i in range(num_files):\n",
    "#             start = i * token_limit\n",
    "#             end = (i + 1) * token_limit\n",
    "#             sub_tokens = tokens[start:end]  # Extract tokens for subfile\n",
    "            \n",
    "#             # Create subfile name (e.g., original_filename_part1.txt, original_filename_part2.txt, ...)\n",
    "#             subfile_name = f\"{os.path.splitext(filename)[0]}_part{i + 1}.txt\"\n",
    "            \n",
    "#             with open(subfile_name, 'w', encoding='utf-8') as subfile:\n",
    "#                 subfile.write(' '.join(sub_tokens))  # Write tokens to subfile\n",
    "        \n",
    "#         os.remove(filename)  # Remove the original file after splitting\n",
    "\n",
    "# # Directory containing the .txt files\n",
    "# directory = 'Data/cleaned_text_manually'\n",
    "\n",
    "# # Loop through each file in the directory\n",
    "# for filename in os.listdir(directory):\n",
    "#     if filename.endswith(\".txt\"):\n",
    "#         file_path = os.path.join(directory, filename)\n",
    "#         split_file(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 517 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting joblib\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "\u001b[K     |████████████████████████████████| 302 kB 3.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: regex>=2021.8.3 in /Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages (from nltk) (4.66.1)\n",
      "Collecting click\n",
      "  Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 2.2 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: joblib, click, nltk\n",
      "Successfully installed click-8.1.7 joblib-1.3.2 nltk-3.8.1\n",
      "\u001b[33mWARNING: You are using pip version 21.1.3; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the '/Users/davidoluyalegbenga/.pyenv/versions/3.9.6/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
