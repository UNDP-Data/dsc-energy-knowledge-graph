{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moonshot  AI Knowledge Graph -> Graph DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline\n",
    "1. Configuration\n",
    "2. Helper Functions\n",
    "3. Prompts\n",
    "4. Running the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install openai\n",
    "# pip install neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from string import Template\n",
    "import json\n",
    "from neo4j import GraphDatabase\n",
    "import glob\n",
    "from timeit import default_timer as timer\n",
    "from dotenv import load_dotenv\n",
    "from time import sleep\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain.schema import Document\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI API configuration\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_key = os.getenv(\"api_key_azure\")\n",
    "openai.api_base = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "openai.api_version = os.getenv(\"api_version\")\n",
    "openai_deployment = \"sdgi-gpt-35-turbo-16k\"\n",
    "\n",
    "# openai.api_key = os.getenv(\"OPENAI_KEY\")\n",
    "\n",
    "\n",
    "# print(openai.api_key)\n",
    "# print(openai.api_base)\n",
    "# print(openai.api_version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neo4j configuration & constraints\n",
    "neo4j_url = os.getenv(\"NEO4J_CONNECTION_URL\")\n",
    "neo4j_user = os.getenv(\"NEO4J_USER\")\n",
    "neo4j_password = os.getenv(\"NEO4J_PASSWORD\")\n",
    "# print(f\" neo4j_url == {neo4j_url}\")\n",
    "# print(f\" neo4j_user == {neo4j_user}\")\n",
    "# print(f\" neo4j_password == {neo4j_password}\")\n",
    "\n",
    "gds = GraphDatabase.driver(neo4j_url, auth=(neo4j_user, neo4j_password))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to call the OpenAI API\n",
    "def process_gpt(file_prompt, system_msg):\n",
    "    sleep(35)\n",
    "    # completion = openai.ChatCompletion.create(\n",
    "    #     engine=openai_deployment,\n",
    "    #     max_tokens=15000,\n",
    "    #     temperature=0,\n",
    "    #     messages=[\n",
    "    #         {\"role\": \"system\", \"content\": system_msg},\n",
    "    #         {\"role\": \"user\", \"content\": file_prompt},\n",
    "    #     ],\n",
    "    # )\n",
    "    completion = openai.chat.completions.create(\n",
    "                    model=openai_deployment,\n",
    "                    max_tokens=15000,\n",
    "                    temperature=0,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": system_msg},\n",
    "                        {\"role\": \"user\", \"content\": file_prompt},\n",
    "                    ]\n",
    "                )\n",
    "    nlp_results = completion.choices[0].message.content\n",
    "    # print(f\"NLP Result:::  {nlp_results}\")\n",
    "    sleep(8)\n",
    "    return nlp_results\n",
    "\n",
    "\n",
    "# Function to take folder of files and a prompt template, and return a json-object of all the entities and relationships\n",
    "def extract_entities_relationships(folder, prompt_template):\n",
    "    start = timer()\n",
    "    files = glob.glob(f\"./Data/{folder}/*\")\n",
    "    system_msg = \"You are a helpful IT-project and account management expert who extracts information from documents.\"\n",
    "    print(f\"Running pipeline for {len(files)} files in {folder} folder\")\n",
    "    results = []\n",
    "    for i, file in enumerate(files):\n",
    "        print(f\"Extracting entities and relationships for {file}\")\n",
    "        try:\n",
    "            with open(file, \"r\") as f:\n",
    "                text = f.read().rstrip()\n",
    "                prompt = Template(prompt_template).substitute(ctext=text)\n",
    "                result = process_gpt(prompt, system_msg=system_msg)\n",
    "                results.append(json.loads(result))\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file}: {e}\")\n",
    "    end = timer()\n",
    "    print(f\"Pipeline completed in {end-start} seconds\")\n",
    "    return results\n",
    "\n",
    "    # for i, file in enumerate(files):\n",
    "    #     print(f\"Extracting entities and relationships for {file}\")\n",
    "    #     try:\n",
    "    #         with open(file, \"r\") as f:\n",
    "    #             #chunk_overlap=14\n",
    "    #             text_splitter = TokenTextSplitter(chunk_size=500)\n",
    "    #             text = f.read().rstrip() \n",
    "    #             docs = text_splitter.create_documents([text])\n",
    "    #             documents = text_splitter.split_documents(docs)  # Split first 3 documents\n",
    "    #             # Document\n",
    "    #             for document in documents:\n",
    "    #                 print(\"***********\")                   \n",
    "    #                 print(f\"on document=== {len(document.page_content)}\")\n",
    "    #                 prompt = Template(prompt_template).substitute(ctext=document.page_content)\n",
    "    #                 result = process_gpt(prompt, system_msg=system_msg)\n",
    "    #                 results.append(json.loads(result))\n",
    "    #                 print(f\"extract_entities_relationships === {results}\")\n",
    "\n",
    "    #                 print(\"***********\")\n",
    "    #     except Exception as e:\n",
    "    #         print(f\"Error processing {file}: {e}\")\n",
    "    # end = timer()\n",
    "    # print(f\"Pipeline completed in {end-start} seconds\")\n",
    "    # return results\n",
    "\n",
    "\n",
    "# Function to take a json-object of entitites and relationships and generate cypher query for creating those entities\n",
    "def generate_cypher(json_obj):\n",
    "    e_statements = []\n",
    "    r_statements = []\n",
    "\n",
    "    e_label_map = {}\n",
    "\n",
    "    print(f\" generating cypher for {json_obj}\")\n",
    "    # loop through our json object\n",
    "    for i, obj in enumerate(json_obj):\n",
    "        print(f\"Generating cypher for file {i+1} of {len(json_obj)}\")\n",
    "        for entity in obj[\"entities\"]:\n",
    "            label = entity[\"label\"]\n",
    "            id = entity[\"id\"]\n",
    "            id = id.replace(\"-\", \"\").replace(\"_\", \"\")\n",
    "            properties = {k: v for k, v in entity.items() if k not in [\"label\", \"id\"]}\n",
    "\n",
    "            cypher = f'MERGE (n:{label} {{id: \"{id}\"}})'\n",
    "            if properties:\n",
    "                props_str = \", \".join(\n",
    "                    [f'n.{key} = \"{val}\"' for key, val in properties.items()]\n",
    "                )\n",
    "                cypher += f\" ON CREATE SET {props_str}\"\n",
    "            e_statements.append(cypher)\n",
    "            e_label_map[id] = label\n",
    "\n",
    "        for rs in obj[\"relationships\"]:\n",
    "            src_id, rs_type, tgt_id = rs.split(\"|\")\n",
    "            src_id = src_id.replace(\"-\", \"\").replace(\"_\", \"\")\n",
    "            tgt_id = tgt_id.replace(\"-\", \"\").replace(\"_\", \"\")\n",
    "\n",
    "            src_label = e_label_map[src_id]\n",
    "            tgt_label = e_label_map[tgt_id]\n",
    "\n",
    "            cypher = f'MERGE (a:{src_label} {{id: \"{src_id}\"}}) MERGE (b:{tgt_label} {{id: \"{tgt_id}\"}}) MERGE (a)-[:{rs_type}]->(b)'\n",
    "            r_statements.append(cypher)\n",
    "\n",
    "    with open(\"cyphers.txt\", \"w\") as outfile:\n",
    "        outfile.write(\"\\n\".join(e_statements + r_statements))\n",
    "\n",
    "    return e_statements + r_statements\n",
    "\n",
    "\n",
    "# Final function to bring all the steps together\n",
    "def ingestion_pipeline(folders):\n",
    "    # Extrating the entites and relationships from each folder, append into one json_object\n",
    "    entities_relationships = []\n",
    "    for key, value in folders.items():\n",
    "        entities_relationships.extend(extract_entities_relationships(key, value))\n",
    "\n",
    "    # Generate and execute cypher statements\n",
    "    cypher_statements = generate_cypher(entities_relationships)\n",
    "    for i, stmt in enumerate(cypher_statements):\n",
    "        print(f\"Executing cypher statement {i+1} of {len(cypher_statements)}\")\n",
    "        try:\n",
    "            gds.execute_query(stmt)\n",
    "        except Exception as e:\n",
    "            with open(\"failed_statements.txt\", \"w\") as f:\n",
    "                f.write(f\"{stmt} - Exception: {e}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Defining Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt_template = \"\"\"\n",
    "From the Brief below, extract the following Entities & relationships described in the mentioned format \n",
    "0. ALWAYS FINISH THE OUTPUT. Never send partial responses\n",
    "1. First, look for  Entity types in the text and generate as comma-separated format similar to entity type.\n",
    "   `id` property of each entity must be alphanumeric and must be unique among the entities. You will be referring this property to define the relationship between entities. Do not create new entity types that aren't mentioned below. Document must be summarized and stored inside Country entity under `summary` property. You will have to generate as many entities as needed as per the types below:\n",
    "    Entity Types:\n",
    "    label:'Entity',id:string,name:string;summary:string \n",
    "2. Next generate each relationships as triples of head, relationship and tail. To refer the head and tail entity, use their respective `id` property. \n",
    "   Relationship property should be mentioned within brackets as comma-separated. \n",
    "   You will have to generate as many relationships as needed as defined below:\n",
    "    Relationship types:\n",
    "    Entity|RELATIONSHIP_TYPE|Entity \n",
    "3. The output should look like :\n",
    "{\n",
    "    \"entities\": [{\"label\":\"Entity\",\"id\":string,\"name\":string,\"summary\":string}],\n",
    "    \"relationships\": [\"Entityid|RELATIONSHIP_TYPE|AnotherEntityid\"]\n",
    "}\n",
    "Entity, RELATIONSHIP_TYPE and AnotherEntityid are to be generate by you based on the brief.\n",
    "Case Sheet:\n",
    "$ctext\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Running the pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running pipeline for 564 files in cleaned_text_manually folder\n",
      "Extracting entities and relationships for ./Data/cleaned_text_manually/NGA-NGP-2017-EN_part26.txt\n",
      "Extracting entities and relationships for ./Data/cleaned_text_manually/NGA-NGP-2017-EN_part32.txt\n",
      "Error processing ./Data/cleaned_text_manually/NGA-NGP-2017-EN_part32.txt: Error code: 429 - {'error': {'code': '429', 'message': 'Requests to the ChatCompletions_Create Operation under Azure OpenAI API version 2023-09-01-preview have exceeded token rate limit of your current OpenAI S0 pricing tier. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit.'}}\n",
      "Extracting entities and relationships for ./Data/cleaned_text_manually/AGO-NEPro-2022-EN_part1.txt\n",
      "Extracting entities and relationships for ./Data/cleaned_text_manually/NER-NREP-2013-EN_part43.txt\n",
      "Error processing ./Data/cleaned_text_manually/NER-NREP-2013-EN_part43.txt: Error code: 429 - {'error': {'code': '429', 'message': 'Requests to the ChatCompletions_Create Operation under Azure OpenAI API version 2023-09-01-preview have exceeded token rate limit of your current OpenAI S0 pricing tier. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit.'}}\n",
      "Extracting entities and relationships for ./Data/cleaned_text_manually/AFG-NREP-2013-EN_part11.txt\n",
      "Extracting entities and relationships for ./Data/cleaned_text_manually/ARM-LCEDP-2020-EN_part43.txt\n",
      "Extracting entities and relationships for ./Data/cleaned_text_manually/AFG-NRER-2017-EN_part26.txt\n",
      "Extracting entities and relationships for ./Data/cleaned_text_manually/ARM-LCEDP-2020-EN_part57.txt\n",
      "Error processing ./Data/cleaned_text_manually/ARM-LCEDP-2020-EN_part57.txt: Error code: 429 - {'error': {'code': '429', 'message': 'Requests to the ChatCompletions_Create Operation under Azure OpenAI API version 2023-09-01-preview have exceeded token rate limit of your current OpenAI S0 pricing tier. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit.'}}\n",
      "Extracting entities and relationships for ./Data/cleaned_text_manually/AFG-NRER-2017-EN_part32.txt\n",
      "Extracting entities and relationships for ./Data/cleaned_text_manually/AFG-NREP-2015-EN_part27.txt\n",
      "Error processing ./Data/cleaned_text_manually/AFG-NREP-2015-EN_part27.txt: Error code: 400 - {'error': {'message': \"This model's maximum context length is 16384 tokens. However, you requested 17203 tokens (2203 in the messages, 15000 in the completion). Please reduce the length of the messages or completion.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "Extracting entities and relationships for ./Data/cleaned_text_manually/AFG-NREP-2015-EN_part33.txt\n",
      "Error processing ./Data/cleaned_text_manually/AFG-NREP-2015-EN_part33.txt: Error code: 429 - {'error': {'code': '429', 'message': 'Requests to the ChatCompletions_Create Operation under Azure OpenAI API version 2023-09-01-preview have exceeded token rate limit of your current OpenAI S0 pricing tier. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit.'}}\n",
      "Extracting entities and relationships for ./Data/cleaned_text_manually/ALB-NES-2018-EN_part27.txt\n",
      "Extracting entities and relationships for ./Data/cleaned_text_manually/ARG-NEPro-2022-EN_part2.txt\n",
      "Error processing ./Data/cleaned_text_manually/ARG-NEPro-2022-EN_part2.txt: Error code: 429 - {'error': {'code': '429', 'message': 'Requests to the ChatCompletions_Create Operation under Azure OpenAI API version 2023-09-01-preview have exceeded token rate limit of your current OpenAI S0 pricing tier. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit.'}}\n",
      "Extracting entities and relationships for ./Data/cleaned_text_manually/ALB-NES-2018-EN_part33.txt\n",
      "Extracting entities and relationships for ./Data/cleaned_text_manually/ARM-CPD-2021-EN_part2.txt\n",
      "Extracting entities and relationships for ./Data/cleaned_text_manually/NER-CPD-2022-EN_part5.txt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[95], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m countries \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcleaned_text_manually\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt_template,\n\u001b[1;32m      3\u001b[0m }\n\u001b[0;32m----> 5\u001b[0m \u001b[43mingestion_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcountries\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[91], line 123\u001b[0m, in \u001b[0;36mingestion_pipeline\u001b[0;34m(folders)\u001b[0m\n\u001b[1;32m    121\u001b[0m entities_relationships \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m folders\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 123\u001b[0m     entities_relationships\u001b[38;5;241m.\u001b[39mextend(\u001b[43mextract_entities_relationships\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# Generate and execute cypher statements\u001b[39;00m\n\u001b[1;32m    126\u001b[0m cypher_statements \u001b[38;5;241m=\u001b[39m generate_cypher(entities_relationships)\n",
      "Cell \u001b[0;32mIn[91], line 41\u001b[0m, in \u001b[0;36mextract_entities_relationships\u001b[0;34m(folder, prompt_template)\u001b[0m\n\u001b[1;32m     39\u001b[0m         text \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39mrstrip()\n\u001b[1;32m     40\u001b[0m         prompt \u001b[38;5;241m=\u001b[39m Template(prompt_template)\u001b[38;5;241m.\u001b[39msubstitute(ctext\u001b[38;5;241m=\u001b[39mtext)\n\u001b[0;32m---> 41\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_gpt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem_msg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msystem_msg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(json\u001b[38;5;241m.\u001b[39mloads(result))\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[0;32mIn[91], line 3\u001b[0m, in \u001b[0;36mprocess_gpt\u001b[0;34m(file_prompt, system_msg)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_gpt\u001b[39m(file_prompt, system_msg):\n\u001b[0;32m----> 3\u001b[0m     \u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m35\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# completion = openai.ChatCompletion.create(\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m#     engine=openai_deployment,\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m#     max_tokens=15000,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m#     ],\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     completion \u001b[38;5;241m=\u001b[39m openai\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m     14\u001b[0m                     model\u001b[38;5;241m=\u001b[39mopenai_deployment,\n\u001b[1;32m     15\u001b[0m                     max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15000\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m                     ]\n\u001b[1;32m     21\u001b[0m                 )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "countries = {\n",
    "    \"cleaned_text_manually\": prompt_template,\n",
    "}\n",
    "\n",
    "ingestion_pipeline(countries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### . Token limit each file  (optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/davidoluyalegbenga/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')  # Download NLTK tokenizer data if not already downloaded\n",
    "\n",
    "# Function to read a file, tokenize its content, and split it into subfiles\n",
    "def split_file(filename, token_limit=500):\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "        tokens = word_tokenize(content)  # Tokenize the content\n",
    "        \n",
    "        num_tokens = len(tokens)\n",
    "        num_files = (num_tokens // token_limit) + 1\n",
    "        \n",
    "        for i in range(num_files):\n",
    "            start = i * token_limit\n",
    "            end = (i + 1) * token_limit\n",
    "            sub_tokens = tokens[start:end]  # Extract tokens for subfile\n",
    "            \n",
    "            # Create subfile name (e.g., original_filename_part1.txt, original_filename_part2.txt, ...)\n",
    "            subfile_name = f\"{os.path.splitext(filename)[0]}_part{i + 1}.txt\"\n",
    "            \n",
    "            with open(subfile_name, 'w', encoding='utf-8') as subfile:\n",
    "                subfile.write(' '.join(sub_tokens))  # Write tokens to subfile\n",
    "        \n",
    "        os.remove(filename)  # Remove the original file after splitting\n",
    "\n",
    "# Directory containing the .txt files\n",
    "directory = 'Data/cleaned_text_manually'\n",
    "\n",
    "# Loop through each file in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        split_file(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 517 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting joblib\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "\u001b[K     |████████████████████████████████| 302 kB 3.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: regex>=2021.8.3 in /Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /Users/davidoluyalegbenga/.pyenv/versions/3.9.6/lib/python3.9/site-packages (from nltk) (4.66.1)\n",
      "Collecting click\n",
      "  Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 2.2 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: joblib, click, nltk\n",
      "Successfully installed click-8.1.7 joblib-1.3.2 nltk-3.8.1\n",
      "\u001b[33mWARNING: You are using pip version 21.1.3; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the '/Users/davidoluyalegbenga/.pyenv/versions/3.9.6/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
