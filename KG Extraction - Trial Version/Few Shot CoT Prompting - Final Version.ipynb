{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24b0a798",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-09 15:31:16.890526: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "import torch, torchvision\n",
    "from transformers import AutoTokenizer, TFAutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "from span_marker import SpanMarkerModel\n",
    "from langdetect import detect\n",
    "from langdetect import LangDetectException\n",
    "import re\n",
    "import pickle\n",
    "import time\n",
    "from langchain.text_splitter import SpacyTextSplitter\n",
    "\n",
    "#nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec2dcfe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## The following requirements were added by pip freeze:\r\n",
      "absl-py==1.4.0\r\n",
      "accelerate==0.23.0\r\n",
      "aiofiles==23.2.1\r\n",
      "aiohttp==3.8.4\r\n",
      "aiosignal==1.3.1\r\n",
      "altair==5.1.2\r\n",
      "altgraph @ file:///AppleInternal/Library/BuildRoots/9dd5efe2-7fad-11ee-b588-aa530c46a9ea/Library/Caches/com.apple.xbs/Sources/python3/altgraph-0.17.2-py2.py3-none-any.whl\r\n",
      "annotated-types==0.5.0\r\n",
      "anyio==3.7.1\r\n",
      "appnope==0.1.3\r\n",
      "argon2-cffi==21.3.0\r\n",
      "argon2-cffi-bindings==21.2.0\r\n",
      "astor==0.8.1\r\n",
      "asttokens==2.2.1\r\n",
      "astunparse==1.6.3\r\n",
      "async-timeout==4.0.2\r\n",
      "asyncio==3.4.3\r\n",
      "attrs==22.1.0\r\n",
      "azureopenai==0.0.1\r\n",
      "Babel==2.11.0\r\n",
      "backcall==0.2.0\r\n",
      "beautifulsoup4==4.11.1\r\n",
      "bert-score==0.3.13\r\n",
      "bleach==5.0.1\r\n",
      "blinker==1.7.0\r\n",
      "blis==0.7.10\r\n",
      "cachetools==5.3.1\r\n",
      "catalogue==2.0.9\r\n",
      "certifi==2022.9.24\r\n",
      "cffi==1.15.1\r\n",
      "charset-normalizer==2.1.1\r\n",
      "click==8.1.3\r\n",
      "colorama==0.4.6\r\n",
      "confection==0.1.1\r\n",
      "conllu==4.5.3\r\n",
      "contourpy==1.0.6\r\n",
      "cycler==0.11.0\r\n",
      "cymem==2.0.7\r\n",
      "dataclasses-json==0.5.14\r\n",
      "datasets==2.14.5\r\n",
      "debugpy==1.6.4\r\n",
      "decorator==5.1.1\r\n",
      "defusedxml==0.7.1\r\n",
      "dill==0.3.7\r\n",
      "distro==1.9.0\r\n",
      "duckdb==0.8.1\r\n",
      "en-core-web-sm @ https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl\r\n",
      "entrypoints==0.4\r\n",
      "et-xmlfile==1.1.0\r\n",
      "eurostat==1.0.4\r\n",
      "evaluate==0.4.0\r\n",
      "exceptiongroup==1.1.3\r\n",
      "executing==1.2.0\r\n",
      "faiss-cpu==1.7.4\r\n",
      "fastapi==0.104.1\r\n",
      "fastjsonschema==2.16.2\r\n",
      "ffmpy==0.3.1\r\n",
      "filelock==3.12.2\r\n",
      "Flask==3.0.0\r\n",
      "Flask-Cors==4.0.0\r\n",
      "flatbuffers==23.5.26\r\n",
      "fonttools==4.38.0\r\n",
      "frozenlist==1.3.3\r\n",
      "fsspec==2023.6.0\r\n",
      "future @ file:///AppleInternal/Library/BuildRoots/9dd5efe2-7fad-11ee-b588-aa530c46a9ea/Library/Caches/com.apple.xbs/Sources/python3/future-0.18.2-py3-none-any.whl\r\n",
      "gast==0.4.0\r\n",
      "google-auth==2.19.1\r\n",
      "google-auth-oauthlib==1.0.0\r\n",
      "google-pasta==0.2.0\r\n",
      "gradio==4.3.0\r\n",
      "gradio_client==0.7.0\r\n",
      "graphviz==0.20.1\r\n",
      "greenlet==2.0.2\r\n",
      "grpcio==1.54.2\r\n",
      "gspread==5.12.0\r\n",
      "h11==0.14.0\r\n",
      "h5py==3.8.0\r\n",
      "httpcore==1.0.2\r\n",
      "httplib2==0.22.0\r\n",
      "httpx==0.25.1\r\n",
      "huggingface-hub==0.16.4\r\n",
      "idna==3.4\r\n",
      "importlib-metadata==5.1.0\r\n",
      "importlib-resources==6.1.1\r\n",
      "interchange==2021.0.4\r\n",
      "ipykernel==6.17.1\r\n",
      "ipython==8.18.1\r\n",
      "ipython-genutils==0.2.0\r\n",
      "ipywidgets==8.0.2\r\n",
      "isodate==0.6.1\r\n",
      "itsdangerous==2.1.2\r\n",
      "jax==0.4.11\r\n",
      "jedi==0.18.2\r\n",
      "Jinja2==3.1.2\r\n",
      "joblib==1.2.0\r\n",
      "json5==0.9.10\r\n",
      "jsonpatch==1.33\r\n",
      "jsonpointer==2.4\r\n",
      "jsonschema==4.17.3\r\n",
      "jupyter==1.0.0\r\n",
      "jupyter-console==6.4.4\r\n",
      "jupyter-contrib-core==0.4.2\r\n",
      "jupyter-contrib-nbextensions==0.7.0\r\n",
      "jupyter-highlight-selected-word==0.2.0\r\n",
      "jupyter-nbextensions-configurator==0.6.3\r\n",
      "jupyter-server==1.23.3\r\n",
      "jupyter_client==7.4.8\r\n",
      "jupyter_core==5.1.0\r\n",
      "jupyterlab==3.5.1\r\n",
      "jupyterlab-pygments==0.2.2\r\n",
      "jupyterlab-widgets==3.0.3\r\n",
      "jupyterlab_server==2.16.3\r\n",
      "keras==2.12.0\r\n",
      "Keras-Preprocessing==1.1.2\r\n",
      "kiwisolver==1.4.4\r\n",
      "langchain==0.0.279\r\n",
      "langchain-community==0.0.17\r\n",
      "langchain-core==0.1.18\r\n",
      "langchain-openai==0.0.5\r\n",
      "langcodes==3.3.0\r\n",
      "langdetect==1.0.9\r\n",
      "langsmith==0.0.86\r\n",
      "libclang==16.0.0\r\n",
      "lxml==4.9.3\r\n",
      "macholib @ file:///AppleInternal/Library/BuildRoots/9dd5efe2-7fad-11ee-b588-aa530c46a9ea/Library/Caches/com.apple.xbs/Sources/python3/macholib-1.15.2-py2.py3-none-any.whl\r\n",
      "Markdown==3.4.3\r\n",
      "markdown-it-py==3.0.0\r\n",
      "MarkupSafe==2.1.1\r\n",
      "marshmallow==3.20.1\r\n",
      "matplotlib==3.8.1\r\n",
      "matplotlib-inline==0.1.6\r\n",
      "mdurl==0.1.2\r\n",
      "mistune==2.0.4\r\n",
      "ml-dtypes==0.1.0\r\n",
      "monotonic==1.6\r\n",
      "moverscore==1.0.3\r\n",
      "mpmath==1.3.0\r\n",
      "multidict==6.0.4\r\n",
      "multiprocess==0.70.15\r\n",
      "murmurhash==1.0.9\r\n",
      "mvlearn==0.5.0\r\n",
      "mypy-extensions==1.0.0\r\n",
      "nbclassic==0.4.8\r\n",
      "nbclient==0.7.2\r\n",
      "nbconvert==7.2.6\r\n",
      "nbformat==5.7.0\r\n",
      "neo4j==5.2.1\r\n",
      "nest-asyncio==1.5.6\r\n",
      "networkx==3.1\r\n",
      "nltk==3.8.1\r\n",
      "nmslib==2.1.1\r\n",
      "notebook==6.5.2\r\n",
      "notebook_shim==0.2.2\r\n",
      "numexpr==2.8.5\r\n",
      "numpy==1.25.2\r\n",
      "oauth2client==4.1.3\r\n",
      "oauthlib==3.2.2\r\n",
      "openai==1.11.1\r\n",
      "openpyxl==3.1.2\r\n",
      "opt-einsum==3.3.0\r\n",
      "orjson==3.9.10\r\n",
      "packaging==23.2\r\n",
      "pandas==1.5.3\r\n",
      "pandasai==1.4.8\r\n",
      "pandocfilters==1.5.0\r\n",
      "pansi==2020.7.3\r\n",
      "parso==0.8.3\r\n",
      "pathy==0.10.2\r\n",
      "pexpect==4.8.0\r\n",
      "pickleshare==0.7.5\r\n",
      "Pillow==9.0.1\r\n",
      "platformdirs==2.5.4\r\n",
      "plotly==5.16.1\r\n",
      "portalocker==2.8.2\r\n",
      "preshed==3.0.8\r\n",
      "prometheus-client==0.15.0\r\n",
      "prompt-toolkit==3.0.43\r\n",
      "protobuf==4.23.2\r\n",
      "psutil==5.9.4\r\n",
      "ptyprocess==0.7.0\r\n",
      "pure-eval==0.2.2\r\n",
      "py2neo==2021.2.3\r\n",
      "pyarrow==13.0.0\r\n",
      "pyasn1==0.5.0\r\n",
      "pyasn1-modules==0.3.0\r\n",
      "pybind11==2.6.1\r\n",
      "pycountry==22.3.5\r\n",
      "pycparser==2.21\r\n",
      "pydantic==1.10.14\r\n",
      "pydantic_core==2.6.1\r\n",
      "pydevd-pycharm==223.7571.175\r\n",
      "pydub==0.25.1\r\n",
      "pyemd==1.0.0\r\n",
      "Pygments==2.13.0\r\n",
      "pyparsing==3.0.9\r\n",
      "pyrsistent==0.19.2\r\n",
      "pysbd==0.3.4\r\n",
      "python-dateutil==2.8.2\r\n",
      "python-dotenv==1.0.0\r\n",
      "python-multipart==0.0.6\r\n",
      "pytz==2022.6\r\n",
      "PyYAML==6.0.1\r\n",
      "pyzmq==24.0.1\r\n",
      "qtconsole==5.4.0\r\n",
      "QtPy==2.3.0\r\n",
      "rdflib==7.0.0\r\n",
      "regex==2023.8.8\r\n",
      "requests==2.28.1\r\n",
      "requests-oauthlib==1.3.1\r\n",
      "responses==0.18.0\r\n",
      "rich==13.7.0\r\n",
      "rsa==4.9\r\n",
      "safetensors==0.3.2\r\n",
      "scikit-learn==1.1.3\r\n",
      "scipy==1.9.3\r\n",
      "scispacy==0.5.3\r\n",
      "SDGDetector @ git+https://gitlab.com/netmode/sdg-detector.git@6baf1bab53dda789340e9a3ee7f3dd1a698bf4d1\r\n",
      "seaborn==0.12.1\r\n",
      "semantic-version==2.10.0\r\n",
      "Send2Trash==1.8.0\r\n",
      "sentence-transformers==2.2.2\r\n",
      "sentencepiece==0.1.99\r\n",
      "seqeval==1.2.2\r\n",
      "shellingham==1.5.4\r\n",
      "six @ file:///AppleInternal/Library/BuildRoots/9dd5efe2-7fad-11ee-b588-aa530c46a9ea/Library/Caches/com.apple.xbs/Sources/python3/six-1.15.0-py2.py3-none-any.whl\r\n",
      "sklearn==0.0.post1\r\n",
      "smart-open==6.3.0\r\n",
      "sniffio==1.3.0\r\n",
      "soupsieve==2.3.2.post1\r\n",
      "spacy==3.6.1\r\n",
      "spacy-legacy==3.0.12\r\n",
      "spacy-loggers==1.0.4\r\n",
      "span-marker==1.4.0\r\n",
      "SPARQLWrapper==2.0.0\r\n",
      "spicy==0.16.0\r\n",
      "SQLAlchemy==1.4.51\r\n",
      "srsly==2.4.7\r\n",
      "stack-data==0.6.2\r\n",
      "starlette==0.27.0\r\n",
      "sympy==1.12\r\n",
      "tenacity==8.2.3\r\n",
      "tensorboard==2.12.3\r\n",
      "tensorboard-data-server==0.7.0\r\n",
      "tensorflow==2.12.0\r\n",
      "tensorflow-estimator==2.12.0\r\n",
      "tensorflow-io-gcs-filesystem==0.32.0\r\n",
      "termcolor==2.3.0\r\n",
      "terminado==0.17.1\r\n",
      "thinc==8.1.12\r\n",
      "threadpoolctl==3.1.0\r\n",
      "tiktoken==0.5.2\r\n",
      "tinycss2==1.2.1\r\n",
      "tokenizers==0.13.3\r\n",
      "tomli==2.0.1\r\n",
      "tomlkit==0.12.0\r\n",
      "toolz==0.12.0\r\n",
      "torch==2.0.1\r\n",
      "torchvision==0.15.2\r\n",
      "tornado==6.2\r\n",
      "tqdm==4.66.1\r\n",
      "traitlets==5.6.0\r\n",
      "transformers==4.31.0\r\n",
      "typer==0.9.0\r\n",
      "typing==3.7.4.3\r\n",
      "typing-inspect==0.9.0\r\n",
      "typing_extensions==4.8.0\r\n",
      "umap==0.1.1\r\n",
      "urllib3==1.26.13\r\n",
      "uvicorn==0.24.0.post1\r\n",
      "wasabi==1.1.2\r\n",
      "wcwidth==0.2.5\r\n",
      "webencodings==0.5.1\r\n",
      "websocket-client==1.4.2\r\n",
      "websockets==11.0.3\r\n",
      "Werkzeug==3.0.1\r\n",
      "widgetsnbextension==4.0.3\r\n",
      "wrapt==1.14.1\r\n",
      "xgboost==1.7.5\r\n",
      "xlrd==2.0.1\r\n",
      "xxhash==3.4.1\r\n",
      "yarl==1.9.2\r\n",
      "zipp==3.11.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip3 freeze -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d297b4e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python-dotenv could not parse statement starting at line 10\n",
      "Python-dotenv could not parse statement starting at line 11\n",
      "Python-dotenv could not parse statement starting at line 12\n",
      "Python-dotenv could not parse statement starting at line 13\n",
      "Python-dotenv could not parse statement starting at line 14\n",
      "Python-dotenv could not parse statement starting at line 15\n",
      "Python-dotenv could not parse statement starting at line 16\n",
      "Python-dotenv could not parse statement starting at line 17\n",
      "Python-dotenv could not parse statement starting at line 18\n",
      "Python-dotenv could not parse statement starting at line 19\n",
      "Python-dotenv could not parse statement starting at line 20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfc59a5",
   "metadata": {},
   "source": [
    "Initialize Entity Categories and Relation Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f5423cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\n",
    "    \n",
    "    \"Person\",\n",
    "    \"Location\",\n",
    "    \"Organization\",\n",
    "    \"Event\",\n",
    "    \"Product\",\n",
    "    \"Project\",\n",
    "    \"Skill\",\n",
    "    \"Strategy\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffa069f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_labels = [\n",
    "    \"implements\",\n",
    "    \"funds\",\n",
    "    \"focuses_on\",\n",
    "    \"in\",\n",
    "    \"partners_with\",\n",
    "    \"contributes_to\",\n",
    "    \"monitors\",\n",
    "    \"targets\",\n",
    "    \"addresses\",\n",
    "    \"employs\",\n",
    "    \"collaborates_with\",\n",
    "    \"supports\",\n",
    "    \"administers\",\n",
    "    \"measures\",\n",
    "    \"aligns_with\",\n",
    "    \"an_instance_of\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e5f1a1",
   "metadata": {},
   "source": [
    "# Setting up OpenAI connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a472f8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI API configuration\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai.api_base = os.getenv(\"OPENAI_API_BASE\")\n",
    "openai.api_version = os.getenv(\"OPENAI_API_VERSION\")\n",
    "openai_deployment = \"sdgi-gpt-35-turbo-16k\"\n",
    "\n",
    "neo4j_pass = os.getenv(\"NEO4JPASS\")\n",
    "#openai.api_key = os.getenv(\"OPENAI_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76acd36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_answer(user_question, timeout_seconds):\n",
    "    messages = [\n",
    "        {'role': 'user', 'content': user_question},\n",
    "    ]\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            engine=\"sdgi-gpt-35-turbo-16k\", \n",
    "            messages=messages,\n",
    "            temperature=0.2,\n",
    "            request_timeout = timeout_seconds\n",
    "            # max_tokens=2000\n",
    "        )\n",
    "        return response.choices[0].message[\"content\"]\n",
    "    except requests.Timeout:\n",
    "        print(f\"Request timed out\")\n",
    "        return []\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e328a35",
   "metadata": {},
   "source": [
    "# Entity Extraction using Transformers \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f369316f",
   "metadata": {},
   "outputs": [],
   "source": [
    "WIKI_API = \"https://api-inference.huggingface.co/models/Babelscape/wikineural-multilingual-ner\"\n",
    "BERT_API = \"https://api-inference.huggingface.co/models/dslim/bert-base-NER\"\n",
    "\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "def query_wiki(payload):\n",
    "\tresponse = requests.post(WIKI_API, headers=headers, json=payload)\n",
    "\treturn response.json()\n",
    "\t\n",
    "\n",
    "def query_bert(payload):\n",
    "\tresponse = requests.post(BERT_API, headers=headers, json=payload)\n",
    "\treturn response.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb268621",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_gpt(text):\n",
    "    \n",
    "    entities_prompt = f\"\"\"\n",
    "\n",
    "    You will be given a >>>>>TEXT<<<<<. You have two tasks:\n",
    "    \n",
    "    1. Your first task is to detect acronyms with their names and store them in python dictionary.\n",
    "    2. Your second task is to detect Proper Nouns in the text and store them in python list.\n",
    "    \n",
    "    Return a JSON array contaning dictionary and the list.\n",
    "\n",
    "    >>>>>TEXT<<<<<\n",
    "    {text}\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    #start_time = time.time()\n",
    "\n",
    "    result = get_answer(entities_prompt, 10)\n",
    "    result = json.loads(result)\n",
    "    \n",
    "    #end_time = time.time()\n",
    "    #elapsed_time = end_time - start_time\n",
    "    #print (f\"TIME TAKEN TO EXECUTE PROMPT: {elapsed_time}\")\n",
    "    return result\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4160364",
   "metadata": {},
   "source": [
    "# Text Pre-Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b140f501",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_spacy(chunk_size, text):\n",
    "    \n",
    "    text_splitter = SpacyTextSplitter(chunk_size=chunk_size)\n",
    "    sections = text_splitter.split_text(text)\n",
    "    \n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a13f4641",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_section(limit, text):\n",
    "    sections_list = []\n",
    "    length = len(text)\n",
    "    i = 0\n",
    "\n",
    "    while i < length - 1:\n",
    "        j = i + limit\n",
    "\n",
    "        if j >= length:\n",
    "            j = length - 1\n",
    "        elif text[j] not in ('.', '\\n', ';'):\n",
    "            while text[j] not in ('.', '\\n', ';'):\n",
    "                j -= 1\n",
    "            j += 1\n",
    "\n",
    "        section = text[i:j]\n",
    "\n",
    "        if is_valid_section(section):\n",
    "            sections_list.append(section)\n",
    "        else: \n",
    "            print(\"INVALID SECTION DETECTED\")\n",
    "            print(section)\n",
    "            #section_list[-1].extend(section)\n",
    "        i = j\n",
    "    \n",
    "    \n",
    "    return sections_list\n",
    "\n",
    "def is_valid_section(section):\n",
    "    return section and len(section) > 20\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4066d417",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(input_text):\n",
    "    # Remove lines with only whitespace\n",
    "    input_text = re.sub(r'^\\s*$', '', input_text, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove lines containing only uppercase text (potential headings)\n",
    "    input_text = re.sub(r'^\\s*[A-Z\\s]+\\s*$', '', input_text, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove lines with multiple consecutive uppercase words (potential headings)\n",
    "    input_text = re.sub(r'^\\s*(?:[A-Z]+\\s*){2,}\\s*$', '', input_text, flags=re.MULTILINE)\n",
    "    \n",
    "    input_text = re.sub(r'^\\s*[A-Za-z\\s]+\\.{3,}\\s*\\d+\\s*$', '', input_text, flags=re.MULTILINE)\n",
    "\n",
    "    return input_text\n",
    "\n",
    "def is_english(line):\n",
    "    try:\n",
    "        return detect(line) == 'en'\n",
    "    except LangDetectException as e:\n",
    "        print(f\"An exception occurred: {e} : {line}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38eb33c",
   "metadata": {},
   "source": [
    "# Entities Post-Processing Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec7e10db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging the broken entities\n",
    "def create_entities(lst):\n",
    "    i = 1\n",
    "    while i < len(lst):\n",
    "        if lst[i][\"word\"].startswith('##'):\n",
    "            lst[i][\"word\"] = lst[i-1][\"word\"] + lst[i][\"word\"][2:]\n",
    "            lst[i][\"score\"] = max(lst[i-1][\"score\"] , lst[i][\"score\"])\n",
    "            del lst[i-1]\n",
    "        else:\n",
    "            i += 1\n",
    "            # todo: return a list of merged entities\n",
    "            \n",
    "\n",
    "\n",
    "def apply_threshold(list_, threshold):\n",
    "    words_list = []\n",
    "    for item in list_:\n",
    "        if item['score'] > threshold:  # threshold score to eliminate unimportant entities\n",
    "            words_list.append(item['word'])\n",
    "    return words_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f27e0af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw(list_):\n",
    "    output = []\n",
    "    for sublist in list_:\n",
    "        new = []\n",
    "        obj = {}\n",
    "        for item in sublist:\n",
    "            #obj = {}\n",
    "            key = ''.join(filter(str.isalpha, item))\n",
    "            obj[key]= item\n",
    "            #obj['raw']= ''.join(filter(str.isalpha, item))\n",
    "        output.append(obj)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5c833db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_extracted_entities_old(wiki, bert, gpt):\n",
    "    \n",
    "    output = []\n",
    "    dict_ = gpt\n",
    "    \n",
    "    \n",
    "    #print(dict_.items())\n",
    "    wiki_set = set(wiki.keys())\n",
    "    #bert_set = set(bert.keys())\n",
    "    gpt_set = set(gpt.keys())\n",
    "    \n",
    "    \n",
    "    #A = gpt_set.intersection(bert_set)\n",
    "    #B = bert_set.intersection(wiki_set)\n",
    "    C = wiki_set.intersection(gpt_set)\n",
    " \n",
    "    #matched = list(A.union(B).union(C))\n",
    "    \n",
    "    \n",
    "    for i in C:\n",
    "        output.append(dict_[i])\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e749549e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_extracted_entities(wiki, bert, gpt):\n",
    "    \n",
    "    output = set(wiki.values())\n",
    "    dict_ = gpt\n",
    "    \n",
    "    bert_set = set(bert.keys()) - set(wiki.keys())\n",
    "    gpt_set = set(gpt.keys())\n",
    "    \n",
    "    A = gpt_set.intersection(bert_set)\n",
    "\n",
    "    matched = list(set(A))\n",
    "    print (\"GPT/BERT: \" + str(matched))\n",
    "\n",
    "    for i in matched:\n",
    "        output.add(dict_[i])\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ff671cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_entities(list_):\n",
    "    \n",
    "    # Define a regular expression pattern to match invalid characters.\n",
    "    pattern = r'\\s*{}\\s*'.format(re.escape(\"’\"))\n",
    "    pattern1 = r'\\s*{}\\s*'.format(re.escape(\"/\"))\n",
    "    output_list = []\n",
    "\n",
    "    for item in list_:\n",
    "        item = re.sub(pattern, \"’\", item)\n",
    "        tem = re.sub(pattern1, \"/\", item)\n",
    "            \n",
    "    return output_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f074fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(index, wiki, bert, gpt, acronym):\n",
    "    checkpoint = {'index': index, 'wiki': wiki, 'bert': bert, 'gpt': gpt, 'acronym': acronym}\n",
    "    with open('checkpoint.pkl', 'wb') as checkpoint_file:\n",
    "        pickle.dump(checkpoint, checkpoint_file)\n",
    "\n",
    "# Function to load the state\n",
    "def load_checkpoint(length):\n",
    "    try:\n",
    "        with open('checkpoint.pkl', 'rb') as checkpoint_file:\n",
    "            checkpoint = pickle.load(checkpoint_file)\n",
    "            return checkpoint['index'], checkpoint['wiki'], checkpoint['bert'], \n",
    "        checkpoint['gpt'], checkpoint['acronym']\n",
    "    except FileNotFoundError:\n",
    "        return 0, [''] * length, [''] * length, [''] * length, {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b6c56a",
   "metadata": {},
   "source": [
    "#  Categorize entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "197cdc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_entities(text, entities, categories):\n",
    "    \n",
    "    \n",
    "    categorization_prompt = f\"\"\"\n",
    "\n",
    "    You will be given a >>>>>TEXT<<<<<, an >>>>>EntityList<<<<< and >>>>>Categories<<<<<. \n",
    "    Your task is to assign a sutiable category to each element of >>>>>EntityList<<<<<.\n",
    "    \n",
    "    Return a list of JSON objects of categorized entities. \n",
    "\n",
    "\n",
    "    >>>>>TEXT<<<<<\n",
    "    {text}\n",
    "\n",
    "    >>>>>Categories<<<<<\n",
    "    {categories}\n",
    "\n",
    "    >>>>>EntityList<<<<<\n",
    "    {entities}\n",
    "    \"\"\"\n",
    "\n",
    "    categorized_entities = get_answer(categorization_prompt, 30)\n",
    "    categorized_entities = json.loads(categorized_entities)\n",
    "    \n",
    "    return (categorized_entities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0d1e03",
   "metadata": {},
   "source": [
    "# Relation Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ebf0d4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_relation_details(text, entities, relation_labels):\n",
    "    relation_extraction_prompt = f\"\"\"\n",
    "    \n",
    "    [Context]\n",
    "    You belong to a team of consultants at UNDP's Sustainable Energy Hub (SEH), working on a project to extract a \n",
    "    Knowledge Graph from the UNDP dataset.\n",
    "    You will be given a >>>>>TEXT<<<<<, an >>>>>EntityList<<<<< and a list of >>>>>RelationLabels<<<<<.\n",
    "\n",
    "   [Task]\n",
    "   \n",
    "   Your task is to perform Relation Extraction on the given >>>>>TEXT<<<<< \n",
    "   to find relations between elements of provided >>>>>EntityList<<<<<.\n",
    "   \n",
    "   Please make sure to read these instructions and constraints carefully.\n",
    "\n",
    "    [Instructions]\n",
    "    1. Carefully read and store the >>>>>RelationLabels<<<<<.\n",
    "    2. Scan the >>>>>TEXT<<<<< to find Named Entites from >>>>>EntityList<<<<< that are related.\n",
    "    3. Scan the >>>>>RelationLabels<<<<< to select a suitable label to\n",
    "    describe the relation between the above selected entities. Mark this label as \"Relation\".\n",
    "    4. Assign \"Subject\" and \"Object\" to entities depending on the selected \"Relation\"\n",
    "    selected in previous step to create a tuple.\n",
    "    5. If available, select a small \"Description\" from the >>>>>TEXT<<<<< for the above relation.\n",
    "    6. Assign a Relevance score between 1 to 10 to the extracted relation, with 10 being the most relevant.\n",
    "    7. Repeat the process to extract remaining relations from >>>>>TEXT<<<<<.\n",
    "    \n",
    "    \n",
    "    [Constraints]\n",
    "    1. Values of 'Relation' key should belong to >>>>>RelationLabels<<<<<.\n",
    "    \n",
    "    [Output Format]\n",
    "    Provide the result as a JSON array.\n",
    "\n",
    "    Perform relation extraction on the below:\n",
    "    \n",
    "    >>>>>TEXT<<<<<\n",
    "    {text}\n",
    "\n",
    "    >>>>>EntityList<<<<<\n",
    "    {entities}\n",
    "\n",
    "    >>>>>RelationLabels<<<<<\n",
    "    {relation_labels}\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "    relations = get_answer(relation_extraction_prompt,60)\n",
    "    relations = json.loads(relations)\n",
    "\n",
    "    return relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513a5f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ontology_relations(text, entities, ontology):\n",
    "    relation_extraction_prompt = f\"\"\"\n",
    "    \n",
    "    [Context]\n",
    "    You belong to a team of consultants at UNDP's Sustainable Energy Hub (SEH), working on a project to extract a \n",
    "    Knowledge Graph from the UNDP dataset.\n",
    "    You will be given a >>>>>TEXT<<<<<, an >>>>>EntityList<<<<< and an >>>>>ONTOLOGY<<<<<.\n",
    "\n",
    "   [Task]\n",
    "   \n",
    "   Your task is to perform Relation Extraction on the given >>>>>TEXT<<<<< \n",
    "   to find relations between elements of provided >>>>>EntityList<<<<<. Use the given >>>>>ONTOLOGY<<<<<\n",
    "   for this purpose.\n",
    "   \n",
    "   Please make sure to read these instructions and constraints carefully.\n",
    "\n",
    "    [Instructions]\n",
    "    1. Carefully read and understand the >>>>>ONTOLOGY<<<<<.\n",
    "    2. Scan the >>>>>TEXT<<<<< to find Named Entites in >>>>>EntityList<<<<< that are related.\n",
    "    3. Read the >>>>>ONTOLOGY<<<<< to select a relationship type for the related entities. Mark this label as \"Relation\".\n",
    "    4. Assign \"Subject\" and \"Object\" to entities depending on the \"Relation\"\n",
    "    selected in previous step to create a tuple.\n",
    "    5. If available, select a small \"Description\" from the >>>>>TEXT<<<<< for the above relation.\n",
    "    6. Assign a Relevance score between 1 to 10 to the extracted relation, with 10 being the most relevant.\n",
    "    7. Repeat the process to extract remaining relations from >>>>>TEXT<<<<<.\n",
    "    \n",
    "    \n",
    "    [Constraints]\n",
    "    1. Values of 'Relation' key should be a label from properties in >>>>>ONTOLOGY<<<<<.\n",
    "    \n",
    "    [Output Format]\n",
    "    Provide the result as a JSON array.\n",
    "\n",
    "    Perform relation extraction on the below:\n",
    "    \n",
    "    >>>>>TEXT<<<<<\n",
    "    {text}\n",
    "\n",
    "    >>>>>EntityList<<<<<\n",
    "    {entities}\n",
    "\n",
    "    >>>>>ONTOLOGY<<<<<\n",
    "    {ontology}\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "    relations = get_answer(relation_extraction_prompt,60)\n",
    "    relations = json.loads(relations)\n",
    "\n",
    "    return relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ffc4f23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ontology.ttl\") as f:\n",
    "    ontology = f.read()\n",
    "    \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4862fd69",
   "metadata": {},
   "source": [
    "# Connecting with DBpedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9a2a53f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "# Define the DBpedia SPARQL endpoint\n",
    "sparql_endpoint = \"http://dbpedia.org/sparql\"\n",
    "\n",
    "# Create a SPARQLWrapper instance\n",
    "sparql = SPARQLWrapper(sparql_endpoint)\n",
    "\n",
    "# Function to search for an entity by label and return its DBpedia URI\n",
    "def search_entity(label):\n",
    "    query = f\"\"\"\n",
    "    SELECT ?entity\n",
    "    WHERE {{\n",
    "      ?entity rdfs:label \"{label}\"@en.\n",
    "    }}\n",
    "    LIMIT 1\n",
    "    \"\"\"\n",
    "\n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "\n",
    "    results = sparql.query().convert()\n",
    "\n",
    "    if \"results\" in results and \"bindings\" in results[\"results\"] and results[\"results\"][\"bindings\"]:\n",
    "        entity_uri = results[\"results\"][\"bindings\"][0][\"entity\"][\"value\"]\n",
    "        return entity_uri\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Function to retrieve and return the abstract or comment of an entity\n",
    "def retrieve_entity_summary(entity_uri):\n",
    "    # Try to retrieve the abstract\n",
    "    abstract_query = f\"\"\"\n",
    "    SELECT ?abstract\n",
    "    WHERE {{\n",
    "      <{entity_uri}> dbo:abstract ?abstract.\n",
    "      FILTER (LANGMATCHES(LANG(?abstract), \"en\"))\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    sparql.setQuery(abstract_query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "\n",
    "    abstract_results = sparql.query().convert()\n",
    "\n",
    "    if \"results\" in abstract_results and \"bindings\" in abstract_results[\"results\"]:\n",
    "        for result in abstract_results[\"results\"][\"bindings\"]:\n",
    "            abstract = result[\"abstract\"][\"value\"]\n",
    "            return abstract\n",
    "\n",
    "    # If abstract is not found, try to retrieve the comment\n",
    "    comment_query = f\"\"\"\n",
    "    SELECT ?comment\n",
    "    WHERE {{\n",
    "      <{entity_uri}> rdfs:comment ?comment.\n",
    "      FILTER (LANGMATCHES(LANG(?comment), \"en\"))\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    sparql.setQuery(comment_query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "\n",
    "    comment_results = sparql.query().convert()\n",
    "\n",
    "    if \"results\" in comment_results and \"bindings\" in comment_results[\"results\"]:\n",
    "        for result in comment_results[\"results\"][\"bindings\"]:\n",
    "            comment = result[\"comment\"][\"value\"]\n",
    "            return comment\n",
    "\n",
    "    # If neither abstract nor comment is found, return None\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c7c57444",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.error\n",
    "\n",
    "def dbpedia_summary(search_label):\n",
    "    entity_uri = search_entity(search_label)\n",
    "\n",
    "    if entity_uri:\n",
    "        print(f\"Entity found with DBpedia URI: {entity_uri}\")\n",
    "        try:\n",
    "            summary = retrieve_entity_summary(entity_uri)\n",
    "            if summary:\n",
    "                return summary\n",
    "            else:\n",
    "                print(\"No abstract or comment found for this entity.\")\n",
    "        except urllib.error.URLError as e:\n",
    "            print(f\"Error: {e}\")\n",
    "    else:\n",
    "        print(f\"No entity found with the label: {search_label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5be620ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_summaries(entities):\n",
    "    \n",
    "    updated_entities = []\n",
    "\n",
    "    for item in entities:\n",
    "        try:\n",
    "            summary = dbpedia_summary(item['entity'])\n",
    "            if summary:\n",
    "                # Only add the summary if it's not None or empty\n",
    "                item['summary'] = summary\n",
    "            updated_entities.append(item)  # Add the item regardless of summary presence\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return updated_entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7975f7",
   "metadata": {},
   "source": [
    "# Creating Graph in Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d2cd7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph, Node, Relationship\n",
    "graph = Graph(uri = 'bolt://localhost:7687',user='neo4j',password=neo4j_pass)\n",
    "\n",
    "class Document:\n",
    "    def __init__(self, metadata, entities, relations):\n",
    "        self.metadata = metadata\n",
    "        self.entities = entities\n",
    "        self.relations = relations\n",
    "\n",
    "# Define a function to create or retrieve a node\n",
    "def get_or_create_node(label, key, value):\n",
    "    # Attempt to find an existing node with the given label and key\n",
    "    existing_node = get_node(label, key, value)\n",
    "    if not existing_node:\n",
    "        existing_node = get_node(label, 'acronym', value)\n",
    "    \n",
    "    if existing_node:\n",
    "        return existing_node\n",
    "    else:\n",
    "        new_node = Node(label, **{key: value})\n",
    "        graph.create(new_node)\n",
    "        return new_node\n",
    "    \n",
    "def get_node(label, key, value):\n",
    "    node = graph.nodes.match(label, **{key:value}).first()\n",
    "    return node\n",
    "def get_node_without_label(key, value):\n",
    "    node = graph.nodes.match(**{key:value}).first()\n",
    "    return node\n",
    "\n",
    "# Define a function to insert relations \n",
    "def insert_relations_neo4j(entities, relations):\n",
    "    #document_node = get_or_create_node(\"Document\", \"name\", document.metadata['Document Title'] )\n",
    "            \n",
    "    #for key,value in metadata.items():\n",
    "     #   document_node[key] = value\n",
    "        \n",
    "    #graph.push(document_node)\n",
    "    \n",
    "    for item in entities:\n",
    "        \n",
    "        node = get_or_create_node('Entity', \"name\", item[\"entity\"])\n",
    "        node['category'] = item[\"category\"]\n",
    "        if \"acronym\" in item:\n",
    "            node['acronym'] = item[\"acronym\"]\n",
    "        if \"summary\" in item:\n",
    "            node['summary'] = item[\"summary\"]\n",
    "        \n",
    "        graph.push(node)\n",
    "        #entity_rel = Relationship(node, 'parent_document', document_node)\n",
    "        \n",
    "        #if \"information\" in item:\n",
    "         #   relation[\"description\"] = item['information']\n",
    "            \n",
    "        #graph.create(relation)\n",
    "        \n",
    "    for item in relations:\n",
    "        subject = get_or_create_node( \"Entity\", \"name\", item[\"Subject\"])\n",
    "        obj = get_or_create_node(\"Entity\", \"name\", item[\"Object\"])\n",
    "        relation = Relationship(subject, item[\"Relation\"], obj)\n",
    "        if 'Description' in item:\n",
    "            relation[\"description\"] = item[\"Description\"]\n",
    "        \n",
    "        # Merge nodes and create relationships\n",
    "        #graph.merge(subject, \"name\")\n",
    "        #graph.merge(obj, \"name\")\n",
    "        graph.create(relation)\n",
    "        \n",
    "        # Link the nodes to the project node\n",
    "        #graph.create(Relationship(subject, \"Belongs To\", document_node))\n",
    "        #graph.create(Relationship(obj, \"Belongs To\", document_node))\n",
    "        \n",
    "    \n",
    "# Define a function to insert summaries \n",
    "def insert_summary_neo4j(data):\n",
    "    for item in data:\n",
    "        node = get_node(\"Entity\", \"name\", item.name)\n",
    "        node[\"Summary\"] = item.summary\n",
    "        graph.push(node)\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af30def",
   "metadata": {},
   "source": [
    "# Extraction Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd35416",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = ('Data/')\n",
    "file_list = os.listdir(folder_path)\n",
    "\n",
    "# Filter the list to include only text files (e.g., .txt files)\n",
    "text_files = [file for file in file_list if file.endswith(\".txt\")]\n",
    "\n",
    "print (f\"Number of files: {len(text_files)}\\n\")  \n",
    "print (text_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644ec132",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(folder_path, text_files[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32344bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (file_path, 'r') as file:\n",
    "    head = [next(file) for _ in range(11)]\n",
    "    next(file)\n",
    "    raw_text = file.read()\n",
    "    \n",
    "    file.close()\n",
    "\n",
    "print (f\"Original text length: {len(raw_text)}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d8fa1e",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Open the file in read mode\n",
    "with open(file_path, 'r') as file:\n",
    "    \n",
    "    pattern = re.compile(r'.*?\\.{3}.*?$', re.MULTILINE)\n",
    "    # Initialize an empty string to store the lines\n",
    "    raw_text = ''\n",
    "    \n",
    "    head = [next(file) for _ in range(11)]\n",
    "    next(file)\n",
    "    \n",
    "    # Iterate over each line in the file\n",
    "    for line in file:\n",
    "        # Append the current line to the string\n",
    "        \n",
    "        if not pattern.search(line) and is_english(line):\n",
    "            raw_text += line\n",
    "            \n",
    "print(f\"Read text length: {len(raw_text)}\") \n",
    "\n",
    "text = clean_text(raw_text)\n",
    "\n",
    "print(f\"Cleaned text length: {len(text)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17194e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = {}\n",
    "\n",
    "# Iterate through the data list\n",
    "for item in head:\n",
    "    # Split each element by ':' and strip the resulting strings\n",
    "    key, value = item.split(':')\n",
    "    key = key.strip()\n",
    "    value = value.strip()\n",
    "    \n",
    "    # Add the key-value pair to the dictionary\n",
    "    metadata[key] = value\n",
    "\n",
    "\n",
    "if 'Exists?' in metadata:\n",
    "    metadata.pop('Exists?')\n",
    "print(metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e918387c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_sections = split_text_spacy(2000, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68cb471",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (f\"The number of sections from the text: {len(text_sections)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a9cc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_length = len(text_sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fb4f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the last checkpoint\n",
    "#start_index, wiki_entity_list, bert_entity_list, gpt_entity_list, acronyms = load_checkpoint(text_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d21a6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_index = 0\n",
    "wiki_entity_list = [''] * text_length\n",
    "bert_entity_list = [''] * text_length\n",
    "gpt_entity_list = [''] * text_length\n",
    "acronyms = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8df97aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue from the last checkpoint\n",
    "\n",
    "start_time = time.time()\n",
    "for index in range(8,12):\n",
    "    try:\n",
    "        segment = text_sections[index]\n",
    "        \n",
    "        ## WIKINEURAL BILINGUAL MODEL\n",
    "        wiki_output = query_wiki({\n",
    "            \"inputs\": segment,\n",
    "        })\n",
    "        create_entities(wiki_output)\n",
    "        wiki_words = list(set(apply_threshold(wiki_output, 0.7)))\n",
    "        wiki_entity_list[index] = wiki_words\n",
    "        print (\"WIKI DONE\")\n",
    "\n",
    "        ## BERT BASE MODEL\n",
    "        bert_output = query_bert({\n",
    "            \"inputs\": segment,\n",
    "        })\n",
    "        create_entities(bert_output)\n",
    "        bert_words = list(set(apply_threshold(bert_output, 0.7)))\n",
    "        bert_entity_list[index] = bert_words\n",
    "        print (\"BERT DONE\")\n",
    "\n",
    "\n",
    "        ## GPT PROMPT\n",
    "        gpt_output = query_gpt(segment)\n",
    "        gpt_entity_list[index] = gpt_output['proper_nouns']\n",
    "\n",
    "        print (\"GPT DONE\")\n",
    "\n",
    "        ## Acronyms extraction\n",
    "        acronyms.update(gpt_output['acronyms'])\n",
    "    \n",
    "        \n",
    "        print(f\"NUMBER OF PROCESSED SECTIONS: {index}\")\n",
    "\n",
    "        # Save checkpoint at intervals\n",
    "        #if index % 5 == 0:\n",
    "            #save_checkpoint(index, wiki_entity_list, bert_entity_list, gpt_entity_list, acronyms)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing section {index}: {str(e)}\")\n",
    "        #save_checkpoint(index, wiki_entity_list, bert_entity_list, gpt_entity_list, acronyms)\n",
    "\n",
    "        continue  # Exit the loop in case of an error\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"TIME TAKEN TO EXTRACT ENTITIES from {text_length} section: {elapsed_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df46581",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (acronyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd9ac20",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "while i < text_length:\n",
    "    #print (text_sections[i])\n",
    "    print (wiki_entity_list[i])\n",
    "    print (bert_entity_list[i])\n",
    "    print (gpt_entity_list[i])\n",
    "    print (\"--------\")\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a46f014",
   "metadata": {},
   "source": [
    "Processing the Entities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017e28b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get raw version of entities for comparison\n",
    "raw_wiki = get_raw(wiki_entity_list)\n",
    "raw_bert = get_raw(bert_entity_list)\n",
    "raw_gpt = get_raw(gpt_entity_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349b1d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_objects = []\n",
    "entity_filter = []\n",
    "merged = []\n",
    "i = 0\n",
    "\n",
    "while i < len(wiki_entity_list):\n",
    "    merged = merge_extracted_entities_old(raw_wiki[i], raw_bert[i], raw_gpt[i])\n",
    "    print (f\"\\nThe number of matching entities in section {i}: {len(merged)}\\n\")\n",
    "    print (merged)\n",
    "    \n",
    "    print (\"\\n--------------\")\n",
    "    entity_filter.extend(merged)\n",
    "    entity_objects.append(merged)\n",
    "    \n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dbc949",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(entity_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36dc7041",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a03230ac",
   "metadata": {},
   "source": [
    "Chain of Thought - Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c43b8d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# invert acronyms dict to ease look up\n",
    "acronyms_dict = {v: k for k, v in acronyms.items()}\n",
    "print (acronyms_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ab435a",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(entity_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d730afd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_list_unfiltered = []\n",
    "relations_list = []\n",
    "entities_with_sections = []\n",
    "relations_with_section = []\n",
    "seen_entities = set()\n",
    "seen_acronyms = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61fea8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for index, uncategorized_entities in enumerate(entity_objects):\n",
    "    try:\n",
    "        entities_subset = categorize_entities(text_sections[index], uncategorized_entities, categories)\n",
    "        #print(seen_acronyms)\n",
    "\n",
    "        # Add 'acronym' key to entity list\n",
    "        for item in entities_subset:\n",
    "            if item[\"entity\"] not in seen_entities and item[\"entity\"] not in seen_acronyms:\n",
    "                seen_entities.add(item[\"entity\"])\n",
    "                \n",
    "                \n",
    "                if item[\"entity\"] in acronyms_dict.keys():\n",
    "                    item[\"acronym\"] = acronyms_dict[item[\"entity\"]]\n",
    "                    seen_acronyms.add(item['acronym'])\n",
    "                    #print(\"SEEN ACRONYM\" + str(item))\n",
    "                    \n",
    "                elif item['entity'] in acronyms.keys():\n",
    "                    item[\"acronym\"] = item['entity']\n",
    "                    item[\"entity\"] = acronyms[item[\"entity\"]]\n",
    "                    seen_acronyms.add(item['acronym'])\n",
    "                    #print(\"SEEN ACRONYM\" + str(item))\n",
    "                    \n",
    "                entities_list_unfiltered.append(item)\n",
    "\n",
    "        print (\"CATEGORIZED ENTITIES of Section: \" + str(index) + \"\\n\")\n",
    "        print (entities_subset)\n",
    "        #store the categorized entities in order of lists for later processing\n",
    "        entities_with_sections.append(entities_subset)\n",
    "\n",
    "        relations_subset = extract_ontology_relations(text_sections[index], entity_objects[index], ontology)\n",
    "\n",
    "        print (\"\\n EXTRACTED RELATIONS: \\n\")\n",
    "        print (relations_subset)\n",
    "\n",
    "        relations_list.extend(relations_subset)\n",
    "        relations_with_section.append(relations_subset)\n",
    "\n",
    "        print (\"\\n-------------------\")\n",
    "\n",
    "    except Exception as e:\n",
    "            print(f\"Error processing section {index}: {str(e)}\")\n",
    "            #save_checkpoint(index, wiki_entity_list, bert_entity_list, gpt_entity_list, acronyms)\n",
    "\n",
    "    continue  # Exit the loop in case of an error\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"TIME TAKEN TO EXTRACT RELATIONS FROM {text_length} SECTIONS: {elapsed_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a973dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(entities_list_unfiltered))\n",
    "print (len(relations_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874c94e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_list = []\n",
    "for i in entities_list_unfiltered:\n",
    "    if i['entity'] in entity_filter:\n",
    "        entities_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3de5f98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "entity_summaries = extract_summaries(entities_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cff8b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(entity_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e264dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a list of names, to check for valid relations\n",
    "\n",
    "entity_names = set([item['entity'] for item in entities_list])\n",
    "entity_names.update([item['acronym'] for item in entities_list if 'acronym' in item ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e409383f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (len(acronyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5915e211",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (len(entity_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af015795",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_dic = {}\n",
    "\n",
    "for i in entity_summaries:\n",
    "    entity_dic[i['entity']] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d2b92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_entities = []\n",
    "for i in entity_dic.values():\n",
    "    final_entities.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1723dc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dae837",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_entities = json.dumps(final_entities, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ecf322",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Entities/' + metadata['File Name']+ '.json', \"w\") as output_file:\n",
    "    output_file.write(json_entities)\n",
    "    output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3d5464",
   "metadata": {},
   "source": [
    "# Write the output to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92563017",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_relations = []\n",
    "for i in relations_list:\n",
    "    if 'Subject' in i and i['Subject'] in entity_names and i['Object'] in entity_names and i['Relation'] in relation_labels:\n",
    "        final_relations.append(i)\n",
    "    elif 'Description' in i and 'Subject' in i and i['Subject'] in entity_dic.keys():\n",
    "        entity_dic[i['Subject']].update({'information':i['Description']})\n",
    "    elif 'Description' in i and 'Object' in i and i['Object'] in entity_dic.keys():\n",
    "        entity_dic[i['Object']].update({'information':i['Description']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7d334d",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_relations = json.dumps(final_relations, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f78c79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Relations/' + metadata['File Name']+ '.json', \"w\") as output_file:\n",
    "    output_file.write(json_relations)\n",
    "    output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6e45ff",
   "metadata": {},
   "source": [
    "Manual review of the entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec38e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(final_relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7589070c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Entities/' + 'ALB-NEP-2013-EN'+ '.json', \"r\") as f:\n",
    "    data = f.read()\n",
    "    ent = json.loads(data)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19524fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Relations/' + 'ALB-NEP-2013-EN'+ '.json', \"r\") as f:\n",
    "    data = f.read()\n",
    "    rel = json.loads(data)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36c6ab8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "print (len(ent))\n",
    "print (len(rel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2822f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_relations_neo4j(ent, rel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079f0f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#document = Document(metadata, final_entities, final_relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c007af4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_relations_neo4j(final_entities, final_relations)\n",
    "#insert_summary_neo4j(summary_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06754a38",
   "metadata": {},
   "source": [
    "# Add Relations to Spreadsheet for Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62e2121",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "\n",
    "# Use the credentials from the service account key JSON file you downloaded\n",
    "scope = [\"https://spreadsheets.google.com/feeds\", \"https://www.googleapis.com/auth/drive\"]\n",
    "creds = ServiceAccountCredentials.from_json_keyfile_name('energy-moonshot-ai-97aa9045e45f.json', scope)\n",
    "client = gspread.authorize(creds)\n",
    "\n",
    "# Open the Google Sheet by its title or URL\n",
    "spreadsheet = client.open_by_url('https://docs.google.com/spreadsheets/d/1yZ-XQQs52kaI5k9MjvV_CdbgWQi-GazjHHGqQUF8gko/edit')\n",
    "\n",
    "\n",
    "# Enter relations in the first sheet\n",
    "sheet = spreadsheet.get_worksheet(0)\n",
    "\n",
    "# Start row index from 5\n",
    "start_row_index = 5\n",
    "index = 1\n",
    "\n",
    "# Check if there's valid data to insert\n",
    "if final_relations:\n",
    "    # Create a list of lists where each inner list represents the values of a row\n",
    "    batch_relations = []\n",
    "    for index, row_data in enumerate(final_relations):\n",
    "        row = [index, row_data['Subject'], row_data['Relation'], row_data.get('Object', ''), \n",
    "               row_data.get('Description', ''), row_data.get('Relevance', '')]\n",
    "        \n",
    "        batch_relations.append(row)\n",
    "        index = index + 1\n",
    "\n",
    "    # Insert the data into the Google Sheet starting from row 5\n",
    "    sheet.insert_rows(batch_relations, start_row_index)\n",
    "\n",
    "    print(f\"{len(final_relations)} entries added to Google Sheet.\")\n",
    "else:\n",
    "    print(\"No data to insert.\")\n",
    "    \n",
    "    \n",
    "# Enter entities in the second sheet\n",
    "sheet = spreadsheet.get_worksheet(1)\n",
    "\n",
    "\n",
    "# Start row index from 5\n",
    "start_row_index = 5\n",
    "index = 1\n",
    "\n",
    "if final_entities:\n",
    "    batch_entities = []\n",
    "    for index, row_data in enumerate(final_entities):\n",
    "        row = [index, row_data['entity'], row_data['category'], row_data.get('acronym', ''), row_data.get('summary', '')]\n",
    "        batch_entities.append(row)\n",
    "        \n",
    "        index = index + 1\n",
    "    sheet.insert_rows(batch_entities, start_row_index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
