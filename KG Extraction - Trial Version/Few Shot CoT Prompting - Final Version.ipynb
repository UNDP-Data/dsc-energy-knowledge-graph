{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b0a798",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "\n",
    "import numpy as np\n",
    "import torch, torchvision\n",
    "from transformers import AutoTokenizer, TFAutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "from span_marker import SpanMarkerModel\n",
    "from langdetect import detect\n",
    "from langdetect import LangDetectException\n",
    "import re\n",
    "import pickle\n",
    "import time\n",
    "from langchain.text_splitter import SpacyTextSplitter\n",
    "\n",
    "#nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d297b4e3",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfc59a5",
   "metadata": {},
   "source": [
    "Initialize Entity Categories and Relation Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5423cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\n",
    "    \n",
    "    \"Person\",\n",
    "    \"Location\",\n",
    "    \"Organization\",\n",
    "    \"Event\",\n",
    "    \"Product\",\n",
    "    \"Project\",\n",
    "    \"Skill\",\n",
    "    \"Strategy\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa069f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_labels = [\n",
    "    \"implements\",\n",
    "    \"funds\",\n",
    "    \"focuses_on\",\n",
    "    \"in\",\n",
    "    \"partners_with\",\n",
    "    \"contributes_to\",\n",
    "    \"monitors\",\n",
    "    \"targets\",\n",
    "    \"addresses\",\n",
    "    \"employs\",\n",
    "    \"collaborates_with\",\n",
    "    \"supports\",\n",
    "    \"administers\",\n",
    "    \"measures\",\n",
    "    \"aligns_with\",\n",
    "    \"an_instance_of\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e5f1a1",
   "metadata": {},
   "source": [
    "# Setting up OpenAI connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a472f8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI API configuration\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai.api_base = os.getenv(\"OPENAI_API_BASE\")\n",
    "openai.api_version = os.getenv(\"OPENAI_API_VERSION\")\n",
    "openai_deployment = \"sdgi-gpt-35-turbo-16k\"\n",
    "\n",
    "neo4j_pass = os.getenv(\"NEO4JPASS\")\n",
    "#openai.api_key = os.getenv(\"OPENAI_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76acd36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_answer(user_question, timeout_seconds):\n",
    "    messages = [\n",
    "        {'role': 'user', 'content': user_question},\n",
    "    ]\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            engine=\"sdgi-gpt-35-turbo-16k\", \n",
    "            messages=messages,\n",
    "            temperature=0.2,\n",
    "            request_timeout = timeout_seconds\n",
    "            # max_tokens=2000\n",
    "        )\n",
    "        return response.choices[0].message[\"content\"]\n",
    "    except requests.Timeout:\n",
    "        print(f\"Request timed out\")\n",
    "        return []\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e328a35",
   "metadata": {},
   "source": [
    "# Entity Extraction using Transformers \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f369316f",
   "metadata": {},
   "outputs": [],
   "source": [
    "WIKI_API = \"https://api-inference.huggingface.co/models/Babelscape/wikineural-multilingual-ner\"\n",
    "BERT_API = \"https://api-inference.huggingface.co/models/dslim/bert-base-NER\"\n",
    "\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "def query_wiki(payload):\n",
    "\tresponse = requests.post(WIKI_API, headers=headers, json=payload)\n",
    "\treturn response.json()\n",
    "\t\n",
    "\n",
    "def query_bert(payload):\n",
    "\tresponse = requests.post(BERT_API, headers=headers, json=payload)\n",
    "\treturn response.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb268621",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_gpt(text):\n",
    "    \n",
    "    entities_prompt = f\"\"\"\n",
    "\n",
    "    You will be given a >>>>>TEXT<<<<<. You have two tasks:\n",
    "    \n",
    "    1. Your first task is to detect acronyms with their names and store them in python dictionary.\n",
    "    2. Your second task is to detect Proper Nouns in the text and store them in python list.\n",
    "    \n",
    "    Return a JSON array contaning dictionary and the list.\n",
    "\n",
    "    >>>>>TEXT<<<<<\n",
    "    {text}\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    #start_time = time.time()\n",
    "\n",
    "    result = get_answer(entities_prompt, 10)\n",
    "    result = json.loads(result)\n",
    "    \n",
    "    #end_time = time.time()\n",
    "    #elapsed_time = end_time - start_time\n",
    "    #print (f\"TIME TAKEN TO EXECUTE PROMPT: {elapsed_time}\")\n",
    "    return result\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4160364",
   "metadata": {},
   "source": [
    "# Text Pre-Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b140f501",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_spacy(chunk_size, text):\n",
    "    \n",
    "    text_splitter = SpacyTextSplitter(chunk_size=chunk_size)\n",
    "    sections = text_splitter.split_text(text)\n",
    "    \n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13f4641",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_section(limit, text):\n",
    "    sections_list = []\n",
    "    length = len(text)\n",
    "    i = 0\n",
    "\n",
    "    while i < length - 1:\n",
    "        j = i + limit\n",
    "\n",
    "        if j >= length:\n",
    "            j = length - 1\n",
    "        elif text[j] not in ('.', '\\n', ';'):\n",
    "            while text[j] not in ('.', '\\n', ';'):\n",
    "                j -= 1\n",
    "            j += 1\n",
    "\n",
    "        section = text[i:j]\n",
    "\n",
    "        if is_valid_section(section):\n",
    "            sections_list.append(section)\n",
    "        else: \n",
    "            print(\"INVALID SECTION DETECTED\")\n",
    "            print(section)\n",
    "            #section_list[-1].extend(section)\n",
    "        i = j\n",
    "    \n",
    "    \n",
    "    return sections_list\n",
    "\n",
    "def is_valid_section(section):\n",
    "    return section and len(section) > 20\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4066d417",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(input_text):\n",
    "    # Remove lines with only whitespace\n",
    "    input_text = re.sub(r'^\\s*$', '', input_text, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove lines containing only uppercase text (potential headings)\n",
    "    input_text = re.sub(r'^\\s*[A-Z\\s]+\\s*$', '', input_text, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove lines with multiple consecutive uppercase words (potential headings)\n",
    "    input_text = re.sub(r'^\\s*(?:[A-Z]+\\s*){2,}\\s*$', '', input_text, flags=re.MULTILINE)\n",
    "    \n",
    "    input_text = re.sub(r'^\\s*[A-Za-z\\s]+\\.{3,}\\s*\\d+\\s*$', '', input_text, flags=re.MULTILINE)\n",
    "\n",
    "    return input_text\n",
    "\n",
    "def is_english(line):\n",
    "    try:\n",
    "        return detect(line) == 'en'\n",
    "    except LangDetectException as e:\n",
    "        print(f\"An exception occurred: {e} : {line}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38eb33c",
   "metadata": {},
   "source": [
    "# Entities Post-Processing Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7e10db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging the broken entities\n",
    "def create_entities(lst):\n",
    "    i = 1\n",
    "    while i < len(lst):\n",
    "        if lst[i][\"word\"].startswith('##'):\n",
    "            lst[i][\"word\"] = lst[i-1][\"word\"] + lst[i][\"word\"][2:]\n",
    "            lst[i][\"score\"] = max(lst[i-1][\"score\"] , lst[i][\"score\"])\n",
    "            del lst[i-1]\n",
    "        else:\n",
    "            i += 1\n",
    "            # todo: return a list of merged entities\n",
    "            \n",
    "\n",
    "\n",
    "def apply_threshold(list_, threshold):\n",
    "    words_list = []\n",
    "    for item in list_:\n",
    "        if item['score'] > threshold:  # threshold score to eliminate unimportant entities\n",
    "            words_list.append(item['word'])\n",
    "    return words_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27e0af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw(list_):\n",
    "    output = []\n",
    "    for sublist in list_:\n",
    "        new = []\n",
    "        obj = {}\n",
    "        for item in sublist:\n",
    "            #obj = {}\n",
    "            key = ''.join(filter(str.isalpha, item))\n",
    "            obj[key]= item\n",
    "            #obj['raw']= ''.join(filter(str.isalpha, item))\n",
    "        output.append(obj)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c833db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_extracted_entities_old(wiki, bert, gpt):\n",
    "    \n",
    "    output = []\n",
    "    dict_ = gpt\n",
    "    \n",
    "    \n",
    "    #print(dict_.items())\n",
    "    wiki_set = set(wiki.keys())\n",
    "    #bert_set = set(bert.keys())\n",
    "    gpt_set = set(gpt.keys())\n",
    "    \n",
    "    \n",
    "    #A = gpt_set.intersection(bert_set)\n",
    "    #B = bert_set.intersection(wiki_set)\n",
    "    C = wiki_set.intersection(gpt_set)\n",
    " \n",
    "    #matched = list(A.union(B).union(C))\n",
    "    \n",
    "    \n",
    "    for i in C:\n",
    "        output.append(dict_[i])\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e749549e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_extracted_entities(wiki, bert, gpt):\n",
    "    \n",
    "    output = set(wiki.values())\n",
    "    dict_ = gpt\n",
    "    \n",
    "    bert_set = set(bert.keys()) - set(wiki.keys())\n",
    "    gpt_set = set(gpt.keys())\n",
    "    \n",
    "    A = gpt_set.intersection(bert_set)\n",
    "\n",
    "    matched = list(set(A))\n",
    "    print (\"GPT/BERT: \" + str(matched))\n",
    "\n",
    "    for i in matched:\n",
    "        output.add(dict_[i])\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff671cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_entities(list_):\n",
    "    \n",
    "    # Define a regular expression pattern to match invalid characters.\n",
    "    pattern = r'\\s*{}\\s*'.format(re.escape(\"’\"))\n",
    "    pattern1 = r'\\s*{}\\s*'.format(re.escape(\"/\"))\n",
    "    output_list = []\n",
    "\n",
    "    for item in list_:\n",
    "        item = re.sub(pattern, \"’\", item)\n",
    "        tem = re.sub(pattern1, \"/\", item)\n",
    "            \n",
    "    return output_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f074fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(index, wiki, bert, gpt, acronym):\n",
    "    checkpoint = {'index': index, 'wiki': wiki, 'bert': bert, 'gpt': gpt, 'acronym': acronym}\n",
    "    with open('checkpoint.pkl', 'wb') as checkpoint_file:\n",
    "        pickle.dump(checkpoint, checkpoint_file)\n",
    "\n",
    "# Function to load the state\n",
    "def load_checkpoint(length):\n",
    "    try:\n",
    "        with open('checkpoint.pkl', 'rb') as checkpoint_file:\n",
    "            checkpoint = pickle.load(checkpoint_file)\n",
    "            return checkpoint['index'], checkpoint['wiki'], checkpoint['bert'], \n",
    "        checkpoint['gpt'], checkpoint['acronym']\n",
    "    except FileNotFoundError:\n",
    "        return 0, [''] * length, [''] * length, [''] * length, {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b6c56a",
   "metadata": {},
   "source": [
    "#  Categorize entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197cdc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_entities(text, entities, categories):\n",
    "    \n",
    "    \n",
    "    categorization_prompt = f\"\"\"\n",
    "\n",
    "    You will be given a >>>>>TEXT<<<<<, an >>>>>EntityList<<<<< and >>>>>Categories<<<<<. \n",
    "    Your task is to assign a sutiable category to each element of >>>>>EntityList<<<<<.\n",
    "    Do not add any new entities.\n",
    "    Return a list of JSON objects of categorized entities. \n",
    "\n",
    "\n",
    "    >>>>>TEXT<<<<<\n",
    "    {text}\n",
    "\n",
    "    >>>>>Categories<<<<<\n",
    "    {categories}\n",
    "\n",
    "    >>>>>EntityList<<<<<\n",
    "    {entities}\n",
    "    \"\"\"\n",
    "\n",
    "    categorized_entities = get_answer(categorization_prompt, 30)\n",
    "    categorized_entities = json.loads(categorized_entities)\n",
    "    \n",
    "    return (categorized_entities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0d1e03",
   "metadata": {},
   "source": [
    "# Relation Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf0d4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_relation_details(text, entities, relation_labels):\n",
    "    relation_extraction_prompt = f\"\"\"\n",
    "    \n",
    "    [Context]\n",
    "    You belong to a team of consultants at UNDP's Sustainable Energy Hub (SEH), working on a project to extract a \n",
    "    Knowledge Graph from the UNDP dataset.\n",
    "    You will be given a >>>>>TEXT<<<<<, an >>>>>EntityList<<<<< and a list of >>>>>RelationLabels<<<<<.\n",
    "\n",
    "   [Task]\n",
    "   \n",
    "   Your task is to perform Relation Extraction on the given >>>>>TEXT<<<<< \n",
    "   to find relations between elements of provided >>>>>EntityList<<<<<.\n",
    "   \n",
    "   Please make sure to read these instructions and constraints carefully.\n",
    "\n",
    "    [Instructions]\n",
    "    1. Carefully read and store the >>>>>RelationLabels<<<<<.\n",
    "    2. Scan the >>>>>TEXT<<<<< to find Named Entites from >>>>>EntityList<<<<< that are related.\n",
    "    3. Scan the >>>>>RelationLabels<<<<< to select a suitable label to\n",
    "    describe the relation between the above selected entities. Mark this label as \"Relation\".\n",
    "    4. Assign \"Subject\" and \"Object\" to entities depending on the selected \"Relation\"\n",
    "    selected in previous step to create a tuple.\n",
    "    5. If available, select a small \"Description\" from the >>>>>TEXT<<<<< for the above relation.\n",
    "    6. Assign a Relevance score between 1 to 10 to the extracted relation, with 10 being the most relevant.\n",
    "    7. Repeat the process to extract remaining relations from >>>>>TEXT<<<<<.\n",
    "    \n",
    "    \n",
    "    [Constraints]\n",
    "    1. Values of 'Relation' key should belong to >>>>>RelationLabels<<<<<.\n",
    "    \n",
    "    [Output Format]\n",
    "    Provide the result as a JSON array.\n",
    "\n",
    "    Perform relation extraction on the below:\n",
    "    \n",
    "    >>>>>TEXT<<<<<\n",
    "    {text}\n",
    "\n",
    "    >>>>>EntityList<<<<<\n",
    "    {entities}\n",
    "\n",
    "    >>>>>RelationLabels<<<<<\n",
    "    {relation_labels}\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "    relations = get_answer(relation_extraction_prompt,60)\n",
    "    relations = json.loads(relations)\n",
    "\n",
    "    return relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513a5f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ontology_relations(text, entities, ontology):\n",
    "    relation_extraction_prompt = f\"\"\"\n",
    "    \n",
    "    [Context]\n",
    "    You belong to a team of consultants at UNDP's Sustainable Energy Hub (SEH), working on a project to extract a \n",
    "    Knowledge Graph from the UNDP dataset.\n",
    "    You will be given a >>>>>TEXT<<<<<, an >>>>>EntityList<<<<< and an >>>>>ONTOLOGY<<<<<.\n",
    "\n",
    "   [Task]\n",
    "   \n",
    "   Your task is to perform Relation Extraction on the given >>>>>TEXT<<<<< \n",
    "   to find relations between elements of provided >>>>>EntityList<<<<<. Use the given >>>>>ONTOLOGY<<<<<\n",
    "   for this purpose.\n",
    "   \n",
    "   Please make sure to read these instructions and constraints carefully.\n",
    "\n",
    "    [Instructions]\n",
    "    1. Carefully read and understand the >>>>>ONTOLOGY<<<<<.\n",
    "    2. Scan the >>>>>TEXT<<<<< to find Named Entites in >>>>>EntityList<<<<< that are related.\n",
    "    3. Read the >>>>>ONTOLOGY<<<<< to select a relationship type for the related entities. Mark this label as \"Relation\".\n",
    "    4. Assign \"Subject\" and \"Object\" to entities depending on the \"Relation\"\n",
    "    selected in previous step to create a tuple.\n",
    "    5. If available, select a small \"Description\" from the >>>>>TEXT<<<<< for the above relation.\n",
    "    6. Assign a Relevance score between 1 to 10 to the extracted relation, with 10 being the most relevant.\n",
    "    7. Repeat the process to extract remaining relations from >>>>>TEXT<<<<<.\n",
    "    \n",
    "    \n",
    "    [Constraints]\n",
    "    1. Values of 'Relation' key should be a label from properties in >>>>>ONTOLOGY<<<<<.\n",
    "    \n",
    "    [Output Format]\n",
    "    Provide the result as a JSON array.\n",
    "\n",
    "    Perform relation extraction on the below:\n",
    "    \n",
    "    >>>>>TEXT<<<<<\n",
    "    {text}\n",
    "\n",
    "    >>>>>EntityList<<<<<\n",
    "    {entities}\n",
    "\n",
    "    >>>>>ONTOLOGY<<<<<\n",
    "    {ontology}\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "    relations = get_answer(relation_extraction_prompt,60)\n",
    "    relations = json.loads(relations)\n",
    "\n",
    "    return relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc4f23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ontology.ttl\") as f:\n",
    "    ontology = f.read()\n",
    "    \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4862fd69",
   "metadata": {},
   "source": [
    "# Connecting with DBpedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2a53f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "# Define the DBpedia SPARQL endpoint\n",
    "sparql_endpoint = \"http://dbpedia.org/sparql\"\n",
    "\n",
    "# Create a SPARQLWrapper instance\n",
    "sparql = SPARQLWrapper(sparql_endpoint)\n",
    "\n",
    "# Function to search for an entity by label and return its DBpedia URI\n",
    "def search_entity(label):\n",
    "    query = f\"\"\"\n",
    "    SELECT ?entity\n",
    "    WHERE {{\n",
    "      ?entity rdfs:label \"{label}\"@en.\n",
    "    }}\n",
    "    LIMIT 1\n",
    "    \"\"\"\n",
    "\n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "\n",
    "    results = sparql.query().convert()\n",
    "\n",
    "    if \"results\" in results and \"bindings\" in results[\"results\"] and results[\"results\"][\"bindings\"]:\n",
    "        entity_uri = results[\"results\"][\"bindings\"][0][\"entity\"][\"value\"]\n",
    "        return entity_uri\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Function to retrieve and return the abstract or comment of an entity\n",
    "def retrieve_entity_summary(entity_uri):\n",
    "    # Try to retrieve the abstract\n",
    "    abstract_query = f\"\"\"\n",
    "    SELECT ?abstract\n",
    "    WHERE {{\n",
    "      <{entity_uri}> dbo:abstract ?abstract.\n",
    "      FILTER (LANGMATCHES(LANG(?abstract), \"en\"))\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    sparql.setQuery(abstract_query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "\n",
    "    abstract_results = sparql.query().convert()\n",
    "\n",
    "    if \"results\" in abstract_results and \"bindings\" in abstract_results[\"results\"]:\n",
    "        for result in abstract_results[\"results\"][\"bindings\"]:\n",
    "            abstract = result[\"abstract\"][\"value\"]\n",
    "            return abstract\n",
    "\n",
    "    # If abstract is not found, try to retrieve the comment\n",
    "    comment_query = f\"\"\"\n",
    "    SELECT ?comment\n",
    "    WHERE {{\n",
    "      <{entity_uri}> rdfs:comment ?comment.\n",
    "      FILTER (LANGMATCHES(LANG(?comment), \"en\"))\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    sparql.setQuery(comment_query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "\n",
    "    comment_results = sparql.query().convert()\n",
    "\n",
    "    if \"results\" in comment_results and \"bindings\" in comment_results[\"results\"]:\n",
    "        for result in comment_results[\"results\"][\"bindings\"]:\n",
    "            comment = result[\"comment\"][\"value\"]\n",
    "            return comment\n",
    "\n",
    "    # If neither abstract nor comment is found, return None\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c57444",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.error\n",
    "\n",
    "def dbpedia_summary(search_label):\n",
    "    entity_uri = search_entity(search_label)\n",
    "\n",
    "    if entity_uri:\n",
    "        print(f\"Entity found with DBpedia URI: {entity_uri}\")\n",
    "        try:\n",
    "            summary = retrieve_entity_summary(entity_uri)\n",
    "            if summary:\n",
    "                return summary\n",
    "            else:\n",
    "                print(\"No abstract or comment found for this entity.\")\n",
    "        except urllib.error.URLError as e:\n",
    "            print(f\"Error: {e}\")\n",
    "    else:\n",
    "        print(f\"No entity found with the label: {search_label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be620ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_summaries(entities):\n",
    "    \n",
    "    updated_entities = []\n",
    "\n",
    "    for item in entities:\n",
    "        try:\n",
    "            summary = dbpedia_summary(item['entity'])\n",
    "            if summary:\n",
    "                # Only add the summary if it's not None or empty\n",
    "                item['summary'] = summary\n",
    "            updated_entities.append(item)  # Add the item regardless of summary presence\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return updated_entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7975f7",
   "metadata": {},
   "source": [
    "# Creating Graph in Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2cd7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph, Node, Relationship\n",
    "graph = Graph(uri = 'bolt://localhost:7687',user='neo4j',password=neo4j_pass)\n",
    "\n",
    "class Document:\n",
    "    def __init__(self, metadata, entities, relations):\n",
    "        self.metadata = metadata\n",
    "        self.entities = entities\n",
    "        self.relations = relations\n",
    "\n",
    "# Define a function to create or retrieve a node\n",
    "def get_or_create_node(label, key, value):\n",
    "    # Attempt to find an existing node with the given label and key\n",
    "    existing_node = get_node(label, key, value)\n",
    "    if not existing_node:\n",
    "        existing_node = get_node(label, 'acronym', value)\n",
    "    \n",
    "    if existing_node:\n",
    "        return existing_node\n",
    "    else:\n",
    "        new_node = Node(label, **{key: value})\n",
    "        graph.create(new_node)\n",
    "        return new_node\n",
    "    \n",
    "def get_node(label, key, value):\n",
    "    node = graph.nodes.match(label, **{key:value}).first()\n",
    "    return node\n",
    "def get_node_without_label(key, value):\n",
    "    node = graph.nodes.match(**{key:value}).first()\n",
    "    return node\n",
    "\n",
    "# Define a function to insert relations \n",
    "def insert_relations_neo4j(entities, relations):\n",
    "    #document_node = get_or_create_node(\"Document\", \"name\", document.metadata['Document Title'] )\n",
    "            \n",
    "    #for key,value in metadata.items():\n",
    "     #   document_node[key] = value\n",
    "        \n",
    "    #graph.push(document_node)\n",
    "    \n",
    "    for item in entities:\n",
    "        \n",
    "        node = get_or_create_node('Entity', \"name\", item[\"entity\"])\n",
    "        node['category'] = item[\"category\"]\n",
    "        if \"acronym\" in item:\n",
    "            node['acronym'] = item[\"acronym\"]\n",
    "        if \"summary\" in item:\n",
    "            node['summary'] = item[\"summary\"]\n",
    "        \n",
    "        graph.push(node)\n",
    "        #entity_rel = Relationship(node, 'parent_document', document_node)\n",
    "        \n",
    "        #if \"information\" in item:\n",
    "         #   relation[\"description\"] = item['information']\n",
    "            \n",
    "        #graph.create(relation)\n",
    "        \n",
    "    for item in relations:\n",
    "        subject = get_or_create_node( \"Entity\", \"name\", item[\"Subject\"])\n",
    "        obj = get_or_create_node(\"Entity\", \"name\", item[\"Object\"])\n",
    "        relation = Relationship(subject, item[\"Relation\"], obj)\n",
    "        if 'Description' in item:\n",
    "            relation[\"description\"] = item[\"Description\"]\n",
    "        \n",
    "        # Merge nodes and create relationships\n",
    "        #graph.merge(subject, \"name\")\n",
    "        #graph.merge(obj, \"name\")\n",
    "        graph.create(relation)\n",
    "        \n",
    "        # Link the nodes to the project node\n",
    "        #graph.create(Relationship(subject, \"Belongs To\", document_node))\n",
    "        #graph.create(Relationship(obj, \"Belongs To\", document_node))\n",
    "        \n",
    "    \n",
    "# Define a function to insert summaries \n",
    "def insert_summary_neo4j(data):\n",
    "    for item in data:\n",
    "        node = get_node(\"Entity\", \"name\", item.name)\n",
    "        node[\"Summary\"] = item.summary\n",
    "        graph.push(node)\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af30def",
   "metadata": {},
   "source": [
    "# Extraction Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 835,
   "id": "7cd35416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files: 3\n",
      "\n",
      "['NGA-NGP-2017-EN.txt', 'NGA-CPD-2023-EN.txt', 'NGA-NEPro-2022-EN.txt']\n"
     ]
    }
   ],
   "source": [
    "folder_path = ('Data/NGA')\n",
    "file_list = os.listdir(folder_path)\n",
    "\n",
    "# Filter the list to include only text files (e.g., .txt files)\n",
    "text_files = [file for file in file_list if file.endswith(\".txt\")]\n",
    "\n",
    "print (f\"Number of files: {len(text_files)}\\n\")  \n",
    "print (text_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 836,
   "id": "644ec132",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(folder_path, text_files[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 837,
   "id": "32344bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text length: 8067\n"
     ]
    }
   ],
   "source": [
    "with open (file_path, 'r') as file:\n",
    "    head = [next(file) for _ in range(11)]\n",
    "    next(file)\n",
    "    raw_text = file.read()\n",
    "    \n",
    "    file.close()\n",
    "\n",
    "print (f\"Original text length: {len(raw_text)}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 838,
   "id": "a1d8fa1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An exception occurred: No features in text. : \n",
      "\n",
      "An exception occurred: No features in text. : 15%\n",
      "\n",
      "An exception occurred: No features in text. : 10%\n",
      "\n",
      "An exception occurred: No features in text. : 0%\n",
      "\n",
      "An exception occurred: No features in text. : 75%\n",
      "\n",
      "An exception occurred: No features in text. : 0% 1%\n",
      "\n",
      "An exception occurred: No features in text. : 99%\n",
      "\n",
      "An exception occurred: No features in text. : 55%\n",
      "\n",
      "An exception occurred: No features in text. : 15%\n",
      "\n",
      "An exception occurred: No features in text. : 81%\n",
      "\n",
      "An exception occurred: No features in text. : 0%\n",
      "\n",
      "An exception occurred: No features in text. : 20%\n",
      "\n",
      "An exception occurred: No features in text. : 40%\n",
      "\n",
      "An exception occurred: No features in text. : 60%\n",
      "\n",
      "An exception occurred: No features in text. : 80%\n",
      "\n",
      "An exception occurred: No features in text. : 100%\n",
      "\n",
      "An exception occurred: No features in text. : 2014 2015 2016 2017 2018 2019 2020\n",
      "\n",
      "An exception occurred: No features in text. : 53.5\n",
      "\n",
      "An exception occurred: No features in text. : 57.4 57.2 57.4 56.2 55.1\n",
      "\n",
      "An exception occurred: No features in text. : 0\n",
      "\n",
      "An exception occurred: No features in text. : 10\n",
      "\n",
      "An exception occurred: No features in text. : 20\n",
      "\n",
      "An exception occurred: No features in text. : 30\n",
      "\n",
      "An exception occurred: No features in text. : 40\n",
      "\n",
      "An exception occurred: No features in text. : 50\n",
      "\n",
      "An exception occurred: No features in text. : 60\n",
      "\n",
      "An exception occurred: No features in text. : 70\n",
      "\n",
      "An exception occurred: No features in text. : 2014 2015 2016 2017 2018 2019\n",
      "\n",
      "An exception occurred: No features in text. : )\n",
      "\n",
      "An exception occurred: No features in text. : 5.0\n",
      "\n",
      "An exception occurred: No features in text. : -4%\n",
      "\n",
      "An exception occurred: No features in text. : 4.6\n",
      "\n",
      "An exception occurred: No features in text. : 4.7\n",
      "\n",
      "An exception occurred: No features in text. : 4.8\n",
      "\n",
      "An exception occurred: No features in text. : 4.9\n",
      "\n",
      "An exception occurred: No features in text. : 5.0\n",
      "\n",
      "An exception occurred: No features in text. : 5.1\n",
      "\n",
      "An exception occurred: No features in text. : 5.2\n",
      "\n",
      "An exception occurred: No features in text. : 5.3\n",
      "\n",
      "An exception occurred: No features in text. : 5.4\n",
      "\n",
      "An exception occurred: No features in text. : 5.5\n",
      "\n",
      "An exception occurred: No features in text. : 5.6\n",
      "\n",
      "An exception occurred: No features in text. : -5%\n",
      "\n",
      "An exception occurred: No features in text. : -4%\n",
      "\n",
      "An exception occurred: No features in text. : -3%\n",
      "\n",
      "An exception occurred: No features in text. : -2%\n",
      "\n",
      "An exception occurred: No features in text. : -1%\n",
      "\n",
      "An exception occurred: No features in text. : 0%\n",
      "\n",
      "An exception occurred: No features in text. : 1%\n",
      "\n",
      "An exception occurred: No features in text. : 2%\n",
      "\n",
      "An exception occurred: No features in text. : 3%\n",
      "\n",
      "An exception occurred: No features in text. : 4%\n",
      "\n",
      "An exception occurred: No features in text. : 1,173\n",
      "\n",
      "An exception occurred: No features in text. : 0.0\n",
      "\n",
      "An exception occurred: No features in text. : 1 000\n",
      "\n",
      "An exception occurred: No features in text. : 2 000\n",
      "\n",
      "An exception occurred: No features in text. : 3 000\n",
      "\n",
      "An exception occurred: No features in text. : 4 000\n",
      "\n",
      "An exception occurred: No features in text. : 5 000\n",
      "\n",
      "An exception occurred: No features in text. : 6 000\n",
      "\n",
      "An exception occurred: No features in text. : 2014 2015 2016 2017 2018 2019\n",
      "\n",
      "An exception occurred: No features in text. : 6.4\n",
      "\n",
      "An exception occurred: No features in text. : 5.8\n",
      "\n",
      "An exception occurred: No features in text. : 5.9\n",
      "\n",
      "An exception occurred: No features in text. : 6.0\n",
      "\n",
      "An exception occurred: No features in text. : 6.1\n",
      "\n",
      "An exception occurred: No features in text. : 6.2\n",
      "\n",
      "An exception occurred: No features in text. : 6.3\n",
      "\n",
      "An exception occurred: No features in text. : 6.4\n",
      "\n",
      "An exception occurred: No features in text. : 6.5\n",
      "\n",
      "An exception occurred: No features in text. : 2014 2015 2016 2017 2018 2019\n",
      "\n",
      "An exception occurred: No features in text. : 10.4\n",
      "\n",
      "An exception occurred: No features in text. : 9.5\n",
      "\n",
      "An exception occurred: No features in text. :  10\n",
      "\n",
      "An exception occurred: No features in text. :  11\n",
      "\n",
      "An exception occurred: No features in text. :  11\n",
      "\n",
      "An exception occurred: No features in text. :  12\n",
      "\n",
      "An exception occurred: No features in text. :  12\n",
      "\n",
      "An exception occurred: No features in text. :  13\n",
      "\n",
      "An exception occurred: No features in text. : 2014 2015 2016 2017 2018 2019 2020\n",
      "\n",
      "An exception occurred: No features in text. :  0\n",
      "\n",
      "An exception occurred: No features in text. : + 9\n",
      "\n",
      "An exception occurred: No features in text. : + 4\n",
      "\n",
      "An exception occurred: No features in text. :  0\n",
      "\n",
      "An exception occurred: No features in text. :  0\n",
      "\n",
      "An exception occurred: No features in text. :  0\n",
      "\n",
      "An exception occurred: No features in text. : 4%\n",
      "\n",
      "An exception occurred: No features in text. : 93%\n",
      "\n",
      "An exception occurred: No features in text. : 3%\n",
      "\n",
      "An exception occurred: No features in text. : 1.9\n",
      "\n",
      "An exception occurred: No features in text. : 0.0\n",
      "\n",
      "An exception occurred: No features in text. : 0.5\n",
      "\n",
      "An exception occurred: No features in text. : 0.0 0.1 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "\n",
      "An exception occurred: No features in text. : 0.0\n",
      "\n",
      "An exception occurred: No features in text. : 0.5\n",
      "\n",
      "An exception occurred: No features in text. : 1.0\n",
      "\n",
      "An exception occurred: No features in text. : 1.5\n",
      "\n",
      "An exception occurred: No features in text. : 2.0\n",
      "\n",
      "An exception occurred: No features in text. : 2.5\n",
      "\n",
      "An exception occurred: No features in text. : 2016 2017 2018 2019 2020 2021\n",
      "\n",
      "An exception occurred: No features in text. : 23%\n",
      "\n",
      "An exception occurred: No features in text. : 17%\n",
      "\n",
      "An exception occurred: No features in text. : 44%\n",
      "\n",
      "An exception occurred: No features in text. : 28%\n",
      "\n",
      "An exception occurred: No features in text. : 0 20 40 60 80 100\n",
      "\n",
      "An exception occurred: No features in text. : 98%\n",
      "\n",
      "An exception occurred: No features in text. : 2%0%\n",
      "\n",
      "An exception occurred: No features in text. : 4 203 4 285 4 354 4 427 4 505 4 570\n",
      "\n",
      "An exception occurred: No features in text. :  0\n",
      "\n",
      "An exception occurred: No features in text. :  1 000\n",
      "\n",
      "An exception occurred: No features in text. :  2 000\n",
      "\n",
      "An exception occurred: No features in text. :  3 000\n",
      "\n",
      "An exception occurred: No features in text. :  4 000\n",
      "\n",
      "An exception occurred: No features in text. :  5 000\n",
      "\n",
      "An exception occurred: No features in text. : 2014 2015 2016 2017 2018 2019\n",
      "\n",
      "An exception occurred: No features in text. : 11 11\n",
      "\n",
      "An exception occurred: No features in text. : 13 13 13 13 13 13\n",
      "\n",
      "An exception occurred: No features in text. : 16%\n",
      "\n",
      "An exception occurred: No features in text. : 0%\n",
      "\n",
      "An exception occurred: No features in text. : 20%\n",
      "\n",
      "An exception occurred: No features in text. : 40%\n",
      "\n",
      "An exception occurred: No features in text. : 60%\n",
      "\n",
      "An exception occurred: No features in text. : 80%\n",
      "\n",
      "An exception occurred: No features in text. : 100%\n",
      "\n",
      "An exception occurred: No features in text. :  0\n",
      "\n",
      "An exception occurred: No features in text. :  2\n",
      "\n",
      "An exception occurred: No features in text. :  4\n",
      "\n",
      "An exception occurred: No features in text. :  6\n",
      "\n",
      "An exception occurred: No features in text. :  8\n",
      "\n",
      "An exception occurred: No features in text. :  10\n",
      "\n",
      "An exception occurred: No features in text. :  12\n",
      "\n",
      "An exception occurred: No features in text. :  14\n",
      "\n",
      "An exception occurred: No features in text. : 2014 2015 2016 2017 2018 2019 2020 2021\n",
      "\n",
      "An exception occurred: No features in text. : 1%\n",
      "\n",
      "An exception occurred: No features in text. : 1 2021\n",
      "\n",
      "An exception occurred: No features in text. : 2 2021\n",
      "\n",
      "An exception occurred: No features in text. : 3 2020\n",
      "\n",
      "An exception occurred: No features in text. : 4 2019\n",
      "\n",
      "An exception occurred: No features in text. : 5 2019\n",
      "\n",
      "An exception occurred: No features in text. :  0\n",
      "\n",
      "An exception occurred: No features in text. :  50\n",
      "\n",
      "An exception occurred: No features in text. :  100\n",
      "\n",
      "An exception occurred: No features in text. :  150\n",
      "\n",
      "An exception occurred: No features in text. :  200\n",
      "\n",
      "An exception occurred: No features in text. : 2015 2016 2017 2018 2019 2020\n",
      "\n",
      "An exception occurred: No features in text. : 0.0\n",
      "\n",
      "An exception occurred: No features in text. :  20\n",
      "\n",
      "An exception occurred: No features in text. :  40\n",
      "\n",
      "An exception occurred: No features in text. :  60\n",
      "\n",
      "An exception occurred: No features in text. :  80\n",
      "\n",
      "An exception occurred: No features in text. :  100\n",
      "\n",
      "An exception occurred: No features in text. :  120\n",
      "\n",
      "An exception occurred: No features in text. :  140\n",
      "\n",
      "An exception occurred: No features in text. :  160\n",
      "\n",
      "An exception occurred: No features in text. : 2015 2016 2017 2018 2019 2020\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An exception occurred: No features in text. : 0%\n",
      "\n",
      "An exception occurred: No features in text. : 100%\n",
      "\n",
      "An exception occurred: No features in text. : 0%\n",
      "\n",
      "An exception occurred: No features in text. : 0.0\n",
      "\n",
      "An exception occurred: No features in text. :  5\n",
      "\n",
      "An exception occurred: No features in text. :  10\n",
      "\n",
      "An exception occurred: No features in text. :  15\n",
      "\n",
      "An exception occurred: No features in text. :  20\n",
      "\n",
      "An exception occurred: No features in text. :  25\n",
      "\n",
      "An exception occurred: No features in text. : 2015 2016 2017 2018 2019 2020\n",
      "\n",
      "An exception occurred: No features in text. : -48%\n",
      "\n",
      "An exception occurred: No features in text. : 419 439\n",
      "\n",
      "An exception occurred: No features in text. : 0.0\n",
      "\n",
      "An exception occurred: No features in text. :  100\n",
      "\n",
      "An exception occurred: No features in text. :  200\n",
      "\n",
      "An exception occurred: No features in text. :  300\n",
      "\n",
      "An exception occurred: No features in text. :  400\n",
      "\n",
      "An exception occurred: No features in text. :  500\n",
      "\n",
      "An exception occurred: No features in text. :  600\n",
      "\n",
      "An exception occurred: No features in text. :  700\n",
      "\n",
      "An exception occurred: No features in text. : 2015 2016 2017 2018 2019 2020\n",
      "\n",
      "An exception occurred: No features in text. : 32\n",
      "\n",
      "An exception occurred: No features in text. : 29\n",
      "\n",
      "An exception occurred: No features in text. : 32\n",
      "\n",
      "An exception occurred: No features in text. : 34 34\n",
      "\n",
      "An exception occurred: No features in text. : 35\n",
      "\n",
      "An exception occurred: No features in text. : 23%\n",
      "\n",
      "An exception occurred: No features in text. : 0%\n",
      "\n",
      "An exception occurred: No features in text. : 20%\n",
      "\n",
      "An exception occurred: No features in text. : 40%\n",
      "\n",
      "An exception occurred: No features in text. : 60%\n",
      "\n",
      "An exception occurred: No features in text. : 80%\n",
      "\n",
      "An exception occurred: No features in text. : 100%\n",
      "\n",
      "An exception occurred: No features in text. :  0\n",
      "\n",
      "An exception occurred: No features in text. :  5\n",
      "\n",
      "An exception occurred: No features in text. :  10\n",
      "\n",
      "An exception occurred: No features in text. :  15\n",
      "\n",
      "An exception occurred: No features in text. :  20\n",
      "\n",
      "An exception occurred: No features in text. :  25\n",
      "\n",
      "An exception occurred: No features in text. :  30\n",
      "\n",
      "An exception occurred: No features in text. :  35\n",
      "\n",
      "An exception occurred: No features in text. :  40\n",
      "\n",
      "An exception occurred: No features in text. : 2015 2016 2017 2018 2019 2020\n",
      "\n",
      "An exception occurred: No features in text. : 0%\n",
      "\n",
      "An exception occurred: No features in text. : 20%\n",
      "\n",
      "An exception occurred: No features in text. : 40%\n",
      "\n",
      "An exception occurred: No features in text. : 60%\n",
      "\n",
      "An exception occurred: No features in text. : 80%\n",
      "\n",
      "An exception occurred: No features in text. : 100%\n",
      "\n",
      "An exception occurred: No features in text. : <260 260-420 420-560 560-670 670-820 820-1060 >1060\n",
      "\n",
      "An exception occurred: No features in text. : 200\n",
      "\n",
      "An exception occurred: No features in text. : 0\n",
      "\n",
      "An exception occurred: No features in text. : 1\n",
      "\n",
      "An exception occurred: No features in text. : 2\n",
      "\n",
      "An exception occurred: No features in text. : 3\n",
      "\n",
      "An exception occurred: No features in text. : 4\n",
      "\n",
      "An exception occurred: No features in text. : 5 6\n",
      "\n",
      "An exception occurred: No features in text. : 7\n",
      "\n",
      "An exception occurred: No features in text. : 8\n",
      "\n",
      "An exception occurred: No features in text. : 9\n",
      "\n",
      "An exception occurred: No features in text. : 10\n",
      "\n",
      "An exception occurred: No features in text. : 11\n",
      "\n",
      "An exception occurred: No features in text. : 0%\n",
      "\n",
      "An exception occurred: No features in text. : 20%\n",
      "\n",
      "An exception occurred: No features in text. : 40%\n",
      "\n",
      "An exception occurred: No features in text. : 60%\n",
      "\n",
      "An exception occurred: No features in text. : 80%\n",
      "\n",
      "An exception occurred: No features in text. : 100%\n",
      "\n",
      "An exception occurred: No features in text. : <1.2 1.2 - 1.4 1.4 - 1.6 1.6 - 1.8 1.8 - 1.9 1.9 - 2.0 >2.0\n",
      "\n",
      "An exception occurred: No features in text. : 2.5\n",
      "\n",
      "An exception occurred: No features in text. : statistics@irena.org. \n",
      "\n",
      "Read text length: 5525\n",
      "Cleaned text length: 5498\n"
     ]
    }
   ],
   "source": [
    " # Open the file in read mode\n",
    "with open(file_path, 'r') as file:\n",
    "    \n",
    "    pattern = re.compile(r'.*?\\.{3}.*?$', re.MULTILINE)\n",
    "    # Initialize an empty string to store the lines\n",
    "    raw_text = ''\n",
    "    \n",
    "    head = [next(file) for _ in range(11)]\n",
    "    next(file)\n",
    "    \n",
    "    # Iterate over each line in the file\n",
    "    for line in file:\n",
    "        # Append the current line to the string\n",
    "        \n",
    "        if not pattern.search(line) and is_english(line):\n",
    "            raw_text += line\n",
    "            \n",
    "print(f\"Read text length: {len(raw_text)}\") \n",
    "\n",
    "text = clean_text(raw_text)\n",
    "#text = raw_text\n",
    "\n",
    "print(f\"Cleaned text length: {len(text)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 839,
   "id": "e17194e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'File Name': 'NGA-NEPro-2022-EN', 'Year': '2022', 'Country Name': 'Nigeria', 'Country Code': 'NGA', 'Category': 'NEPro', 'Document Title': 'nan', 'Publication Date': '24th August, 2022', 'Start Year': '2014', 'End Year': '2022', 'Language': 'EN'}\n"
     ]
    }
   ],
   "source": [
    "metadata = {}\n",
    "\n",
    "# Iterate through the data list\n",
    "for item in head:\n",
    "    # Split each element by ':' and strip the resulting strings\n",
    "    key, value = item.split(':')\n",
    "    key = key.strip()\n",
    "    value = value.strip()\n",
    "    \n",
    "    # Add the key-value pair to the dictionary\n",
    "    metadata[key] = value\n",
    "\n",
    "\n",
    "if 'Exists?' in metadata:\n",
    "    metadata.pop('Exists?')\n",
    "print(metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 840,
   "id": "e918387c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_sections = split_text_spacy(2000, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 841,
   "id": "c68cb471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of sections from the text: 4\n"
     ]
    }
   ],
   "source": [
    "print (f\"The number of sections from the text: {len(text_sections)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 842,
   "id": "49a9cc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_length = len(text_sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 843,
   "id": "60fb4f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the last checkpoint\n",
    "#start_index, wiki_entity_list, bert_entity_list, gpt_entity_list, acronyms = load_checkpoint(text_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 858,
   "id": "9d21a6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_index = 0\n",
    "wiki_entity_list = [''] * text_length\n",
    "bert_entity_list = [''] * text_length\n",
    "gpt_entity_list = [''] * text_length\n",
    "acronyms = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 859,
   "id": "e8df97aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WIKI DONE\n",
      "BERT DONE\n",
      "GPT DONE\n",
      "NUMBER OF PROCESSED SECTIONS: 0\n",
      "WIKI DONE\n",
      "BERT DONE\n",
      "GPT DONE\n",
      "NUMBER OF PROCESSED SECTIONS: 1\n",
      "WIKI DONE\n",
      "BERT DONE\n",
      "GPT DONE\n",
      "NUMBER OF PROCESSED SECTIONS: 2\n",
      "WIKI DONE\n",
      "BERT DONE\n",
      "GPT DONE\n",
      "NUMBER OF PROCESSED SECTIONS: 3\n",
      "TIME TAKEN TO EXTRACT ENTITIES from 4 section: 16.769766807556152\n"
     ]
    }
   ],
   "source": [
    "# Continue from the last checkpoint\n",
    "\n",
    "start_time = time.time()\n",
    "for index in range(text_length):\n",
    "    try:\n",
    "        segment = text_sections[index]\n",
    "        \n",
    "        ## WIKINEURAL BILINGUAL MODEL\n",
    "        wiki_output = query_wiki({\n",
    "            \"inputs\": segment,\n",
    "        })\n",
    "        create_entities(wiki_output)\n",
    "        wiki_words = list(set(apply_threshold(wiki_output, 0.7)))\n",
    "        wiki_entity_list[index] = wiki_words\n",
    "        print (\"WIKI DONE\")\n",
    "        ## BERT BASE MODEL\n",
    "        bert_output = query_bert({\n",
    "            \"inputs\": segment,\n",
    "        })\n",
    "        create_entities(bert_output)\n",
    "        bert_words = list(set(apply_threshold(bert_output, 0.7)))\n",
    "        bert_entity_list[index] = bert_words\n",
    "        print (\"BERT DONE\")\n",
    "\n",
    "\n",
    "        ## GPT PROMPT\n",
    "        gpt_output = query_gpt(segment)\n",
    "        gpt_entity_list[index] = gpt_output['proper_nouns']\n",
    "\n",
    "        print (\"GPT DONE\")\n",
    "\n",
    "        ## Acronyms extraction\n",
    "        acronyms.update(gpt_output['acronyms'])\n",
    "    \n",
    "        \n",
    "        print(f\"NUMBER OF PROCESSED SECTIONS: {index}\")\n",
    "\n",
    "        # Save checkpoint at intervals\n",
    "        #if index % 5 == 0:\n",
    "            #save_checkpoint(index, wiki_entity_list, bert_entity_list, gpt_entity_list, acronyms)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing section {index}: {str(e)}\")\n",
    "        #save_checkpoint(index, wiki_entity_list, bert_entity_list, gpt_entity_list, acronyms)\n",
    "\n",
    "        continue  # Exit the loop in case of an error\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"TIME TAKEN TO EXTRACT ENTITIES from {text_length} section: {elapsed_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 860,
   "id": "6df46581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'TES': 'Total Energy Supply', 'TJ': 'Terajoules', \"USD'000s\": 'United States Dollars', 'PPP': 'Purchasing Power Parity', 'GDP': 'Gross Domestic Product', 'WHO': 'World Health Organization', 'GW': 'Gigawatts', 'MW': 'Megawatts', 'TFEC': 'Total Final Energy Consumption', 'RE': 'Renewable Energy', 'CO2': 'Carbon Dioxide', 'kWh': 'Kilowatt-hours', 'NDC': 'Nationally Determined Contribution', 'FDNIS': 'Framework for the implementation of intervention facility for the national gas expansion programme', 'ECOSTAND': 'Minimum Energy Performance Standards', 'NGA': 'Nigeria', 'GWh': 'Gigawatt-hours', 'PV': 'Photovoltaic', 'MWh': 'Megawatt-hours', 'kWp': 'Kilowatt-peak', 'W/m2': 'Watts per square meter', 'NREL': 'National Renewable Energy Laboratory', 'NPP': 'Net primary production', 'IRENA': 'International Renewable Energy Agency', 'UN': 'United Nations', 'SDG': 'Sustainable Development Goal', 'IEA': 'International Energy Agency', 'UNSD': 'United Nations Statistics Division', 'COMTRADE': 'United Nations Commodity Trade Statistics Database', 'EDGAR': 'Emissions Database for Global Atmospheric Research', 'REN21': 'Renewable Energy Policy Network for the 21st Century', 'HS': 'Harmonised System'}\n"
     ]
    }
   ],
   "source": [
    "print (acronyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 861,
   "id": "5cd9ac20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Total Energy Supply']\n",
      "['T']\n",
      "['Total Energy Supply', 'Non-renewable', 'Renewable', 'Growth', 'Imports', 'Exports', 'Energy self-sufficiency', 'Coal', 'Renewables', 'Geothermal', 'Access to electricity', 'Access to clean cooking', 'Urban', 'Rural', 'GDP per capita', 'Real GDP growth rate', 'Public flows to renewables', 'Energy intensity', 'Per capita renewable capacity', 'Consumption by sector', 'Industry', 'Households', 'Other', 'Capacity change', 'Capacity utilisation', 'Renewable energy consumption', 'Net capacity change', 'Hydro and marine', 'Renewable capacity', 'Installed capacity trend', 'Electricity', 'Commercial heat', 'Bioenergy', 'Solar direct', 'Fossil fuels', 'Nuclear', 'Other', 'Hydro/marine', 'Wind', 'Solar', 'Avoided emissions', 'Calculated', 'Electricity generation', 'Energy-related CO2 emissions', 'Per capita electricity generation']\n",
      "--------\n",
      "['Nationally Determined Contribution', 'Paris Agreement : Nigeria Nigerian Economic Sustainability Plan Framework', 'Minimum Energy Performance Standards Part 2', 'Nigeria']\n",
      "['Africa World Reneenergythermal Rene', 'Nigeria', 'NL', 'Standards', 'FDNIS ECOSTA', 'National Action Plan', 'Elec', 'Paris Agreement', 'N']\n",
      "['Nigeria', 'Nigerian', 'Economic Sustainability Plan', 'national gas expansion programme', \"Nigeria's National Action Plan\", 'Renewable', 'Coal', 'Mt CO2 Emissions', 'NGA Africa World', 'Biomass', 'Indicators', 'Distribution', 'Solar', 'Wind', 'Onshore wind', 'NREL']\n",
      "--------\n",
      "['IEA', 'IRENA', 'WHO', 'Global Wind Atlas', 'United Arab Emirates', 'Harmonised System', 'World Bank', 'UN World Population Prospects', 'Masdar City', 'IRENA Global Atlas', 'World Bank Global Solar Atlas']\n",
      "['IEA', 'UNSD', 'IRENA', 'WHO', 'Masdar City United Arab Emirates', 'World Bank', 'R21', 'UN COMTDE', 'UN World Population Prospects', 'UN SDG Database', 'EGA', 'UNSD Energy Balances', 'Status', 'Global Wind Atlasised System', 'IRENA Joint Policies and Measures Database', 'IRENA Global Atlas', 'World Bank Global Solar Atlas', 'World Bank World Development Indicators']\n",
      "['Biomass', 'NPP', 'IRENA', 'Masdar City', 'United Arab Emirates', 'UN', 'SDG', 'WHO', 'World Bank', 'IEA', 'UNSD', 'UN World Population Prospects', 'UNSD Energy Balances', 'UN COMTRADE', 'World Bank World Development Indicators', 'EDGAR', 'REN21 Global Status Report', 'IEA-IRENA Joint Policies and Measures Database', 'IRENA Global Atlas', 'World Bank Global Solar Atlas', 'Global Wind Atlas', 'Capacity per capita', 'SDGs', 'Energy self-sufficiency', 'Harmonised System', 'Capacity utilisation', 'Avoided emissions', 'renewable power', 'fossil fuel generation', 'renewable energy']\n",
      "--------\n",
      "['IRENA']\n",
      "['IRENA']\n",
      "['IRENA']\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "while i < text_length:\n",
    "    #print (text_sections[i])\n",
    "    print (wiki_entity_list[i])\n",
    "    print (bert_entity_list[i])\n",
    "    print (gpt_entity_list[i])\n",
    "    print (\"--------\")\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a46f014",
   "metadata": {},
   "source": [
    "Processing the Entities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 862,
   "id": "017e28b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get raw version of entities for comparison\n",
    "raw_wiki = get_raw(wiki_entity_list)\n",
    "raw_bert = get_raw(bert_entity_list)\n",
    "raw_gpt = get_raw(gpt_entity_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 863,
   "id": "349b1d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The number of matching entities in section 0: 1\n",
      "\n",
      "['Total Energy Supply']\n",
      "\n",
      "--------------\n",
      "\n",
      "The number of matching entities in section 1: 1\n",
      "\n",
      "['Nigeria']\n",
      "\n",
      "--------------\n",
      "\n",
      "The number of matching entities in section 2: 11\n",
      "\n",
      "['IEA', 'IRENA', 'IRENA Global Atlas', 'WHO', 'United Arab Emirates', 'Global Wind Atlas', 'World Bank Global Solar Atlas', 'Harmonised System', 'World Bank', 'UN World Population Prospects', 'Masdar City']\n",
      "\n",
      "--------------\n",
      "\n",
      "The number of matching entities in section 3: 1\n",
      "\n",
      "['IRENA']\n",
      "\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "entity_objects = []\n",
    "entity_filter = set()\n",
    "merged = []\n",
    "i = 0\n",
    "\n",
    "while i < len(wiki_entity_list):\n",
    "    merged = merge_extracted_entities_old(raw_wiki[i], raw_bert[i], raw_gpt[i])\n",
    "    print (f\"\\nThe number of matching entities in section {i}: {len(merged)}\\n\")\n",
    "    print (merged)\n",
    "    \n",
    "    print (\"\\n--------------\")\n",
    "\n",
    "    entity_objects.append(merged)\n",
    "    entity_filter.update(merged)\n",
    "    \n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 864,
   "id": "a3dbc949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 864,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(entity_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 865,
   "id": "dd6a07fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Total Energy Supply'],\n",
       " ['Nigeria'],\n",
       " ['IEA',\n",
       "  'IRENA',\n",
       "  'IRENA Global Atlas',\n",
       "  'WHO',\n",
       "  'United Arab Emirates',\n",
       "  'Global Wind Atlas',\n",
       "  'World Bank Global Solar Atlas',\n",
       "  'Harmonised System',\n",
       "  'World Bank',\n",
       "  'UN World Population Prospects',\n",
       "  'Masdar City'],\n",
       " ['IRENA']]"
      ]
     },
     "execution_count": 865,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03230ac",
   "metadata": {},
   "source": [
    "Chain of Thought - Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 866,
   "id": "7c43b8d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Total Energy Supply': 'TES', 'Terajoules': 'TJ', 'United States Dollars': \"USD'000s\", 'Purchasing Power Parity': 'PPP', 'Gross Domestic Product': 'GDP', 'World Health Organization': 'WHO', 'Gigawatts': 'GW', 'Megawatts': 'MW', 'Total Final Energy Consumption': 'TFEC', 'Renewable Energy': 'RE', 'Carbon Dioxide': 'CO2', 'Kilowatt-hours': 'kWh', 'Nationally Determined Contribution': 'NDC', 'Framework for the implementation of intervention facility for the national gas expansion programme': 'FDNIS', 'Minimum Energy Performance Standards': 'ECOSTAND', 'Nigeria': 'NGA', 'Gigawatt-hours': 'GWh', 'Photovoltaic': 'PV', 'Megawatt-hours': 'MWh', 'Kilowatt-peak': 'kWp', 'Watts per square meter': 'W/m2', 'National Renewable Energy Laboratory': 'NREL', 'Net primary production': 'NPP', 'International Renewable Energy Agency': 'IRENA', 'United Nations': 'UN', 'Sustainable Development Goal': 'SDG', 'International Energy Agency': 'IEA', 'United Nations Statistics Division': 'UNSD', 'United Nations Commodity Trade Statistics Database': 'COMTRADE', 'Emissions Database for Global Atmospheric Research': 'EDGAR', 'Renewable Energy Policy Network for the 21st Century': 'REN21', 'Harmonised System': 'HS'}\n"
     ]
    }
   ],
   "source": [
    "# invert acronyms dict to ease look up\n",
    "acronyms_dict = {v: k for k, v in acronyms.items()}\n",
    "print (acronyms_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 867,
   "id": "d730afd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_list = []\n",
    "relations_list = []\n",
    "entities_with_sections = []\n",
    "relations_with_section = []\n",
    "seen_entities = set()\n",
    "seen_acronyms = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 868,
   "id": "a61fea8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CATEGORIZED ENTITIES of Section: 0\n",
      "\n",
      "[{'entity': 'Total Energy Supply', 'category': 'Product', 'acronym': 'TES'}]\n",
      "\n",
      " EXTRACTED RELATIONS: \n",
      "\n",
      "[{'Subject': 'Total Energy Supply', 'Relation': 'an instance of', 'Object': 'Total Energy Supply'}]\n",
      "\n",
      "-------------------\n",
      "CATEGORIZED ENTITIES of Section: 1\n",
      "\n",
      "[{'entity': 'Nigeria', 'category': 'Location', 'acronym': 'NGA'}]\n",
      "\n",
      " EXTRACTED RELATIONS: \n",
      "\n",
      "[{'Subject': 'Nigeria', 'Relation': 'contributes to', 'Object': 'Paris Agreement', 'Description': \"Nigeria's Nationally Determined Contribution (NDC) to the Paris Agreement\"}, {'Subject': 'Nigeria', 'Relation': 'contributes to', 'Object': 'Economic Sustainability Plan', 'Description': \"Nigeria's Nigerian Economic Sustainability Plan\"}, {'Subject': 'Nigeria', 'Relation': 'contributes to', 'Object': 'intervention facility for the national gas expansion programme', 'Description': \"Nigeria's Framework for the implementation of intervention facility for the national gas expansion programme\"}, {'Subject': 'Nigeria', 'Relation': 'contributes to', 'Object': 'Minimum Energy Performance Standards Part 2: Air conditioning products', 'Description': \"Nigeria's FDNIS ECOSTAND 071-2:2017EE: Minimum Energy Performance Standards Part 2: Air conditioning products\"}, {'Subject': 'Nigeria', 'Relation': 'contributes to', 'Object': 'National Action Plan to reduce short-lived climate pollutants', 'Description': \"Nigeria's Nigeria's National Action Plan to reduce short-lived climate pollutants\"}, {'Subject': 'Nigeria', 'Relation': 'contributes to', 'Object': 'renewable elec. & heat CO2 emission factor', 'Description': \"Nigeria's Avoided emissions from renewable elec. & heat CO2 emission factor for elec. & heat generation\"}, {'Subject': 'Nigeria', 'Relation': 'contributes to', 'Object': 'Renewable share (%)', 'Description': \"Nigeria's Renewable share (%)\"}, {'Subject': 'Nigeria', 'Relation': 'contributes to', 'Object': 'Biomass potential: net primary production', 'Description': \"Nigeria's Biomass potential: net primary production\"}, {'Subject': 'Nigeria', 'Relation': 'contributes to', 'Object': 'Distribution of solar potential', 'Description': \"Nigeria's Distribution of solar potential\"}, {'Subject': 'Nigeria', 'Relation': 'contributes to', 'Object': 'Distribution of wind potential', 'Description': \"Nigeria's Distribution of wind potential\"}, {'Subject': 'Nigeria', 'Relation': 'contributes to', 'Object': 'Proportion of land area', 'Description': \"Nigeria's Proportion of land area\"}, {'Subject': 'Nigeria', 'Relation': 'contributes to', 'Object': 'Wind power density at 100m height (W/m2)', 'Description': \"Nigeria's Wind power density at 100m height (W/m2)\"}, {'Subject': 'Nigeria', 'Relation': 'contributes to', 'Object': 'Proportion of land area', 'Description': \"Nigeria's Proportion of land area\"}, {'Subject': 'Nigeria', 'Relation': 'contributes to', 'Object': 'Annual generation per unit of installed PV capacity (MWh/kWp)', 'Description': \"Nigeria's Annual generation per unit of installed PV capacity (MWh/kWp)\"}, {'Subject': 'Nigeria', 'Relation': 'contributes to', 'Object': 'Solar PV: Solar resource potential', 'Description': \"Nigeria's Solar PV: Solar resource potential has been divided into seven classes, each representing a range of annual PV output per unit of capacity (kWh/kWp/yr).\"}, {'Subject': 'Nigeria', 'Relation': 'contributes to', 'Object': 'Potential wind power density (W/m2)', 'Description': \"Nigeria's Onshore wind: Potential wind power density (W/m2) is shown in the seven classes used by NREL, measured at a height of 100m.\"}]\n",
      "\n",
      "-------------------\n",
      "CATEGORIZED ENTITIES of Section: 2\n",
      "\n",
      "[{'entity': 'International Energy Agency', 'category': 'Organization', 'acronym': 'IEA'}, {'entity': 'International Renewable Energy Agency', 'category': 'Organization', 'acronym': 'IRENA'}, {'entity': 'IRENA Global Atlas', 'category': 'Product'}, {'entity': 'World Health Organization', 'category': 'Organization', 'acronym': 'WHO'}, {'entity': 'United Arab Emirates', 'category': 'Location'}, {'entity': 'Global Wind Atlas', 'category': 'Product'}, {'entity': 'World Bank Global Solar Atlas', 'category': 'Product'}, {'entity': 'Harmonised System', 'category': 'Product', 'acronym': 'HS'}, {'entity': 'World Bank', 'category': 'Organization'}, {'entity': 'UN World Population Prospects', 'category': 'Product'}, {'entity': 'Masdar City', 'category': 'Location'}]\n",
      "\n",
      " EXTRACTED RELATIONS: \n",
      "\n",
      "[{'Subject': 'IRENA', 'Relation': 'supports', 'Object': 'IRENA Global Atlas', 'Description': 'IRENA supports IRENA Global Atlas', 'Relevance': 8}, {'Subject': 'WHO', 'Relation': 'an_instance_of', 'Object': 'Person', 'Description': 'WHO is an instance of Person', 'Relevance': 7}, {'Subject': 'United Arab Emirates', 'Relation': 'in', 'Object': 'Masdar City', 'Description': 'United Arab Emirates is located in Masdar City', 'Relevance': 9}, {'Subject': 'World Bank Global Solar Atlas', 'Relation': 'addresses', 'Object': 'Harmonised System', 'Description': 'World Bank Global Solar Atlas addresses Harmonised System', 'Relevance': 6}, {'Subject': 'World Bank', 'Relation': 'funds', 'Object': 'UN World Population Prospects', 'Description': 'World Bank funds UN World Population Prospects', 'Relevance': 8}, {'Subject': 'IRENA', 'Relation': 'partners with', 'Object': 'World Bank', 'Description': 'IRENA partners with World Bank', 'Relevance': 9}, {'Subject': 'IRENA', 'Relation': 'monitors', 'Object': 'World Bank Global Solar Atlas', 'Description': 'IRENA monitors World Bank Global Solar Atlas', 'Relevance': 7}, {'Subject': 'IRENA', 'Relation': 'monitors', 'Object': 'Global Wind Atlas', 'Description': 'IRENA monitors Global Wind Atlas', 'Relevance': 8}, {'Subject': 'IRENA', 'Relation': 'monitors', 'Object': 'World Bank', 'Description': 'IRENA monitors World Bank', 'Relevance': 7}, {'Subject': 'IRENA', 'Relation': 'monitors', 'Object': 'UN World Population Prospects', 'Description': 'IRENA monitors UN World Population Prospects', 'Relevance': 6}, {'Subject': 'IRENA', 'Relation': 'monitors', 'Object': 'Masdar City', 'Description': 'IRENA monitors Masdar City', 'Relevance': 5}, {'Subject': 'IRENA', 'Relation': 'monitors', 'Object': 'United Arab Emirates', 'Description': 'IRENA monitors United Arab Emirates', 'Relevance': 7}, {'Subject': 'IRENA', 'Relation': 'monitors', 'Object': 'WHO', 'Description': 'IRENA monitors WHO', 'Relevance': 6}, {'Subject': 'IRENA', 'Relation': 'monitors', 'Object': 'IEA', 'Description': 'IRENA monitors IEA', 'Relevance': 7}, {'Subject': 'IRENA', 'Relation': 'monitors', 'Object': 'IRENA Global Atlas', 'Description': 'IRENA monitors IRENA Global Atlas', 'Relevance': 8}, {'Subject': 'IRENA', 'Relation': 'monitors', 'Object': 'World Bank Global Solar Atlas', 'Description': 'IRENA monitors World Bank Global Solar Atlas', 'Relevance': 7}, {'Subject': 'IRENA', 'Relation': 'monitors', 'Object': 'Harmonised System', 'Description': 'IRENA monitors Harmonised System', 'Relevance': 6}, {'Subject': 'IRENA', 'Relation': 'monitors', 'Object': 'World Bank', 'Description': 'IRENA monitors World Bank', 'Relevance': 7}, {'Subject': 'IRENA', 'Relation': 'monitors', 'Object': 'UN World Population Prospects', 'Description': 'IRENA monitors UN World Population Prospects', 'Relevance': 6}, {'Subject': 'IRENA', 'Relation': 'monitors', 'Object': 'Masdar City', 'Description': 'IRENA monitors Masdar City', 'Relevance': 5}, {'Subject': 'IRENA', 'Relation': 'monitors', 'Object': 'United Arab Emirates', 'Description': 'IRENA monitors United Arab Emirates', 'Relevance': 7}, {'Subject': 'IRENA', 'Relation': 'monitors', 'Object': 'WHO', 'Description': 'IRENA monitors WHO', 'Relevance': 6}, {'Subject': 'IRENA', 'Relation': 'monitors', 'Object': 'IEA', 'Description': 'IRENA monitors IEA', 'Relevance': 7}, {'Subject': 'IRENA', 'Relation': 'monitors', 'Object': 'IRENA Global Atlas', 'Description': 'IRENA monitors IRENA Global Atlas', 'Relevance': 8}]\n",
      "\n",
      "-------------------\n",
      "CATEGORIZED ENTITIES of Section: 3\n",
      "\n",
      "[{'entity': 'IRENA', 'category': 'Organization'}]\n",
      "\n",
      " EXTRACTED RELATIONS: \n",
      "\n",
      "[{'Subject': 'IRENA', 'Relation': 'focuses on', 'Object': 'renewable energy', 'Description': 'These profiles have been produced to provide an overview of developments in renewable energy in different countries and areas.', 'Relevance': 8}]\n",
      "\n",
      "-------------------\n",
      "TIME TAKEN TO EXTRACT RELATIONS FROM 4 SECTIONS: 23.82150101661682\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for index, uncategorized_entities in enumerate(entity_objects):\n",
    "    try:\n",
    "        if len(uncategorized_entities) == 0:\n",
    "            print (\"-- Empty Entities--\")\n",
    "            print (\"\\n-------------------\")\n",
    "\n",
    "            continue\n",
    "        entities_subset = categorize_entities(text_sections[index], uncategorized_entities, categories)\n",
    "        #print(seen_acronyms)\n",
    "\n",
    "        # Add 'acronym' key to entity list\n",
    "        for item in entities_subset:\n",
    "            if item[\"entity\"] not in seen_entities and item[\"entity\"] not in seen_acronyms:\n",
    "                seen_entities.add(item[\"entity\"])\n",
    "                \n",
    "                \n",
    "                if item[\"entity\"] in acronyms_dict.keys():\n",
    "                    item[\"acronym\"] = acronyms_dict[item[\"entity\"]]\n",
    "                    seen_acronyms.add(item['acronym'])\n",
    "                    #print(\"SEEN ACRONYM\" + str(item))\n",
    "                    \n",
    "                elif item['entity'] in acronyms.keys():\n",
    "                    item[\"acronym\"] = item['entity']\n",
    "                    item[\"entity\"] = acronyms[item[\"entity\"]]\n",
    "                    seen_acronyms.add(item['acronym'])\n",
    "                    #print(\"SEEN ACRONYM\" + str(item))\n",
    "                    \n",
    "                entities_list.append(item)\n",
    "\n",
    "        print (\"CATEGORIZED ENTITIES of Section: \" + str(index) + \"\\n\")\n",
    "        print (entities_subset)\n",
    "        #store the categorized entities in order of lists for later processing\n",
    "        #entities_with_sections.append(entities_subset)\n",
    "\n",
    "        relations_subset = extract_ontology_relations(text_sections[index], entity_objects[index], ontology)\n",
    "\n",
    "        print (\"\\n EXTRACTED RELATIONS: \\n\")\n",
    "        print (relations_subset)\n",
    "\n",
    "        relations_list.extend(relations_subset)\n",
    "        relations_with_section.append(relations_subset)\n",
    "\n",
    "        print (\"\\n-------------------\")\n",
    "\n",
    "    except Exception as e:\n",
    "            print(f\"Error processing section {index}: {str(e)}\")\n",
    "            #save_checkpoint(index, wiki_entity_list, bert_entity_list, gpt_entity_list, acronyms)\n",
    "\n",
    "    continue  # Exit the loop in case of an error\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"TIME TAKEN TO EXTRACT RELATIONS FROM {text_length} SECTIONS: {elapsed_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 869,
   "id": "49ee4e5f",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "filtered_entities_list = []\n",
    "for i in entities_list:\n",
    "    if i['entity'] in entity_filter:\n",
    "        filtered_entities_list.append(i)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 870,
   "id": "0e3d9d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "10\n",
      "42\n"
     ]
    }
   ],
   "source": [
    "print(len(entities_list))\n",
    "print(len(filtered_entities_list))\n",
    "print (len(relations_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 871,
   "id": "d3de5f98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No entity found with the label: Total Energy Supply\n",
      "Entity found with DBpedia URI: http://dbpedia.org/resource/Category:Nigeria\n",
      "No abstract or comment found for this entity.\n",
      "No entity found with the label: IRENA Global Atlas\n",
      "Entity found with DBpedia URI: http://dbpedia.org/resource/United_Arab_Emirates\n",
      "Entity found with DBpedia URI: http://dbpedia.org/resource/Global_Wind_Atlas\n",
      "No entity found with the label: World Bank Global Solar Atlas\n",
      "Entity found with DBpedia URI: http://dbpedia.org/resource/Harmonised_System\n",
      "No abstract or comment found for this entity.\n",
      "Entity found with DBpedia URI: http://dbpedia.org/resource/Category:World_Bank\n",
      "No abstract or comment found for this entity.\n",
      "No entity found with the label: UN World Population Prospects\n",
      "Entity found with DBpedia URI: http://dbpedia.org/resource/Masdar_City\n"
     ]
    }
   ],
   "source": [
    "entity_summaries = extract_summaries(filtered_entities_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 872,
   "id": "0cff8b0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 872,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(entity_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 873,
   "id": "21e264dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a list of names, to check for valid relations\n",
    "\n",
    "entity_names = set([item['entity'] for item in filtered_entities_list])\n",
    "entity_names.update([item['acronym'] for item in filtered_entities_list if 'acronym' in item] )\n",
    "#entity_names.update(acronyms.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 874,
   "id": "5915e211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "print (len(entity_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 875,
   "id": "af015795",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_dic = {}\n",
    "\n",
    "for i in entity_summaries:\n",
    "    entity_dic[i['entity']] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 876,
   "id": "3445b2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_entities = []\n",
    "for i in entity_dic.values():\n",
    "    final_entities.append(i)\n",
    "    \n",
    "json_entities = json.dumps(final_entities, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 877,
   "id": "2d99a090",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Entities/' + metadata['File Name']+ '.json', \"w\") as output_file:\n",
    "    output_file.write(json_entities)\n",
    "    output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 878,
   "id": "24872f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(final_entities))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 879,
   "id": "7589070c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Entities/' + metadata['File Name']+ '.json', \"r\") as f:\n",
    "    data = f.read()\n",
    "    ent = json.loads(data)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ce9275",
   "metadata": {},
   "source": [
    "# Write the relations to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 880,
   "id": "9d1beff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "final_relations = []\n",
    "\n",
    "for i in relations_list:\n",
    "    i['Relation'] = i['Relation'].replace(\" \", \"_\")\n",
    "    if 'Subject' in i and i['Subject'] in entity_names and i['Object'] in entity_names and i['Relation'] in relation_labels:\n",
    "        final_relations.append(i)\n",
    "    #elif 'Description' in i and 'Subject' in i and i['Subject'] in entity_dic.keys():\n",
    "        #entity_dic[i['Subject']].update({'information':i['Description']})\n",
    "    #elif 'Description' in i and 'Object' in i and i['Object'] in entity_dic.keys():\n",
    "        #entity_dic[i['Object']].update({'information':i['Description']})\n",
    "print (len(final_relations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 881,
   "id": "b071ef18",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_relations = json.dumps(final_relations, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 882,
   "id": "8c596dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Relations/' + metadata['File Name']+ '.json', \"w\") as output_file:\n",
    "    output_file.write(json_relations)\n",
    "    output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 883,
   "id": "c007af4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_relations_neo4j(final_entities, final_relations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06754a38",
   "metadata": {},
   "source": [
    "# Add Relations to Spreadsheet for Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62e2121",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "\n",
    "# Use the credentials from the service account key JSON file you downloaded\n",
    "scope = [\"https://spreadsheets.google.com/feeds\", \"https://www.googleapis.com/auth/drive\"]\n",
    "creds = ServiceAccountCredentials.from_json_keyfile_name('energy-moonshot-ai-97aa9045e45f.json', scope)\n",
    "client = gspread.authorize(creds)\n",
    "\n",
    "# Open the Google Sheet by its title or URL\n",
    "spreadsheet = client.open_by_url('https://docs.google.com/spreadsheets/d/1yZ-XQQs52kaI5k9MjvV_CdbgWQi-GazjHHGqQUF8gko/edit')\n",
    "\n",
    "\n",
    "# Enter relations in the first sheet\n",
    "sheet = spreadsheet.get_worksheet(0)\n",
    "\n",
    "# Start row index from 5\n",
    "start_row_index = 5\n",
    "index = 1\n",
    "\n",
    "# Check if there's valid data to insert\n",
    "if final_relations:\n",
    "    # Create a list of lists where each inner list represents the values of a row\n",
    "    batch_relations = []\n",
    "    for index, row_data in enumerate(final_relations):\n",
    "        row = [index, row_data['Subject'], row_data['Relation'], row_data.get('Object', ''), \n",
    "               row_data.get('Description', ''), row_data.get('Relevance', '')]\n",
    "        \n",
    "        batch_relations.append(row)\n",
    "        index = index + 1\n",
    "\n",
    "    # Insert the data into the Google Sheet starting from row 5\n",
    "    sheet.insert_rows(batch_relations, start_row_index)\n",
    "\n",
    "    print(f\"{len(final_relations)} entries added to Google Sheet.\")\n",
    "else:\n",
    "    print(\"No data to insert.\")\n",
    "    \n",
    "    \n",
    "# Enter entities in the second sheet\n",
    "sheet = spreadsheet.get_worksheet(1)\n",
    "\n",
    "\n",
    "# Start row index from 5\n",
    "start_row_index = 5\n",
    "index = 1\n",
    "\n",
    "if final_entities:\n",
    "    batch_entities = []\n",
    "    for index, row_data in enumerate(final_entities):\n",
    "        row = [index, row_data['entity'], row_data['category'], row_data.get('acronym', ''), row_data.get('summary', '')]\n",
    "        batch_entities.append(row)\n",
    "        \n",
    "        index = index + 1\n",
    "    sheet.insert_rows(batch_entities, start_row_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02afb885",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
