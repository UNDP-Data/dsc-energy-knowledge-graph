{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24b0a798",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-04 19:37:02.650025: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "\n",
    "import numpy as np\n",
    "import torch, torchvision\n",
    "from transformers import AutoTokenizer, TFAutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "from span_marker import SpanMarkerModel\n",
    "from langdetect import detect\n",
    "from langdetect import LangDetectException\n",
    "import re\n",
    "import pickle\n",
    "import time\n",
    "from langchain.text_splitter import SpacyTextSplitter\n",
    "\n",
    "#nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d297b4e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python-dotenv could not parse statement starting at line 10\n",
      "Python-dotenv could not parse statement starting at line 11\n",
      "Python-dotenv could not parse statement starting at line 12\n",
      "Python-dotenv could not parse statement starting at line 13\n",
      "Python-dotenv could not parse statement starting at line 14\n",
      "Python-dotenv could not parse statement starting at line 15\n",
      "Python-dotenv could not parse statement starting at line 16\n",
      "Python-dotenv could not parse statement starting at line 17\n",
      "Python-dotenv could not parse statement starting at line 18\n",
      "Python-dotenv could not parse statement starting at line 19\n",
      "Python-dotenv could not parse statement starting at line 20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfc59a5",
   "metadata": {},
   "source": [
    "Initialize Entity Categories and Relation Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f5423cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\n",
    "    \n",
    "    \"Person\",\n",
    "    \"Location\",\n",
    "    \"Organization\",\n",
    "    \"Event\",\n",
    "    \"Product\",\n",
    "    \"Project\",\n",
    "    \"Skill\",\n",
    "    \"Strategy\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2b0aa441",
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_labels = [\n",
    "    \"implements\",\n",
    "    \"funds\",\n",
    "    \"focuses on\",\n",
    "    \"in\",\n",
    "    \"partners with\",\n",
    "    \"contributes to\",\n",
    "    \"monitors\",\n",
    "    \"targets\",\n",
    "    \"addresses\",\n",
    "    \"employs\",\n",
    "    \"collaborates with\",\n",
    "    \"supports\",\n",
    "    \"administers\",\n",
    "    \"measures\",\n",
    "    \"aligns with\",\n",
    "    \"an instance of\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e5f1a1",
   "metadata": {},
   "source": [
    "# Setting up OpenAI connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a472f8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI API configuration\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai.api_base = os.getenv(\"OPENAI_API_BASE\")\n",
    "openai.api_version = os.getenv(\"OPENAI_API_VERSION\")\n",
    "openai_deployment = \"sdgi-gpt-35-turbo-16k\"\n",
    "\n",
    "neo4j_pass = os.getenv(\"NEO4JPASS\")\n",
    "#openai.api_key = os.getenv(\"OPENAI_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76acd36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_answer(user_question, timeout_seconds):\n",
    "    messages = [\n",
    "        {'role': 'user', 'content': user_question},\n",
    "    ]\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            engine=\"sdgi-gpt-35-turbo-16k\", \n",
    "            messages=messages,\n",
    "            temperature=0.2,\n",
    "            request_timeout = timeout_seconds\n",
    "            # max_tokens=2000\n",
    "        )\n",
    "        return response.choices[0].message[\"content\"]\n",
    "    except requests.Timeout:\n",
    "        print(f\"Request timed out\")\n",
    "        return []\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e328a35",
   "metadata": {},
   "source": [
    "# Entity Extraction using Transformers \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f369316f",
   "metadata": {},
   "outputs": [],
   "source": [
    "WIKI_API = \"https://api-inference.huggingface.co/models/Babelscape/wikineural-multilingual-ner\"\n",
    "BERT_API = \"https://api-inference.huggingface.co/models/dslim/bert-base-NER\"\n",
    "\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "def query_wiki(payload):\n",
    "\tresponse = requests.post(WIKI_API, headers=headers, json=payload)\n",
    "\treturn response.json()\n",
    "\t\n",
    "\n",
    "def query_bert(payload):\n",
    "\tresponse = requests.post(BERT_API, headers=headers, json=payload)\n",
    "\treturn response.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb268621",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_gpt(text):\n",
    "    \n",
    "    entities_prompt = f\"\"\"\n",
    "\n",
    "    You will be given a >>>>>TEXT<<<<<. You have two tasks:\n",
    "    \n",
    "    1. Your first task is to detect acronyms with their names and store them in python dictionary.\n",
    "    2. Your second task is to detect Proper Nouns in the text and store them in python list.\n",
    "    \n",
    "    Return a JSON array contaning dictionary and the list.\n",
    "\n",
    "    >>>>>TEXT<<<<<\n",
    "    {text}\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    #start_time = time.time()\n",
    "\n",
    "    result = get_answer(entities_prompt, 10)\n",
    "    result = json.loads(result)\n",
    "    \n",
    "    #end_time = time.time()\n",
    "    #elapsed_time = end_time - start_time\n",
    "    #print (f\"TIME TAKEN TO EXECUTE PROMPT: {elapsed_time}\")\n",
    "    return result\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4160364",
   "metadata": {},
   "source": [
    "# Text Pre-Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b140f501",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_spacy(chunk_size, text):\n",
    "    \n",
    "    text_splitter = SpacyTextSplitter(chunk_size=chunk_size)\n",
    "    sections = text_splitter.split_text(text)\n",
    "    \n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a13f4641",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_section(limit, text):\n",
    "    sections_list = []\n",
    "    length = len(text)\n",
    "    i = 0\n",
    "\n",
    "    while i < length - 1:\n",
    "        j = i + limit\n",
    "\n",
    "        if j >= length:\n",
    "            j = length - 1\n",
    "        elif text[j] not in ('.', '\\n', ';'):\n",
    "            while text[j] not in ('.', '\\n', ';'):\n",
    "                j -= 1\n",
    "            j += 1\n",
    "\n",
    "        section = text[i:j]\n",
    "\n",
    "        if is_valid_section(section):\n",
    "            sections_list.append(section)\n",
    "        else: \n",
    "            print(\"INVALID SECTION DETECTED\")\n",
    "            print(section)\n",
    "            #section_list[-1].extend(section)\n",
    "        i = j\n",
    "    \n",
    "    \n",
    "    return sections_list\n",
    "\n",
    "def is_valid_section(section):\n",
    "    return section and len(section) > 20\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4066d417",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(input_text):\n",
    "    # Remove lines with only whitespace\n",
    "    input_text = re.sub(r'^\\s*$', '', input_text, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove lines containing only uppercase text (potential headings)\n",
    "    input_text = re.sub(r'^\\s*[A-Z\\s]+\\s*$', '', input_text, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove lines with multiple consecutive uppercase words (potential headings)\n",
    "    input_text = re.sub(r'^\\s*(?:[A-Z]+\\s*){2,}\\s*$', '', input_text, flags=re.MULTILINE)\n",
    "    \n",
    "    input_text = re.sub(r'^\\s*[A-Za-z\\s]+\\.{3,}\\s*\\d+\\s*$', '', input_text, flags=re.MULTILINE)\n",
    "\n",
    "    return input_text\n",
    "\n",
    "def is_english(line):\n",
    "    try:\n",
    "        return detect(line) == 'en'\n",
    "    except LangDetectException as e:\n",
    "        print(f\"An exception occurred: {e} : {line}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38eb33c",
   "metadata": {},
   "source": [
    "# Entities Post-Processing Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec7e10db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging the broken entities\n",
    "def create_entities(lst):\n",
    "    i = 1\n",
    "    while i < len(lst):\n",
    "        if lst[i][\"word\"].startswith('##'):\n",
    "            lst[i][\"word\"] = lst[i-1][\"word\"] + lst[i][\"word\"][2:]\n",
    "            lst[i][\"score\"] = max(lst[i-1][\"score\"] , lst[i][\"score\"])\n",
    "            del lst[i-1]\n",
    "        else:\n",
    "            i += 1\n",
    "            # todo: return a list of merged entities\n",
    "            \n",
    "\n",
    "\n",
    "def apply_threshold(list_, threshold):\n",
    "    words_list = []\n",
    "    for item in list_:\n",
    "        if item['score'] > threshold:  # threshold score to eliminate unimportant entities\n",
    "            words_list.append(item['word'])\n",
    "    return words_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f27e0af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw(list_):\n",
    "    output = []\n",
    "    for sublist in list_:\n",
    "        new = []\n",
    "        obj = {}\n",
    "        for item in sublist:\n",
    "            #obj = {}\n",
    "            key = ''.join(filter(str.isalpha, item))\n",
    "            obj[key]= item\n",
    "            #obj['raw']= ''.join(filter(str.isalpha, item))\n",
    "        output.append(obj)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5c833db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_extracted_entities_old(wiki, bert, gpt):\n",
    "    \n",
    "    output = []\n",
    "    dict_ = gpt\n",
    "    \n",
    "    \n",
    "    #print(dict_.items())\n",
    "    wiki_set = set(wiki.keys())\n",
    "    #bert_set = set(bert.keys())\n",
    "    gpt_set = set(gpt.keys())\n",
    "    \n",
    "    \n",
    "    #A = gpt_set.intersection(bert_set)\n",
    "    #B = bert_set.intersection(wiki_set)\n",
    "    C = wiki_set.intersection(gpt_set)\n",
    " \n",
    "    #matched = list(A.union(B).union(C))\n",
    "    \n",
    "    \n",
    "    for i in C:\n",
    "        output.append(dict_[i])\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e749549e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_extracted_entities(wiki, bert, gpt):\n",
    "    \n",
    "    output = set(wiki.values())\n",
    "    dict_ = gpt\n",
    "    \n",
    "    bert_set = set(bert.keys()) - set(wiki.keys())\n",
    "    gpt_set = set(gpt.keys())\n",
    "    \n",
    "    A = gpt_set.intersection(bert_set)\n",
    "\n",
    "    matched = list(set(A))\n",
    "    print (\"GPT/BERT: \" + str(matched))\n",
    "\n",
    "    for i in matched:\n",
    "        output.add(dict_[i])\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ff671cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_entities(list_):\n",
    "    \n",
    "    # Define a regular expression pattern to match invalid characters.\n",
    "    pattern = r'\\s*{}\\s*'.format(re.escape(\"’\"))\n",
    "    pattern1 = r'\\s*{}\\s*'.format(re.escape(\"/\"))\n",
    "    output_list = []\n",
    "\n",
    "    for item in list_:\n",
    "        item = re.sub(pattern, \"’\", item)\n",
    "        tem = re.sub(pattern1, \"/\", item)\n",
    "            \n",
    "    return output_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f074fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(index, wiki, bert, gpt, acronym):\n",
    "    checkpoint = {'index': index, 'wiki': wiki, 'bert': bert, 'gpt': gpt, 'acronym': acronym}\n",
    "    with open('checkpoint.pkl', 'wb') as checkpoint_file:\n",
    "        pickle.dump(checkpoint, checkpoint_file)\n",
    "\n",
    "# Function to load the state\n",
    "def load_checkpoint(length):\n",
    "    try:\n",
    "        with open('checkpoint.pkl', 'rb') as checkpoint_file:\n",
    "            checkpoint = pickle.load(checkpoint_file)\n",
    "            return checkpoint['index'], checkpoint['wiki'], checkpoint['bert'], \n",
    "        checkpoint['gpt'], checkpoint['acronym']\n",
    "    except FileNotFoundError:\n",
    "        return 0, [''] * length, [''] * length, [''] * length, {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b6c56a",
   "metadata": {},
   "source": [
    "#  Categorize entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "197cdc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_entities(text, entities, categories):\n",
    "    \n",
    "    \n",
    "    categorization_prompt = f\"\"\"\n",
    "\n",
    "    You will be given a >>>>>TEXT<<<<<, an >>>>>EntityList<<<<< and >>>>>Categories<<<<<. \n",
    "    Your task is to assign a sutiable category to each element of >>>>>EntityList<<<<<.\n",
    "    \n",
    "    Return a list of JSON objects of categorized entities. \n",
    "\n",
    "\n",
    "    >>>>>TEXT<<<<<\n",
    "    {text}\n",
    "\n",
    "    >>>>>Categories<<<<<\n",
    "    {categories}\n",
    "\n",
    "    >>>>>EntityList<<<<<\n",
    "    {entities}\n",
    "    \"\"\"\n",
    "\n",
    "    categorized_entities = get_answer(categorization_prompt, 30)\n",
    "    categorized_entities = json.loads(categorized_entities)\n",
    "    \n",
    "    return (categorized_entities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0d1e03",
   "metadata": {},
   "source": [
    "# Relation Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ebf0d4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_relation_details(text, entities, relation_labels):\n",
    "    relation_extraction_prompt = f\"\"\"\n",
    "    \n",
    "    [Context]\n",
    "    You belong to a team of consultants at UNDP's Sustainable Energy Hub (SEH), working on a project to extract a \n",
    "    Knowledge Graph from the UNDP dataset.\n",
    "    You will be given a >>>>>TEXT<<<<<, an >>>>>EntityList<<<<< and a list of >>>>>RelationLabels<<<<<.\n",
    "\n",
    "   [Task]\n",
    "   \n",
    "   Your task is to perform Relation Extraction on the given >>>>>TEXT<<<<< \n",
    "   to find relations between elements of provided >>>>>EntityList<<<<<.\n",
    "   \n",
    "   Please make sure to read these instructions and constraints carefully.\n",
    "\n",
    "    [Instructions]\n",
    "    1. Carefully read and store the >>>>>RelationLabels<<<<<.\n",
    "    2. Scan the >>>>>TEXT<<<<< to find Named Entites from >>>>>EntityList<<<<< that are related.\n",
    "    3. Scan the >>>>>RelationLabels<<<<< to select a suitable label to\n",
    "    describe the relation between the above selected entities. Mark this label as \"Relation\".\n",
    "    4. Assign \"Subject\" and \"Object\" to entities depending on the selected \"Relation\"\n",
    "    selected in previous step to create a tuple.\n",
    "    5. If available, select a small \"Description\" from the >>>>>TEXT<<<<< for the above relation.\n",
    "    6. Assign a Relevance score between 1 to 10 to the extracted relation, with 10 being the most relevant.\n",
    "    7. Repeat the process to extract remaining relations from >>>>>TEXT<<<<<.\n",
    "    \n",
    "    \n",
    "    [Constraints]\n",
    "    1. Values of 'Relation' key should belong to >>>>>RelationLabels<<<<<.\n",
    "    \n",
    "    [Output Format]\n",
    "    Provide the result as a JSON array.\n",
    "\n",
    "    Perform relation extraction on the below:\n",
    "    \n",
    "    >>>>>TEXT<<<<<\n",
    "    {text}\n",
    "\n",
    "    >>>>>EntityList<<<<<\n",
    "    {entities}\n",
    "\n",
    "    >>>>>RelationLabels<<<<<\n",
    "    {relation_labels}\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "    relations = get_answer(relation_extraction_prompt,60)\n",
    "    relations = json.loads(relations)\n",
    "\n",
    "    return relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "513a5f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ontology_relations(text, entities, ontology):\n",
    "    relation_extraction_prompt = f\"\"\"\n",
    "    \n",
    "    [Context]\n",
    "    You belong to a team of consultants at UNDP's Sustainable Energy Hub (SEH), working on a project to extract a \n",
    "    Knowledge Graph from the UNDP dataset.\n",
    "    You will be given a >>>>>TEXT<<<<<, an >>>>>EntityList<<<<< and an >>>>>ONTOLOGY<<<<<.\n",
    "\n",
    "   [Task]\n",
    "   \n",
    "   Your task is to perform Relation Extraction on the given >>>>>TEXT<<<<< \n",
    "   to find relations between elements of provided >>>>>EntityList<<<<<. Use the given >>>>>ONTOLOGY<<<<<\n",
    "   for this purpose.\n",
    "   \n",
    "   Please make sure to read these instructions and constraints carefully.\n",
    "\n",
    "    [Instructions]\n",
    "    1. Carefully read and understand the >>>>>ONTOLOGY<<<<<.\n",
    "    2. Scan the >>>>>TEXT<<<<< to find Named Entites in >>>>>EntityList<<<<< that are related.\n",
    "    3. Read the >>>>>ONTOLOGY<<<<< to select a relationship type for the related entities. Mark this label as \"Relation\".\n",
    "    4. Assign \"Subject\" and \"Object\" to entities depending on the \"Relation\"\n",
    "    selected in previous step to create a tuple.\n",
    "    5. If available, select a small \"Description\" from the >>>>>TEXT<<<<< for the above relation.\n",
    "    6. Assign a Relevance score between 1 to 10 to the extracted relation, with 10 being the most relevant.\n",
    "    7. Repeat the process to extract remaining relations from >>>>>TEXT<<<<<.\n",
    "    \n",
    "    \n",
    "    [Constraints]\n",
    "    1. Values of 'Relation' key should be a label from properties in >>>>>ONTOLOGY<<<<<.\n",
    "    \n",
    "    [Output Format]\n",
    "    Provide the result as a JSON array.\n",
    "\n",
    "    Perform relation extraction on the below:\n",
    "    \n",
    "    >>>>>TEXT<<<<<\n",
    "    {text}\n",
    "\n",
    "    >>>>>EntityList<<<<<\n",
    "    {entities}\n",
    "\n",
    "    >>>>>ONTOLOGY<<<<<\n",
    "    {ontology}\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "    relations = get_answer(relation_extraction_prompt,60)\n",
    "    relations = json.loads(relations)\n",
    "\n",
    "    return relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f646343a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ontology_relations_3(text, entities, ontology):\n",
    "    relation_extraction_prompt = f\"\"\"\n",
    "    \n",
    "    [Context]\n",
    "    You belong to a team of consultants at UNDP's Sustainable Energy Hub (SEH), working on a project to extract a \n",
    "    Knowledge Graph from the UNDP dataset.\n",
    "    You will be given a text >>>>>TEXT<<<<<, and an ontology >>>>>ONTOLOGY<<<<<.\n",
    "\n",
    "   [Task]\n",
    "   \n",
    "   Your task is to perform Relation Extraction on the given >>>>>TEXT<<<<< \n",
    "   to find relations between named entities based on given >>>>>ONTOLOGY<<<<<.\n",
    "   \n",
    "   Please make sure to read these instructions and constraints carefully.\n",
    "\n",
    "    [Instructions]\n",
    "    1. Carefully read and understand the >>>>>ONTOLOGY<<<<<.\n",
    "    2. Identify all related Proper Nouns in the text.\n",
    "    3. Use the >>>>>ONTOLOGY<<<<< to select a relationship type for the related Proper Nouns. \n",
    "    4. Assign \"Subject\" and \"Object\" to entities depending on the selected \"Relation\"\n",
    "    selected in previous step to create a tuple.\n",
    "    5. If available, select a small \"Description\" from the >>>>>TEXT<<<<< for the above relation.\n",
    "    6. Assign a Relevance score between 1 to 10 to the extracted relation, with 10 being the most relevant.\n",
    "    7. Repeat the process to extract remaining relations from >>>>>TEXT<<<<<.\n",
    "    \n",
    "    \n",
    "    [Constraints]\n",
    "    1. Values of 'Relation' key should be according a label from properties in >>>>>ONTOLOGY<<<<<.\n",
    "    2. The extracted relaions must abide by >>>>>ONTOLOGY<<<<<.\n",
    "    \n",
    "    [Output Format]\n",
    "    Provide the result as a JSON array.\n",
    "\n",
    "    Perform relation extraction on the below:\n",
    "    \n",
    "    >>>>>TEXT<<<<<\n",
    "    {text}\n",
    "\n",
    "    >>>>>EntityList<<<<<\n",
    "    {entities}\n",
    "\n",
    "    >>>>>ONTOLOGY<<<<<\n",
    "    {ontology}\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "    relations = get_answer(relation_extraction_prompt,60)\n",
    "    relations = json.loads(relations)\n",
    "\n",
    "    return relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ffc4f23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ontology.ttl\") as f:\n",
    "    ontology = f.read()\n",
    "    \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4862fd69",
   "metadata": {},
   "source": [
    "# Connecting with DBpedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2a53f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "# Define the DBpedia SPARQL endpoint\n",
    "sparql_endpoint = \"http://dbpedia.org/sparql\"\n",
    "\n",
    "# Create a SPARQLWrapper instance\n",
    "sparql = SPARQLWrapper(sparql_endpoint)\n",
    "\n",
    "# Function to search for an entity by label and return its DBpedia URI\n",
    "def search_entity(label):\n",
    "    query = f\"\"\"\n",
    "    SELECT ?entity\n",
    "    WHERE {{\n",
    "      ?entity rdfs:label \"{label}\"@en.\n",
    "    }}\n",
    "    LIMIT 1\n",
    "    \"\"\"\n",
    "\n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "\n",
    "    results = sparql.query().convert()\n",
    "\n",
    "    if \"results\" in results and \"bindings\" in results[\"results\"] and results[\"results\"][\"bindings\"]:\n",
    "        entity_uri = results[\"results\"][\"bindings\"][0][\"entity\"][\"value\"]\n",
    "        return entity_uri\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Function to retrieve and return the abstract or comment of an entity\n",
    "def retrieve_entity_summary(entity_uri):\n",
    "    # Try to retrieve the abstract\n",
    "    abstract_query = f\"\"\"\n",
    "    SELECT ?abstract\n",
    "    WHERE {{\n",
    "      <{entity_uri}> dbo:abstract ?abstract.\n",
    "      FILTER (LANGMATCHES(LANG(?abstract), \"en\"))\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    sparql.setQuery(abstract_query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "\n",
    "    abstract_results = sparql.query().convert()\n",
    "\n",
    "    if \"results\" in abstract_results and \"bindings\" in abstract_results[\"results\"]:\n",
    "        for result in abstract_results[\"results\"][\"bindings\"]:\n",
    "            abstract = result[\"abstract\"][\"value\"]\n",
    "            return abstract\n",
    "\n",
    "    # If abstract is not found, try to retrieve the comment\n",
    "    comment_query = f\"\"\"\n",
    "    SELECT ?comment\n",
    "    WHERE {{\n",
    "      <{entity_uri}> rdfs:comment ?comment.\n",
    "      FILTER (LANGMATCHES(LANG(?comment), \"en\"))\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    sparql.setQuery(comment_query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "\n",
    "    comment_results = sparql.query().convert()\n",
    "\n",
    "    if \"results\" in comment_results and \"bindings\" in comment_results[\"results\"]:\n",
    "        for result in comment_results[\"results\"][\"bindings\"]:\n",
    "            comment = result[\"comment\"][\"value\"]\n",
    "            return comment\n",
    "\n",
    "    # If neither abstract nor comment is found, return None\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c57444",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.error\n",
    "\n",
    "def dbpedia_summary(search_label):\n",
    "    entity_uri = search_entity(search_label)\n",
    "\n",
    "    if entity_uri:\n",
    "        print(f\"Entity found with DBpedia URI: {entity_uri}\")\n",
    "        try:\n",
    "            summary = retrieve_entity_summary(entity_uri)\n",
    "            if summary:\n",
    "                return summary\n",
    "            else:\n",
    "                print(\"No abstract or comment found for this entity.\")\n",
    "        except urllib.error.URLError as e:\n",
    "            print(f\"Error: {e}\")\n",
    "    else:\n",
    "        print(f\"No entity found with the label: {search_label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be620ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_summaries(entities):\n",
    "    \n",
    "    updated_entities = []\n",
    "\n",
    "    for item in entities:\n",
    "        try:\n",
    "            summary = dbpedia_summary(item['entity'])\n",
    "            if summary:\n",
    "                # Only add the summary if it's not None or empty\n",
    "                item['summary'] = summary\n",
    "            updated_entities.append(item)  # Add the item regardless of summary presence\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return updated_entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7975f7",
   "metadata": {},
   "source": [
    "# Creating Graph in Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2cd7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph, Node, Relationship\n",
    "graph = Graph(uri = 'bolt://localhost:7687',user='neo4j',password=neo4j_pass)\n",
    "\n",
    "class Document:\n",
    "    def __init__(self, metadata, entities, relations):\n",
    "        self.metadata = metadata\n",
    "        self.entities = entities\n",
    "        self.relations = relations\n",
    "\n",
    "# Define a function to create or retrieve a node\n",
    "def get_or_create_node(label, key, value):\n",
    "    # Attempt to find an existing node with the given label and key\n",
    "    existing_node = get_node(label, key, value)\n",
    "    \n",
    "    if existing_node:\n",
    "        return existing_node\n",
    "    else:\n",
    "        new_node = Node(label, **{key: value})\n",
    "        graph.create(new_node)\n",
    "        return new_node\n",
    "    \n",
    "def get_node(label, key, value):\n",
    "    node = graph.nodes.match(label, **{key:value}).first()\n",
    "    return node\n",
    "\n",
    "# Define a function to insert relations \n",
    "def insert_relations_neo4j(document):\n",
    "    document_node = get_or_create_node(\"Document\", \"name\", document.metadata['Document Title'] )\n",
    "            \n",
    "    for key,value in metadata.items():\n",
    "        document_node[key] = value\n",
    "        \n",
    "    graph.push(document_node)\n",
    "    \n",
    "    for item in document.entities:\n",
    "        \n",
    "        node = get_or_create_node(\"Entity\", \"name\", item[\"entity\"])\n",
    "        node['category'] = item[\"category\"]\n",
    "        if \"acronym\" in item:\n",
    "            node['acronym'] = item[\"acronym\"]\n",
    "        if \"summary\" in item:\n",
    "            node['summary'] = item[\"summary\"]\n",
    "        \n",
    "        graph.push(node)\n",
    "        entity_rel = Relationship(node, 'parent_document', document_node)\n",
    "        \n",
    "        if \"information\" in item:\n",
    "            relation[\"description\"] = item['information']\n",
    "            \n",
    "        graph.create(relation)\n",
    "        \n",
    "    for item in document.relations:\n",
    "        subject = get_or_create_node(\"Entity\", \"name\", item[\"Subject\"])\n",
    "        obj = get_or_create_node(\"Entity\", \"name\", item[\"Object\"])\n",
    "        relation = Relationship(subject, item[\"Relation\"], obj)\n",
    "        if 'Description' in item:\n",
    "            relation[\"description\"] = item[\"Description\"]\n",
    "        \n",
    "        # Merge nodes and create relationships\n",
    "        #graph.merge(subject, \"name\")\n",
    "        #graph.merge(obj, \"name\")\n",
    "        graph.create(relation)\n",
    "        \n",
    "        # Link the nodes to the project node\n",
    "        #graph.create(Relationship(subject, \"Belongs To\", document_node))\n",
    "        #graph.create(Relationship(obj, \"Belongs To\", document_node))\n",
    "        \n",
    "    \n",
    "# Define a function to insert summaries \n",
    "def insert_summary_neo4j(data):\n",
    "    for item in data:\n",
    "        node = get_node(\"Entity\", \"name\", item.name)\n",
    "        node[\"Summary\"] = item.summary\n",
    "        graph.push(node)\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af30def",
   "metadata": {},
   "source": [
    "# Extraction Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7cd35416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files: 8\n",
      "\n",
      "['ALB-NES-2018-EN.txt', 'ALB-NETS-2019-EN.txt', 'ALB-CPD-2021-EN.txt', 'ALB-NREP-2021-EN.txt', 'ALB-NREAP-2016-EN.txt', 'ALB-NREAP-2015-EN.txt', 'ALB-NEP-2013-EN.txt', 'ALB-NECP-2021-EN.txt']\n"
     ]
    }
   ],
   "source": [
    "folder_path = ('Data/')\n",
    "file_list = os.listdir(folder_path)\n",
    "\n",
    "# Filter the list to include only text files (e.g., .txt files)\n",
    "text_files = [file for file in file_list if file.endswith(\".txt\")]\n",
    "\n",
    "print (f\"Number of files: {len(text_files)}\\n\")  \n",
    "print (text_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "644ec132",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(folder_path, text_files[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "32344bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text length: 21595\n"
     ]
    }
   ],
   "source": [
    "with open (file_path, 'r') as file:\n",
    "    head = [next(file) for _ in range(11)]\n",
    "    next(file)\n",
    "    raw_text = file.read()\n",
    "    \n",
    "    file.close()\n",
    "\n",
    "print (f\"Original text length: {len(raw_text)}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a1d8fa1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An exception occurred: No features in text. : \n",
      "\n",
      "Read text length: 21362\n",
      "Cleaned text length: 21362\n"
     ]
    }
   ],
   "source": [
    " # Open the file in read mode\n",
    "with open(file_path, 'r') as file:\n",
    "    \n",
    "    pattern = re.compile(r'.*?\\.{3}.*?$', re.MULTILINE)\n",
    "    # Initialize an empty string to store the lines\n",
    "    raw_text = ''\n",
    "    \n",
    "    head = [next(file) for _ in range(11)]\n",
    "    next(file)\n",
    "    \n",
    "    # Iterate over each line in the file\n",
    "    for line in file:\n",
    "        # Append the current line to the string\n",
    "        \n",
    "        if not pattern.search(line) and is_english(line):\n",
    "            raw_text += line\n",
    "            \n",
    "print(f\"Read text length: {len(raw_text)}\") \n",
    "\n",
    "text = clean_text(raw_text)      \n",
    "\n",
    "print(f\"Cleaned text length: {len(text)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e17194e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'File Name': 'ALB-CPD-2021-EN', 'Year': '2021', 'Country Name': 'Albania', 'Country Code': 'ALB', 'Category': 'CPD', 'Document Title': 'UN Country programme document for Albania (2022–2026)', 'Publication Date': '30 August–2 September 2021', 'Start Year': '2022', 'End Year': '2026', 'Language': 'EN'}\n"
     ]
    }
   ],
   "source": [
    "metadata = {}\n",
    "\n",
    "# Iterate through the data list\n",
    "for item in head:\n",
    "    # Split each element by ':' and strip the resulting strings\n",
    "    key, value = item.split(':')\n",
    "    key = key.strip()\n",
    "    value = value.strip()\n",
    "    \n",
    "    # Add the key-value pair to the dictionary\n",
    "    metadata[key] = value\n",
    "\n",
    "\n",
    "if 'Exists?' in metadata:\n",
    "    metadata.pop('Exists?')\n",
    "print(metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e918387c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "span-marker is already registered. Overwriting pipeline for task span-marker...\n",
      "/Users/admin/Library/Python/3.9/lib/python/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    }
   ],
   "source": [
    "text_sections = split_text_spacy(2000, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c68cb471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of sections from the text: 12\n"
     ]
    }
   ],
   "source": [
    "print (f\"The number of sections from the text: {len(text_sections)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "49a9cc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_length = len(text_sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fb4f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the last checkpoint\n",
    "#start_index, wiki_entity_list, bert_entity_list, gpt_entity_list, acronyms = load_checkpoint(text_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9d21a6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_index = 0\n",
    "wiki_entity_list = [''] * text_length\n",
    "bert_entity_list = [''] * text_length\n",
    "gpt_entity_list = [''] * text_length\n",
    "acronyms = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e8df97aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WIKI DONE\n",
      "BERT DONE\n",
      "GPT DONE\n",
      "NUMBER OF PROCESSED SECTIONS: 8\n",
      "WIKI DONE\n",
      "BERT DONE\n",
      "GPT DONE\n",
      "NUMBER OF PROCESSED SECTIONS: 9\n",
      "WIKI DONE\n",
      "BERT DONE\n",
      "GPT DONE\n",
      "NUMBER OF PROCESSED SECTIONS: 10\n",
      "WIKI DONE\n",
      "BERT DONE\n",
      "GPT DONE\n",
      "NUMBER OF PROCESSED SECTIONS: 11\n",
      "TIME TAKEN TO EXTRACT ENTITIES from 12 section: 12.952004671096802\n"
     ]
    }
   ],
   "source": [
    "# Continue from the last checkpoint\n",
    "\n",
    "start_time = time.time()\n",
    "for index in range(8,12):\n",
    "    try:\n",
    "        segment = text_sections[index]\n",
    "        \n",
    "        ## WIKINEURAL BILINGUAL MODEL\n",
    "        wiki_output = query_wiki({\n",
    "            \"inputs\": segment,\n",
    "        })\n",
    "        create_entities(wiki_output)\n",
    "        wiki_words = list(set(apply_threshold(wiki_output, 0.7)))\n",
    "        wiki_entity_list[index] = wiki_words\n",
    "        print (\"WIKI DONE\")\n",
    "\n",
    "        ## BERT BASE MODEL\n",
    "        bert_output = query_bert({\n",
    "            \"inputs\": segment,\n",
    "        })\n",
    "        create_entities(bert_output)\n",
    "        bert_words = list(set(apply_threshold(bert_output, 0.7)))\n",
    "        bert_entity_list[index] = bert_words\n",
    "        print (\"BERT DONE\")\n",
    "\n",
    "\n",
    "        ## GPT PROMPT\n",
    "        gpt_output = query_gpt(segment)\n",
    "        gpt_entity_list[index] = gpt_output['proper_nouns']\n",
    "\n",
    "        print (\"GPT DONE\")\n",
    "\n",
    "        ## Acronyms extraction\n",
    "        acronyms.update(gpt_output['acronyms'])\n",
    "    \n",
    "        \n",
    "        print(f\"NUMBER OF PROCESSED SECTIONS: {index}\")\n",
    "\n",
    "        # Save checkpoint at intervals\n",
    "        #if index % 5 == 0:\n",
    "            #save_checkpoint(index, wiki_entity_list, bert_entity_list, gpt_entity_list, acronyms)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing section {index}: {str(e)}\")\n",
    "        #save_checkpoint(index, wiki_entity_list, bert_entity_list, gpt_entity_list, acronyms)\n",
    "\n",
    "        continue  # Exit the loop in case of an error\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"TIME TAKEN TO EXTRACT ENTITIES from {text_length} section: {elapsed_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6df46581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'UNDP': 'United Nations Development Programme', 'UNFPA': 'United Nations Population Fund', 'UNOPS': 'United Nations Office for Project Services', 'UN': 'United Nations', 'SDCF': 'Sustainable Development Cooperation Framework', 'GDI': 'Gender Development Index', 'GII': 'Gender Inequality Index', 'EU': 'European Union', 'NSDI': 'National Spatial Data Infrastructure', 'SDG': 'Sustainable Development Goals', 'UNSDCF': 'United Nations Sustainable Development Cooperation Framework', 'SDGs': 'Sustainable Development Goals', 'CPD': 'Country Programme Document'}\n"
     ]
    }
   ],
   "source": [
    "print (acronyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5cd9ac20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Human Development Index', 'English', 'United Nations Population Fund', 'New York', 'United Nations Development Programme', 'Albania', 'European Union', 'Gender Inequality Index', 'Gender Development Index', 'United Nations Office for Project Services']\n",
      "['Human Development Index', 'CO', 'English', 'United Nations Population Fund', 'New York', 'United Nations Development Programme', 'Albania', 'European Union', 'UNDP', 'Gender Inequality Index', 'United Nations Sustainable Development Cooperation Framework', 'Gender Development Index', 'United Nations Office for Project Services']\n",
      "['Executive Board', 'United Nations Development Programme', 'United Nations Population Fund', 'United Nations Office for Project Services', 'July', 'English', 'September', 'New York', 'Albania', 'Human Development Index', 'Gender Development Index', 'Gender Inequality Index', 'Women', 'UNDP', 'UN', 'COVID-19', 'EU']\n",
      "--------\n",
      "['Albania', 'European Union', 'UNDP', 'Agenda for Sustainable Development', 'Sustainable Development Goals', 'National Strategy for Development and Integration', 'United Nations Sustainable Development Cooperation Framework', 'Sustainable Development Goal', 'United Nations']\n",
      "['Albania', '2030 Agenda for Sustainable Development', 'European Union', 'UNDP', 'UNSDCF', 'for Development and Integration', 'Sustainable Development Goals', 'National', 'Sustainable Development Goal', 'United Nations', 'Framework', 'NSDI', 'United Nations Sustainable Development Cooperation']\n",
      "['European Union', 'UNDP', 'UN', '2030 Agenda for Sustainable Development', 'Government', 'Albania']\n",
      "--------\n",
      "['Roma', 'Agenda', 'Albania', 'European Union', 'Egyptian']\n",
      "['Roma', 'Albania', 'European Union', 'UNDP', 'Sustainable Development Goals', '2030 Agenda', 'Egyptian']\n",
      "['Albania', '2030 Agenda', 'UNDP Albania', 'Roma', 'Egyptian']\n",
      "--------\n",
      "['European Union', 'UNDP', 'Sustainable Development Goals', 'United Nations', 'Government']\n",
      "['European Union', 'UNDP', 'Sustainable Development Goals', 'Sustainable Development Goal', 'United Nations', 'Government']\n",
      "['Women', 'UNDP', 'United Nations', 'Government’s', 'European Union', 'United Nations agencies']\n",
      "--------\n",
      "['Western Balkan Regional Economic Area', 'Albania', 'European Union', 'UNDP', 'Sustainable Development Goals', 'United Nations', 'European']\n",
      "['Western Balkan Regional Economic Area', 'Albania', 'European Union', 'UNDP', 'Sustainable Development Goals', 'United Nations', 'European Union Civil Protection Mechanism', 'South']\n",
      "['Albania', 'Western Balkan Regional Economic Area', 'United Nations', 'European Union', 'Civil Protection Mechanism', 'South-South', 'Sustainable Development Goals']\n",
      "--------\n",
      "['Sustainable Development', 'Albania', 'European Union', 'UNDP', 'United Nations']\n",
      "['UNDP Strategic', 'Albania', 'UNSDCF', 'UNDP', 'European Union', 'Sustainable Development Goal', 'United Nations', 'NSDI']\n",
      "['Albania', 'UNDP', 'UN', 'European Union']\n",
      "--------\n",
      "['Drini River Basin', 'European Union', 'UNDP', 'United Nations']\n",
      "['Drini River Basin', 'European Union', 'UNDP', 'United Nations COVID', 'COD', 'United Nations', '19']\n",
      "['United Nations', 'UNDP', 'European Union', 'COVID-19', '2019 earthquake', 'Government', 'Drini River Basin']\n",
      "--------\n",
      "[]\n",
      "['Roma', 'UNDP', 'Egyptian']\n",
      "['UNDP', 'forestry', 'greenhouse', 'nature-based', 'tourism', 'ecosystem', 'services', 'financial', 'sustainability', 'entities', 'qualitative', 'skills', 'development', 'digital', 'economy', 'entrepreneurship', 'multisectoral', 'solutions', 'vocational', 'education', 'training', 'decent', 'jobs', 'technology', 'sustainable', 'livelihoods', 'business', 'opportunities', 'young', 'people', 'population', 'productive', 'human', 'capital', 'labour-market', 'stakeholders', 'employment', 'consolidate', 'reforms', 'capacities', 'skills', 'mismatch', 'labour', 'force', 'participation', 'women', 'young', 'persons', 'disabilities', 'Roma', 'Egyptian', 'communities', 'long-term', 'unemployed', 'inclusive', 'growth', 'driver', 'vulnerability', 'building', 'resilience', 'policies', 'measures', 'economic', 'recovery', 'competitiveness', 'productivity', 'micro', 'small', 'medium-sized', 'enterprises', 'environments', 'digital', 'innovation', 'data-driven', 'innovations', 'higher', 'education', 'labour', 'market', 'course', 'curricula', 'future', 'skills']\n",
      "--------\n",
      "['National Public Administration Reform Strategy', 'UNDP', 'Government']\n",
      "['Administration Reform Strategy', 'National', 'UNDP', 'Public']\n",
      "['Government', 'UNDP', 'National Public Administration Reform Strategy']\n",
      "--------\n",
      "['UNDP', 'Commissioners for Information and Data Protection and Protection from Discrimination', 'People ’ s Advocate', 'Government', 'United Nations agencies']\n",
      "['Information and Data Protection and Protection from Disc', 'People ’ s Advocate', 'UNDP', 'United Nations']\n",
      "['Innovation', 'digitalization', 'UNDP', 'citizen-centred', 'municipal', 'one-stop-shop', 'People’s Advocate', 'Commissioners', 'Information', 'Data Protection', 'Protection from Discrimination', 'domestic', 'sexual', 'gender-based violence', 'gender stereotypes', 'Government', 'gender equality', 'women’s empowerment', 'gender-disaggregated data', 'programme', 'strengthened sectoral analysis', 'evidence-based policymaking', 'accountability', 'performance assessment', 'gender champions', 'civic engagement', 'dialogue platforms', 'existing donors', 'vertical funds', 'United Nations agencies', 'national finance framework', 'private sector', 'academia', 'civil society', 'national human rights institutions', 'media']\n",
      "--------\n",
      "['Albania', 'Executive Board', 'European Union', 'UNDP', 'United Nations']\n",
      "['CP', 'CO', 'Albania', 'Executive Board', 'European Union', 'UNDP', 'CPD', 'United Nations', 'Government']\n",
      "['UNDP', 'Executive Board', 'United Nations', 'Albania']\n",
      "--------\n",
      "['Executive Board', 'European Union', 'UNDP', 'Sustainable Development Goals', 'National Statistical Office']\n",
      "['Executive Board', 'European Union', 'UNDP', 'CPD', 'Sustainable Development Goals', 'National Statistical Office', 'NSDI']\n",
      "['European Union', 'Sustainable Development Goals', 'UNDP', 'midterm review', 'National Statistical Office']\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "while i < text_length:\n",
    "    #print (text_sections[i])\n",
    "    print (wiki_entity_list[i])\n",
    "    print (bert_entity_list[i])\n",
    "    print (gpt_entity_list[i])\n",
    "    print (\"--------\")\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a46f014",
   "metadata": {},
   "source": [
    "Processing the Entities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "017e28b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get raw version of entities for comparison\n",
    "raw_wiki = get_raw(wiki_entity_list)\n",
    "raw_bert = get_raw(bert_entity_list)\n",
    "raw_gpt = get_raw(gpt_entity_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "349b1d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The number of matching entities in section 0: 9\n",
      "\n",
      "['Gender Development Index', 'United Nations Population Fund', 'English', 'New York', 'Albania', 'Gender Inequality Index', 'United Nations Development Programme', 'Human Development Index', 'United Nations Office for Project Services']\n",
      "\n",
      "--------------\n",
      "\n",
      "The number of matching entities in section 1: 4\n",
      "\n",
      "['Albania', '2030 Agenda for Sustainable Development', 'UNDP', 'European Union']\n",
      "\n",
      "--------------\n",
      "\n",
      "The number of matching entities in section 2: 4\n",
      "\n",
      "['Roma', 'Albania', '2030 Agenda', 'Egyptian']\n",
      "\n",
      "--------------\n",
      "\n",
      "The number of matching entities in section 3: 3\n",
      "\n",
      "['United Nations', 'UNDP', 'European Union']\n",
      "\n",
      "--------------\n",
      "\n",
      "The number of matching entities in section 4: 5\n",
      "\n",
      "['Sustainable Development Goals', 'Western Balkan Regional Economic Area', 'Albania', 'European Union', 'United Nations']\n",
      "\n",
      "--------------\n",
      "\n",
      "The number of matching entities in section 5: 3\n",
      "\n",
      "['Albania', 'UNDP', 'European Union']\n",
      "\n",
      "--------------\n",
      "\n",
      "The number of matching entities in section 6: 4\n",
      "\n",
      "['United Nations', 'UNDP', 'European Union', 'Drini River Basin']\n",
      "\n",
      "--------------\n",
      "\n",
      "The number of matching entities in section 7: 0\n",
      "\n",
      "[]\n",
      "\n",
      "--------------\n",
      "\n",
      "The number of matching entities in section 8: 3\n",
      "\n",
      "['UNDP', 'National Public Administration Reform Strategy', 'Government']\n",
      "\n",
      "--------------\n",
      "\n",
      "The number of matching entities in section 9: 4\n",
      "\n",
      "['People’s Advocate', 'UNDP', 'Government', 'United Nations agencies']\n",
      "\n",
      "--------------\n",
      "\n",
      "The number of matching entities in section 10: 4\n",
      "\n",
      "['Albania', 'United Nations', 'UNDP', 'Executive Board']\n",
      "\n",
      "--------------\n",
      "\n",
      "The number of matching entities in section 11: 4\n",
      "\n",
      "['UNDP', 'Sustainable Development Goals', 'National Statistical Office', 'European Union']\n",
      "\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "entity_objects = []\n",
    "merged = []\n",
    "i = 0\n",
    "\n",
    "while i < len(wiki_entity_list):\n",
    "    merged = merge_extracted_entities_old(raw_wiki[i], raw_bert[i], raw_gpt[i])\n",
    "    print (f\"\\nThe number of matching entities in section {i}: {len(merged)}\\n\")\n",
    "    print (merged)\n",
    "    \n",
    "    print (\"\\n--------------\")\n",
    "\n",
    "    entity_objects.append(merged)\n",
    "    \n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a3dbc949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(entity_objects)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03230ac",
   "metadata": {},
   "source": [
    "Chain of Thought - Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7c43b8d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'United Nations Development Programme': 'UNDP', 'United Nations Population Fund': 'UNFPA', 'United Nations Office for Project Services': 'UNOPS', 'United Nations': 'UN', 'Sustainable Development Cooperation Framework': 'SDCF', 'Gender Development Index': 'GDI', 'Gender Inequality Index': 'GII', 'European Union': 'EU', 'National Spatial Data Infrastructure': 'NSDI', 'Sustainable Development Goals': 'SDGs', 'United Nations Sustainable Development Cooperation Framework': 'UNSDCF', 'Country Programme Document': 'CPD'}\n"
     ]
    }
   ],
   "source": [
    "# invert acronyms dict to ease look up\n",
    "acronyms_dict = {v: k for k, v in acronyms.items()}\n",
    "print (acronyms_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "86ab435a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(entity_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d730afd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_list = []\n",
    "relations_list = []\n",
    "entities_with_sections = []\n",
    "relations_with_section = []\n",
    "seen_entities = set()\n",
    "seen_acronyms = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61fea8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CATEGORIZED ENTITIES of Section: 0\n",
      "\n",
      "[{'entity': 'Gender Development Index', 'category': 'Strategy', 'acronym': 'GDI'}, {'entity': 'United Nations Population Fund', 'category': 'Organization', 'acronym': 'UNFPA'}, {'entity': 'English', 'category': 'Skill'}, {'entity': 'New York', 'category': 'Location'}, {'entity': 'Albania', 'category': 'Location'}, {'entity': 'Gender Inequality Index', 'category': 'Strategy', 'acronym': 'GII'}, {'entity': 'United Nations Development Programme', 'category': 'Organization', 'acronym': 'UNDP'}, {'entity': 'Human Development Index', 'category': 'Strategy'}, {'entity': 'United Nations Office for Project Services', 'category': 'Organization', 'acronym': 'UNOPS'}]\n",
      "\n",
      " EXTRACTED RELATIONS: \n",
      "\n",
      "[{'Subject': 'Gender Development Index', 'Relation': 'focuses on', 'Object': 'Albania', 'Description': 'Albania has a Gender Development Index of 0.967.', 'Relevance': 8}, {'Subject': 'United Nations Population Fund', 'Relation': 'partners with', 'Object': 'United Nations Development Programme', 'Description': 'The United Nations Population Fund partners with the United Nations Development Programme.', 'Relevance': 9}, {'Subject': 'English', 'Relation': 'in', 'Object': 'New York', 'Description': 'The document is in English and the event is in New York.', 'Relevance': 7}, {'Subject': 'Albania', 'Relation': 'an instance of', 'Object': 'Country programmes', 'Description': 'The country programme document for Albania (2022–2026) is an instance of country programmes.', 'Relevance': 6}, {'Subject': 'Gender Inequality Index', 'Relation': 'focuses on', 'Object': 'Albania', 'Description': 'Albania has a Gender Inequality Index of 0.181.', 'Relevance': 8}, {'Subject': 'United Nations Development Programme', 'Relation': 'focuses on', 'Object': 'United Nations Sustainable Development Cooperation Framework', 'Description': 'The United Nations Development Programme focuses on the United Nations Sustainable Development Cooperation Framework.', 'Relevance': 9}, {'Subject': 'Human Development Index', 'Relation': 'focuses on', 'Object': 'Albania', 'Description': 'Albania has a Human Development Index of 0.791.', 'Relevance': 8}, {'Subject': 'United Nations Office for Project Services', 'Relation': 'partners with', 'Object': 'United Nations Development Programme', 'Description': 'The United Nations Office for Project Services partners with the United Nations Development Programme.', 'Relevance': 9}]\n",
      "\n",
      "-------------------\n",
      "CATEGORIZED ENTITIES of Section: 1\n",
      "\n",
      "[{'entity': 'Albania', 'category': 'Location'}, {'entity': '2030 Agenda for Sustainable Development', 'category': 'Strategy'}, {'entity': 'UNDP', 'category': 'Organization'}, {'entity': 'European Union', 'category': 'Organization', 'acronym': 'EU'}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for index, uncategorized_entities in enumerate(entity_objects):\n",
    "    try:\n",
    "        entities_subset = categorize_entities(text_sections[index], uncategorized_entities, categories)\n",
    "        #print(seen_acronyms)\n",
    "\n",
    "        # Add 'acronym' key to entity list\n",
    "        for item in entities_subset:\n",
    "            if item[\"entity\"] not in seen_entities and item[\"entity\"] not in seen_acronyms:\n",
    "                seen_entities.add(item[\"entity\"])\n",
    "                \n",
    "                \n",
    "                if item[\"entity\"] in acronyms_dict.keys():\n",
    "                    item[\"acronym\"] = acronyms_dict[item[\"entity\"]]\n",
    "                    seen_acronyms.add(item['acronym'])\n",
    "                    #print(\"SEEN ACRONYM\" + str(item))\n",
    "                    \n",
    "                elif item['entity'] in acronyms.keys():\n",
    "                    item[\"acronym\"] = item['entity']\n",
    "                    item[\"entity\"] = acronyms[item[\"entity\"]]\n",
    "                    seen_acronyms.add(item['acronym'])\n",
    "                    #print(\"SEEN ACRONYM\" + str(item))\n",
    "                    \n",
    "                entities_list.append(item)\n",
    "\n",
    "        print (\"CATEGORIZED ENTITIES of Section: \" + str(index) + \"\\n\")\n",
    "        print (entities_subset)\n",
    "        #store the categorized entities in order of lists for later processing\n",
    "        entities_with_sections.append(entities_subset)\n",
    "\n",
    "        relations_subset = extract_ontology_relations(text_sections[index], entity_objects[index], ontology)\n",
    "\n",
    "        print (\"\\n EXTRACTED RELATIONS: \\n\")\n",
    "        print (relations_subset)\n",
    "\n",
    "        relations_list.extend(relations_subset)\n",
    "        relations_with_section.append(relations_subset)\n",
    "\n",
    "        print (\"\\n-------------------\")\n",
    "\n",
    "    except Exception as e:\n",
    "            print(f\"Error processing section {index}: {str(e)}\")\n",
    "            #save_checkpoint(index, wiki_entity_list, bert_entity_list, gpt_entity_list, acronyms)\n",
    "\n",
    "    continue  # Exit the loop in case of an error\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"TIME TAKEN TO EXTRACT RELATIONS FROM {text_length} SECTIONS: {elapsed_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a973dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(entities_list))\n",
    "print (len(relations_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3de5f98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "entity_summaries = extract_summaries(entities_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cff8b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(entity_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e264dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a list of names, to check for valid relations\n",
    "\n",
    "entity_names = set([item['entity'] for item in entities_list])\n",
    "entity_names.update(acronyms.keys())\n",
    "entity_names.update(acronyms.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e409383f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (len(acronyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5915e211",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (len(entity_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af015795",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_dic = {}\n",
    "\n",
    "for i in entity_summaries:\n",
    "    entity_dic[i['entity']] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d23ea00",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(entity_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fc7d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(relations_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6b18fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(relations_list_ontology)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5296a72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_relations = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0afd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_relations_ontology = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1beff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in relations_list:\n",
    "    if 'Subject' in i and i['Subject'] in entity_names and i['Object'] in entity_names:\n",
    "        final_relations.append(i)\n",
    "    elif 'Description' in i and 'Subject' in i and i['Subject'] in entity_dic.keys():\n",
    "        entity_dic[i['Subject']].update({'information':i['Description']})\n",
    "    elif 'Description' in i and 'Object' in i and i['Object'] in entity_dic.keys():\n",
    "        entity_dic[i['Object']].update({'information':i['Description']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76deb51e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "final_entities = []\n",
    "for i in entity_dic.values():\n",
    "    final_entities.append(i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ce9275",
   "metadata": {},
   "source": [
    "# Write the output to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b071ef18",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_relations = json.dumps(final_relations, indent=2)\n",
    "json_entities = json.dumps(final_entities, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24872f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d99a090",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Entities/' + metadata['File Name']+ '.json', \"w\") as output_file:\n",
    "    output_file.write(json_entities)\n",
    "    output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c596dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Relations/' + metadata['File Name']+ '.json', \"w\") as output_file:\n",
    "    output_file.write(json_relations)\n",
    "    output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6e45ff",
   "metadata": {},
   "source": [
    "Manual review of the entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bcdc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Entities/' + metadata['File Name']+ '.json', \"r\") as file:\n",
    "    data = file.read()\n",
    "    final_entities = json.loads(data)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec38e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(final_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eaa91c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(final_entities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079f0f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "document = Document(metadata, final_entities, final_relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c007af4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_relations_neo4j(document)\n",
    "#insert_summary_neo4j(summary_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06754a38",
   "metadata": {},
   "source": [
    "# Add Relations to Spreadsheet for Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62e2121",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "\n",
    "# Use the credentials from the service account key JSON file you downloaded\n",
    "scope = [\"https://spreadsheets.google.com/feeds\", \"https://www.googleapis.com/auth/drive\"]\n",
    "creds = ServiceAccountCredentials.from_json_keyfile_name('energy-moonshot-ai-97aa9045e45f.json', scope)\n",
    "client = gspread.authorize(creds)\n",
    "\n",
    "# Open the Google Sheet by its title or URL\n",
    "spreadsheet = client.open_by_url('https://docs.google.com/spreadsheets/d/1yZ-XQQs52kaI5k9MjvV_CdbgWQi-GazjHHGqQUF8gko/edit')\n",
    "\n",
    "\n",
    "# Enter relations in the first sheet\n",
    "sheet = spreadsheet.get_worksheet(0)\n",
    "\n",
    "# Start row index from 5\n",
    "start_row_index = 5\n",
    "index = 1\n",
    "\n",
    "# Check if there's valid data to insert\n",
    "if final_relations:\n",
    "    # Create a list of lists where each inner list represents the values of a row\n",
    "    batch_relations = []\n",
    "    for index, row_data in enumerate(final_relations):\n",
    "        row = [index, row_data['Subject'], row_data['Relation'], row_data.get('Object', ''), \n",
    "               row_data.get('Description', ''), row_data.get('Relevance', '')]\n",
    "        \n",
    "        batch_relations.append(row)\n",
    "        index = index + 1\n",
    "\n",
    "    # Insert the data into the Google Sheet starting from row 5\n",
    "    sheet.insert_rows(batch_relations, start_row_index)\n",
    "\n",
    "    print(f\"{len(final_relations)} entries added to Google Sheet.\")\n",
    "else:\n",
    "    print(\"No data to insert.\")\n",
    "    \n",
    "    \n",
    "# Enter entities in the second sheet\n",
    "sheet = spreadsheet.get_worksheet(1)\n",
    "\n",
    "\n",
    "# Start row index from 5\n",
    "start_row_index = 5\n",
    "index = 1\n",
    "\n",
    "if final_entities:\n",
    "    batch_entities = []\n",
    "    for index, row_data in enumerate(final_entities):\n",
    "        row = [index, row_data['entity'], row_data['category'], row_data.get('acronym', ''), row_data.get('summary', '')]\n",
    "        batch_entities.append(row)\n",
    "        \n",
    "        index = index + 1\n",
    "    sheet.insert_rows(batch_entities, start_row_index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
