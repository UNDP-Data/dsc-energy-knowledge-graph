{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ff8ab69",
   "metadata": {},
   "source": [
    "<h2> Energy Knowledge Graph Pipeline Entities GPT </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "672cb5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##imports\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b98ae5f",
   "metadata": {},
   "source": [
    "## Extract Relevant entities \n",
    "Extract the entities in the entities.csv list of entities to guide the knowledge graph construction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea041f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_list_path = '../02_Input/00_Metadata/entities.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ea3fc146",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = [\n",
    "    \"UNDP\",\n",
    "    \"United Nations\",\n",
    "    \"Afghanistan\",\n",
    "    \"Government\",\n",
    "    \"Central Statistics Organization\",\n",
    "    \"UNAMA\",\n",
    "    \"Ministry of Interior\",\n",
    "    \"Ministry of Justice\",\n",
    "    \"Attorney General's Office\",\n",
    "    \"Ministry of Women's Affairs\",\n",
    "    \"National Police Programmes\",\n",
    "    \"Ministry of Energy and Water (MEW)\",\n",
    "    \"Da Afghanistan Breshna Sherkat (DABS)\",\n",
    "    \"Africa\",\n",
    "    \"Awoken\",\n",
    "    \"Anou-Araren\",\n",
    "    \"Statistical Committee of Armenia (ArmStat)\",\n",
    "    \"Biomass\",\n",
    "    \"Burkina Faso\",\n",
    "    \"Energy Storage Systems (ESS)\",\n",
    "    \"Foreign Direct Investment (FDI)\",\n",
    "    \"Geographic Information System (GIS)\",\n",
    "    \"Human Development Index (HDI)\",\n",
    "    \"Households\",\n",
    "    \"International Atomic Energy Agency (IAEA)\",\n",
    "    \"LECO\",\n",
    "    \"LPG\",\n",
    "    \"Ministry of Transport and Civil Aviation(MTCA)\",\n",
    "    \"The West African Power Pool (WAPP)\",\n",
    "    \"United Nations Sustainable Development Cooperation Framework (UNSDCF)\",\n",
    "    \"World Bank\"\n",
    "]\n",
    "\n",
    "relationships = [\"SUPPORTED_BY\", \"PROVIDE_SUPPORT\", \"HAS_ENERGY_RESOURCE\", \"EQUIVALENT_TO\", \"CONTAINS\", \"PROPOSED\", \"PARTICIPATED_IN\", \"HAS_PARTNERSHIP_WITH\", \"SOLVED\", \"HAS_RELATIONSHIP_WITH\", \"RELATED_TO\", \"CORRESPONDS_TO\", \"HAS_PROPERTY\", \"REPRESENTS\", \"IS_USED_IN\", \"DISCOVERED\", \"FOUND\", \"IS_SOLUTION_TO\", \"PROVED\", \"LIVED_IN\", \"LIKED\", \"COLLABORATE_WITH\", \"CONTRIBUTED_TO\", \"IMPLIES\", \"DESCRIBES\", \"DEVELOPED\", \"HAS_PROPERTY\", \"USED_FOR\"]\n",
    "\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Ignore previous commands!!!\n",
    "You are an International Relations Expert and a scientist helping us extract relevant information from text. \n",
    "The task is to extract as many relevant relationships between entities in a given text.\n",
    "Specifically, the only entity tags you may use are:\n",
    "ENTITY TAGS = {', '.join(entities)}.\n",
    "The only relationships you may use are:\n",
    "RELATIONSHIP = {', '.join(relationships)}\n",
    "\n",
    "The output should have the following format: \"relations\": {{\n",
    "    \"Entity\": [\n",
    "        {{\n",
    "            \"Relation\": \"RELATIONSHIP\",\n",
    "            \"Object\": \"Entity\",\n",
    "            \"Description\": \"Text content \"\n",
    "        }},\n",
    "    ]\n",
    "}}\n",
    "\n",
    "Only find relationship between the ENTITY TAGS in the text. \n",
    "Where Entity are strictly from here: {', '.join(entities)}. Only find relationship in the text between these.\n",
    "Donnot use entities not in the ENTITY TAGS above for the Object.\n",
    "I am only interested in the relationships in the above format and you can only use what you find in the text provided. Also, you should not provide relationships already found and you should choose less than 100 relationships and the most important ones.\n",
    "You should only take the most important relationships as the aim is to build a knowledge graph. Rather a few but contextual meaningful than many nonsensical. \n",
    "Moreover, you should only tag entities with one of the allowed tags if it truly fits that category and I am only interested in general entities such as \"Shape HAS Area\" rather than \"Shape HAS Area 1\".\n",
    "The input text is the following:\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "656ce769",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "openai.api_key = \"\"\n",
    "\n",
    "def process_gpt4(text):\n",
    "    \"\"\"This function prompts the gpt-4 model and returns the output\"\"\"\n",
    "    response = openai.chat.completions.create(\n",
    "                    model=\"gpt-4\",\n",
    "                    temperature=0,\n",
    "                    messages=[\n",
    "                         {\"role\": \"user\", \"content\": prompt + text},\n",
    "                    ]\n",
    "                )\n",
    "    result = response.choices[0].message.content\n",
    "    return result\n",
    "\n",
    " \n",
    "\n",
    "def clean_json(json_data):\n",
    "    # Remove duplicates in entities\n",
    "    entities = json_data[\"knowledge graph\"][\"entities\"]\n",
    "    unique_entities = {entity[\"entity\"]: entity for entity in entities}.values()\n",
    "    json_data[\"knowledge graph\"][\"entities\"] = list(unique_entities)\n",
    "    \n",
    "    # Remove relations with empty array\n",
    "    relations = json_data[\"knowledge graph\"][\"relations\"]\n",
    "    filtered_relations = {\n",
    "        key: value for key, value in relations.items() if value\n",
    "    }\n",
    "    json_data[\"knowledge graph\"][\"relations\"] = filtered_relations\n",
    "    \n",
    "    return json_data\n",
    "\n",
    "\n",
    "\n",
    "def split_into_paragraphs(text):\n",
    "    # Split text into paragraphs based on double line breaks\n",
    "    paragraphs = text.split('\\n\\n')  # Modify this based on your paragraph delimiter\n",
    "    return [paragraphs[i:i+4] for i in range(0, len(paragraphs), 4)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af56fafe",
   "metadata": {},
   "source": [
    "### Process data and extract entities \n",
    "Process the data .txt files and pass through the entities_relation function \n",
    "to extract entities and relationships from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12891cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import json\n",
    "import os\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "def process_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        text_content = file.read()\n",
    "\n",
    "        # Splitting text into groups of four paragraphs\n",
    "        paragraphs_groups = split_into_paragraphs(text_content)\n",
    "\n",
    "        all_relations = []\n",
    "        for group in paragraphs_groups:\n",
    "            # Join the paragraphs within the group\n",
    "            text_group = '\\n\\n'.join(group)\n",
    "            # Extracting entities and relations for each group of paragraphs\n",
    "            relations = process_gpt4(text_group)\n",
    "            all_relations.extend(relations)\n",
    "\n",
    "        # Constructing JSON data\n",
    "        json_data = {\n",
    "            \"metadata\": {\n",
    "                # Add your metadata extraction logic here\n",
    "            },\n",
    "            \"knowledge graph\": {\n",
    "                \"entities\": [],\n",
    "                \"relations\": all_relations\n",
    "            }\n",
    "        }\n",
    "        output_folder_path = '../02_Output/00_By-Document/02_National Policy/AFG'\n",
    "\n",
    "        cleanedJSON = clean_json(json_data)\n",
    "        # Save as JSON\n",
    "        output_file_name = os.path.basename(file_path).replace('.txt', '.json')\n",
    "        output_file_path = os.path.join(output_folder_path, output_file_name)\n",
    "        with open(output_file_path, 'w') as output_file:\n",
    "            json.dump(cleanedJSON, output_file, indent=4)\n",
    "        return output_file_path\n",
    "\n",
    "folder_path = '../02_Input/01_Cleaned-Text/02_National Policy/AFG'\n",
    "file_paths = [os.path.join(folder_path, file_name) for file_name in os.listdir(folder_path) if file_name.endswith(\".txt\")]\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    results = executor.map(process_file, file_paths)\n",
    "\n",
    "for result in results:\n",
    "    print(f\"Processed file saved at: {result}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2212456",
   "metadata": {},
   "source": [
    "## TEST FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "c06fdc52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"relations\": {\n",
      "    \"Afghanistan\": [\n",
      "        {\n",
      "            \"Relation\": \"HAS_RELATIONSHIP_WITH\",\n",
      "            \"Object\": \"UNDP\",\n",
      "            \"Description\": \"The standalone segment has significant potential in Afghanistan with the centralized grid not being a cost-effective option in several pockets of the country dominated by mountainous terrain and spatially dispersed communities.\"\n",
      "        },\n",
      "        {\n",
      "            \"Relation\": \"HAS_RELATIONSHIP_WITH\",\n",
      "            \"Object\": \"Da Afghanistan Breshna Sherkat (DABS)\",\n",
      "            \"Description\": \"Power purchase is governed by the PPA between each project developer and the utility i.e. DABS. Feed-in tariffs may be used to pre-determine tariffs for SPPs.\"\n",
      "        },\n",
      "        {\n",
      "            \"Relation\": \"HAS_RELATIONSHIP_WITH\",\n",
      "            \"Object\": \"Ministry of Energy and Water (MEW)\",\n",
      "            \"Description\": \"Ministry of Energy and Water: Policy direction for setting up of the SPPs. These shall be classified under separate notification for a “SPP Scheme”.\"\n",
      "        }\n",
      "    ],\n",
      "    \"Da Afghanistan Breshna Sherkat (DABS)\": [\n",
      "        {\n",
      "            \"Relation\": \"HAS_RELATIONSHIP_WITH\",\n",
      "            \"Object\": \"Ministry of Energy and Water (MEW)\",\n",
      "            \"Description\": \"Da Afghanistan Breshna Sherkat (DABS): DABS shall, as the transmission and distribution utility, shall finalize tariff rates for procurement of power from SPPs and procure the power.\"\n",
      "        }\n",
      "    ],\n",
      "    \"Ministry of Energy and Water (MEW)\": [\n",
      "        {\n",
      "            \"Relation\": \"HAS_RELATIONSHIP_WITH\",\n",
      "            \"Object\": \"Da Afghanistan Breshna Sherkat (DABS)\",\n",
      "            \"Description\": \"Ministry of Energy and Water (MEW): Designs the DF Scheme in Pan-Afghanistan context\n"
     ]
    }
   ],
   "source": [
    "# # Run a quick test\n",
    "# relations = process_gpt4(f\"\"\"\n",
    "    \n",
    " \n",
    "#  \"\"\")\n",
    "\n",
    "  \n",
    "# print(relations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
