{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# This notebook extracts dataframes of entities from the documents"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import csv\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "ent_path = '../03_Output/00_GPT KGs/Entities.csv'\n",
    "entities = pd.read_csv(ent_path, delimiter=';')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "rel_path = '../03_Output/00_GPT KGs/Relations.csv'\n",
    "relations = pd.read_csv(rel_path, delimiter=';')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       entity  doc_id  character_index  \\\n",
      "0  Sustainable Energy Academy       1                4   \n",
      "1                      energy       1               57   \n",
      "2                     academy       2               30   \n",
      "3                      energy       2               53   \n",
      "\n",
      "                                       large_extract  \n",
      "0  The Sustainable Energy Academy offers various ...  \n",
      "1  tainable Energy Academy offers various modules...  \n",
      "2  One of the key focuses of the academy is susta...  \n",
      "3   of the key focuses of the academy is sustaina...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def extract_entities_from_documents(documents):\n",
    "    \"\"\"\n",
    "    Extracts entities from documents and creates a DataFrame with the following columns:\n",
    "    entity, doc_id, character_index, large_extract.\n",
    "\n",
    "    Parameters:\n",
    "    documents (list of tuples): List of tuples where each tuple contains (doc_id, document_text)\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with the extracted entity information.\n",
    "    \"\"\"\n",
    "    # Define a simple pattern for entity extraction (for demonstration purposes)\n",
    "    # This should be replaced with a more sophisticated entity recognition approach\n",
    "    entity_pattern = re.compile(r'\\b(Sustainable Energy Academy|energy|academy|module)\\b', re.IGNORECASE)\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for doc_id, text in documents:\n",
    "        for match in entity_pattern.finditer(text):\n",
    "            entity = match.group()\n",
    "            char_index = match.start()\n",
    "            extract_start = max(char_index - 50, 0)\n",
    "            extract_end = min(char_index + 50, len(text))\n",
    "            large_extract = text[extract_start:extract_end]\n",
    "\n",
    "            data.append({\n",
    "                'entity': entity,\n",
    "                'doc_id': doc_id,\n",
    "                'character_index': char_index,\n",
    "                'large_extract': large_extract\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "documents = [\n",
    "    ('SEH1', \"The Sustainable Energy Academy offers various modules on energy efficiency.\"),\n",
    "    ('2', \"One of the key focuses of the academy is sustainable energy solutions.\"),\n",
    "]\n",
    "\n",
    "df = extract_entities_from_documents(documents)\n",
    "print(df)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "\n",
    "# Define a simple pattern for entity extraction (this can be replaced with a more sophisticated entity recognition approach)\n",
    "entity_pattern = re.compile(r'\\b(Sustainable Energy Academy|energy|academy|module)\\b', re.IGNORECASE)\n",
    "\n",
    "def extract_entities_from_documents(documents):\n",
    "    data = []\n",
    "\n",
    "    for doc_id, text in documents:\n",
    "        for match in entity_pattern.finditer(text):\n",
    "            entity = match.group()\n",
    "            char_index = match.start()\n",
    "            extract_start = max(char_index - 50, 0)\n",
    "            extract_end = min(char_index + 50, len(text))\n",
    "            large_extract = text[extract_start:extract_end]\n",
    "\n",
    "            data.append({\n",
    "                'entity': entity,\n",
    "                'doc_id': doc_id,\n",
    "                'character_index': char_index,\n",
    "                'large_extract': large_extract\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "def get_sub_extracts(df):\n",
    "    sub_extracts = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        entity = row['entity']\n",
    "        large_extract = row['large_extract']\n",
    "\n",
    "        # Assuming entity definition/metadata provides a pattern to find relevant sub-extracts\n",
    "        # For demonstration, we'll just take a 20-character context around the entity mention\n",
    "        entity_pattern = re.compile(re.escape(entity), re.IGNORECASE)\n",
    "        match = entity_pattern.search(large_extract)\n",
    "\n",
    "        if match:\n",
    "            start = max(match.start() - 10, 0)\n",
    "            end = min(match.end() + 10, len(large_extract))\n",
    "            sub_extract = large_extract[start:end]\n",
    "            sub_extracts.append({\n",
    "                'entity': entity,\n",
    "                'doc_id': row['doc_id'],\n",
    "                'sub_extract': sub_extract\n",
    "            })\n",
    "\n",
    "    sub_extracts_df = pd.DataFrame(sub_extracts)\n",
    "    return sub_extracts_df\n",
    "\n",
    "def synthesize_long_description(sub_extracts_df):\n",
    "    long_descriptions = sub_extracts_df.groupby('entity')['sub_extract'].apply(lambda x: ' '.join(x)).reset_index()\n",
    "    long_descriptions.columns = ['entity', 'long_description']\n",
    "    return long_descriptions\n",
    "\n",
    "def extract_relations(sub_extracts_df):\n",
    "    # Placeholder function to extract new relations\n",
    "    # This would need to be implemented based on the specific criteria for extracting relations\n",
    "    relations = []\n",
    "\n",
    "    for _, row in sub_extracts_df.iterrows():\n",
    "        # Dummy relation extraction for demonstration\n",
    "        relations.append({\n",
    "            'entity': row['entity'],\n",
    "            'related_entity': 'Dummy Related Entity',\n",
    "            'relation': 'related_to'\n",
    "        })\n",
    "\n",
    "    relations_df = pd.DataFrame(relations)\n",
    "    return relations_df\n",
    "\n",
    "def add_metadata_to_entities(df, long_descriptions, thumbnail_links):\n",
    "    entities_metadata = []\n",
    "\n",
    "    for entity, group in df.groupby('entity'):\n",
    "        document_ids = group['doc_id'].unique().tolist()\n",
    "        thumbnail_link = thumbnail_links.get(entity, '')\n",
    "\n",
    "        entity_metadata = {\n",
    "            'entity': entity,\n",
    "            'document_ids': document_ids,\n",
    "            'thumbnail_link': thumbnail_link\n",
    "        }\n",
    "\n",
    "        description_row = long_descriptions[long_descriptions['entity'] == entity]\n",
    "        if not description_row.empty:\n",
    "            entity_metadata['long_description'] = description_row['long_description'].values[0]\n",
    "\n",
    "        entities_metadata.append(entity_metadata)\n",
    "\n",
    "    return entities_metadata\n",
    "\n",
    "def export_entities_metadata(entities_metadata, csv_path, json_path):\n",
    "    df = pd.DataFrame(entities_metadata)\n",
    "    df.to_csv(csv_path, index=False)\n",
    "\n",
    "    with open(json_path, 'w') as json_file:\n",
    "        json.dump(entities_metadata, json_file, indent=4)\n",
    "\n",
    "# Example usage\n",
    "documents = [\n",
    "    (1, \"The Sustainable Energy Academy offers various modules on energy efficiency.\"),\n",
    "    (2, \"One of the key focuses of the academy is sustainable energy solutions.\"),\n",
    "]\n",
    "\n",
    "# Step 1: Extract entities from documents\n",
    "entities_df = extract_entities_from_documents(documents)\n",
    "\n",
    "# Step 2: Get sub-extracts\n",
    "sub_extracts_df = get_sub_extracts(entities_df)\n",
    "\n",
    "# Step 3: Synthesize long descriptions\n",
    "long_descriptions = synthesize_long_description(sub_extracts_df)\n",
    "\n",
    "# Step 4: Extract relations\n",
    "relations_df = extract_relations(sub_extracts_df)\n",
    "\n",
    "# Step 5: Add metadata and export\n",
    "thumbnail_links = {\n",
    "    'Sustainable Energy Academy': 'http://example.com/thumbnail1.jpg',\n",
    "    'energy': 'http://example.com/thumbnail2.jpg',\n",
    "    'academy': 'http://example.com/thumbnail3.jpg',\n",
    "    'module': 'http://example.com/thumbnail4.jpg'\n",
    "}\n",
    "\n",
    "entities_metadata = add_metadata_to_entities(entities_df, long_descriptions, thumbnail_links)\n",
    "export_entities_metadata(entities_metadata, 'entities_metadata.csv', 'entities_metadata.json')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def update_csv_and_create_json(csv_file_path, updated_csv_file_path, json_file_path, synonyms_dict):\n",
    "    updated_entities = []\n",
    "\n",
    "    with open(csv_file_path, mode='r', encoding='utf-8') as csvfile, \\\n",
    "         open(updated_csv_file_path, mode='w', encoding='utf-8', newline='') as updated_csvfile:\n",
    "\n",
    "        reader = csv.DictReader(csvfile, delimiter=',')\n",
    "        fieldnames = reader.fieldnames\n",
    "        writer = csv.DictWriter(updated_csvfile, fieldnames=fieldnames, delimiter=';')\n",
    "        writer.writeheader()\n",
    "\n",
    "        for row in reader:\n",
    "            entity = row['Object'].lower()\n",
    "            if entity in synonyms_dict:\n",
    "                row['Object'] = synonyms_dict[entity]\n",
    "            else:\n",
    "                row['Object'] = entity\n",
    "\n",
    "            writer.writerow(row)\n",
    "            updated_entities.append(row)\n",
    "\n",
    "    with open(json_file_path, mode='w', encoding='utf-8') as jsonfile:\n",
    "        json.dump(updated_entities, jsonfile, indent=4)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV updated and JSON file created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "\n",
    "updated_csv_file_path = '../03_Output/00_GPT KGs/Relations_replaced.csv'\n",
    "json_file_path = '../03_Output/00_GPT KGs/Relations_replaced.json'\n",
    "\n",
    "update_csv_and_create_json(rel_path, updated_csv_file_path, json_file_path, synonyms_dict)\n",
    "\n",
    "print(\"CSV updated and JSON file created successfully.\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
