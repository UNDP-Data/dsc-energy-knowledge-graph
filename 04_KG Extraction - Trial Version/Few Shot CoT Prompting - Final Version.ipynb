{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24b0a798",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-11 17:02:54.987102: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "from config import OPENAI_API_KEY\n",
    "from config import NEO4JPASS\n",
    "import numpy as np\n",
    "import torch, torchvision\n",
    "from transformers import AutoTokenizer, TFAutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "from span_marker import SpanMarkerModel\n",
    "from langdetect import detect\n",
    "from langdetect import LangDetectException\n",
    "import re\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfc59a5",
   "metadata": {},
   "source": [
    "Initialize Entity Categories and Relation Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f5423cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\n",
    "    \n",
    "    \"Person\",\n",
    "    \"Location\",\n",
    "    \"Organization\",\n",
    "    \"Event\",\n",
    "    \"Product\",\n",
    "    \"Project\",\n",
    "    \"Skill\",\n",
    "    \"Strategy\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b0aa441",
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_labels = [\n",
    "    \"implements\",\n",
    "    \"funds\",\n",
    "    \"focuses_on\",\n",
    "    \"in\",\n",
    "    \"partners_with\",\n",
    "    \"contributes_to\",\n",
    "    \"monitors\",\n",
    "    \"targets\",\n",
    "    \"addresses\",\n",
    "    \"employs\",\n",
    "    \"collaborates_with\",\n",
    "    \"supports\",\n",
    "    \"administers\",\n",
    "    \"measures\",\n",
    "    \"aligns_with\",\n",
    "    \"an_instance_of\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e5f1a1",
   "metadata": {},
   "source": [
    "# Setting up OpenAI connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "76acd36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_type = \"azure\"\n",
    "openai.api_base = \"https://undp-ngd-openai-datafutures-dev-2.openai.azure.com/\"\n",
    "openai.api_version = \"2023-07-01-preview\"\n",
    "openai.api_key = \"2c10778db282466e8bd61e5791b1a41b\"\n",
    "\n",
    "def get_answer(user_question, timeout_seconds):\n",
    "    messages = [\n",
    "        {'role': 'user', 'content': user_question},\n",
    "    ]\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            engine=\"sdgi-gpt-35-turbo-16k\", \n",
    "            messages=messages,\n",
    "            temperature=0.2,\n",
    "            request_timeout = timeout_seconds\n",
    "            # max_tokens=2000\n",
    "        )\n",
    "        return response.choices[0].message[\"content\"]\n",
    "    except requests.Timeout:\n",
    "        print(f\"Request timed out\")\n",
    "        return []\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e328a35",
   "metadata": {},
   "source": [
    "# Entity Extraction using Transformers \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f369316f",
   "metadata": {},
   "outputs": [],
   "source": [
    "WIKI_API = \"https://api-inference.huggingface.co/models/Babelscape/wikineural-multilingual-ner\"\n",
    "BERT_API = \"https://api-inference.huggingface.co/models/dslim/bert-base-NER\"\n",
    "\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "def query_wiki(payload):\n",
    "\tresponse = requests.post(WIKI_API, headers=headers, json=payload)\n",
    "\treturn response.json()\n",
    "\t\n",
    "\n",
    "def query_bert(payload):\n",
    "\tresponse = requests.post(BERT_API, headers=headers, json=payload)\n",
    "\treturn response.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb268621",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_gpt(text):\n",
    "    \n",
    "    entities_prompt = f\"\"\"\n",
    "\n",
    "    You will be given a >>>>>TEXT<<<<<. You have two tasks:\n",
    "    \n",
    "    1. Your first task is to detect acronyms with their names and store them in python dictionary.\n",
    "    2. Your second task is to detect Proper Nouns in the text and store them in python list.\n",
    "    \n",
    "    Return a JSON array contaning dictionary and the list.\n",
    "\n",
    "    >>>>>TEXT<<<<<\n",
    "    {text}\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    #start_time = time.time()\n",
    "\n",
    "    result = get_answer(entities_prompt, 10)\n",
    "    result = json.loads(result)\n",
    "    \n",
    "    #end_time = time.time()\n",
    "    #elapsed_time = end_time - start_time\n",
    "    #print (f\"TIME TAKEN TO EXECUTE PROMPT: {elapsed_time}\")\n",
    "    return result\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4160364",
   "metadata": {},
   "source": [
    "# Text Pre-Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a13f4641",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_section(limit, text):\n",
    "    sections_list = []\n",
    "    length = len(text)\n",
    "    i = 0\n",
    "\n",
    "    while i < length - 1:\n",
    "        j = i + limit\n",
    "\n",
    "        if j >= length:\n",
    "            j = length - 1\n",
    "        elif text[j] not in ('.', '\\n', ';'):\n",
    "            while text[j] not in ('.', '\\n', ';'):\n",
    "                j -= 1\n",
    "            j += 1\n",
    "\n",
    "        section = text[i:j]\n",
    "\n",
    "        if is_valid_section(section):\n",
    "            sections_list.append(section)\n",
    "        else: \n",
    "            print(\"INVALID SECTION DETECTED\")\n",
    "            print(section)\n",
    "            section_list[-1].extend(section)\n",
    "        i = j\n",
    "    \n",
    "    \n",
    "    return sections_list\n",
    "\n",
    "def is_valid_section(section):\n",
    "    return section and len(section) > 20\n",
    "\n",
    "def is_english(line):\n",
    "    try:\n",
    "        return detect(line) == 'en'\n",
    "    except LangDetectException as e:\n",
    "        print(f\"An exception occurred: {e} : {line}\")\n",
    "        return False\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4066d417",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(input_text):\n",
    "    # Remove lines with only whitespace\n",
    "    input_text = re.sub(r'^\\s*$', '', input_text, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove lines containing only uppercase text (potential headings)\n",
    "    input_text = re.sub(r'^\\s*[A-Z\\s]+\\s*$', '', input_text, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove lines with multiple consecutive uppercase words (potential headings)\n",
    "    input_text = re.sub(r'^\\s*(?:[A-Z]+\\s*){2,}\\s*$', '', input_text, flags=re.MULTILINE)\n",
    "    \n",
    "    input_text = re.sub(r'^\\s*[A-Za-z\\s]+\\.{3,}\\s*\\d+\\s*$', '', input_text, flags=re.MULTILINE)\n",
    "    \n",
    "     # Remove lines with dots and numbers (potential table of contents entries)\n",
    "    #input_text = re.sub(r'^\\s*[A-Za-z\\s]+\\.{3,}\\s*\\d+\\s*$', '', input_text, flags=re.MULTILINE)\n",
    "\n",
    "\n",
    "    return input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cd35416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files: 8\n",
      "\n",
      "['ALB-NES-2018-EN.txt', 'ALB-NETS-2019-EN.txt', 'ALB-CPD-2021-EN.txt', 'ALB-NREP-2021-EN.txt', 'ALB-NREAP-2016-EN.txt', 'ALB-NREAP-2015-EN.txt', 'ALB-NEP-2013-EN.txt', 'ALB-NECP-2021-EN.txt']\n"
     ]
    }
   ],
   "source": [
    "folder_path = ('Data/')\n",
    "file_list = os.listdir(folder_path)\n",
    "\n",
    "# Filter the list to include only text files (e.g., .txt files)\n",
    "text_files = [file for file in file_list if file.endswith(\".txt\")]\n",
    "\n",
    "print (f\"Number of files: {len(text_files)}\\n\")  \n",
    "print (text_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "644ec132",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(folder_path, text_files[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32344bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text length: 21907\n"
     ]
    }
   ],
   "source": [
    "with open (file_path, 'r') as file:\n",
    "    raw_text = file.read()\n",
    "    file.close()\n",
    "\n",
    "print (f\"Original text length: {len(raw_text)}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1d8fa1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An exception occurred: No features in text. : \n",
      "\n",
      "Read text length: 21377\n",
      "Cleaned text length: 21377\n"
     ]
    }
   ],
   "source": [
    " # Open the file in read mode\n",
    "with open(file_path, 'r') as file:\n",
    "    \n",
    "    pattern = re.compile(r'.*?\\.{3}.*?$', re.MULTILINE)\n",
    "    # Initialize an empty string to store the lines\n",
    "    raw_text = ''\n",
    "    \n",
    "    head = [next(file) for _ in range(11)]\n",
    "    next(file)\n",
    "    \n",
    "    # Iterate over each line in the file\n",
    "    for line in file:\n",
    "        # Append the current line to the string\n",
    "        if not pattern.search(line) and is_english(line):\n",
    "            raw_text += line\n",
    "            \n",
    "print(f\"Read text length: {len(raw_text)}\") \n",
    "\n",
    "text = clean_text(raw_text)      \n",
    "\n",
    "print(f\"Cleaned text length: {len(text)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e17194e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'File Name': 'ALB-CPD-2021-EN', 'Year': '2021', 'Country Name': 'Albania', 'Country Code': 'ALB', 'Category': 'CPD', 'Document Title': 'UN Country programme document for Albania (2022–2026)', 'Publication Date': '30 August–2 September 2021', 'Start Year': '2022', 'End Year': '2026', 'Language': 'EN'}\n"
     ]
    }
   ],
   "source": [
    "metadata = {}\n",
    "\n",
    "# Iterate through the data list\n",
    "for item in head:\n",
    "    # Split each element by ':' and strip the resulting strings\n",
    "    key, value = item.split(':')\n",
    "    key = key.strip()\n",
    "    value = value.strip()\n",
    "    \n",
    "    # Add the key-value pair to the dictionary\n",
    "    metadata[key] = value\n",
    "\n",
    "\n",
    "if 'Exists?' in metadata:\n",
    "    metadata.pop('Exists?')\n",
    "print(metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e918387c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_sections = get_text_section(3000, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c68cb471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of sections from the text: 8\n"
     ]
    }
   ],
   "source": [
    "print (f\"The number of sections from the text: {len(text_sections)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b815a198",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = re.sub('== see also ==.*|[@#:&\\\"]|===.*?===|==.*?==|\\(.*?\\)', '', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38eb33c",
   "metadata": {},
   "source": [
    "# Entities Post-Processing Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec7e10db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging the broken entities\n",
    "def create_entities(lst):\n",
    "    i = 1\n",
    "    while i < len(lst):\n",
    "        if lst[i][\"word\"].startswith('##'):\n",
    "            lst[i][\"word\"] = lst[i-1][\"word\"] + lst[i][\"word\"][2:]\n",
    "            lst[i][\"score\"] = max(lst[i-1][\"score\"] , lst[i][\"score\"])\n",
    "            del lst[i-1]\n",
    "        else:\n",
    "            i += 1\n",
    "            # todo: return a list of merged entities\n",
    "            \n",
    "\n",
    "\n",
    "def apply_threshold(list_, threshold):\n",
    "    words_list = []\n",
    "    for item in list_:\n",
    "        if item['score'] > threshold:  # threshold score to eliminate unimportant entities\n",
    "            words_list.append(item['word'])\n",
    "    return words_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f27e0af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw(list_):\n",
    "    output = []\n",
    "    for sublist in list_:\n",
    "        new = []\n",
    "        obj = {}\n",
    "        for item in sublist:\n",
    "            #obj = {}\n",
    "            key = ''.join(filter(str.isalpha, item))\n",
    "            obj[key]= item\n",
    "            #obj['raw']= ''.join(filter(str.isalpha, item))\n",
    "        output.append(obj)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d5c833db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_extracted_entities(wiki, bert, gpt):\n",
    "    \n",
    "    output = []\n",
    "    dict_ = gpt\n",
    "    dict_.update(wiki)\n",
    "    \n",
    "    wiki_set = set(wiki.keys())\n",
    "    bert_set = set(bert.keys())\n",
    "    gpt_set = set(gpt.keys())\n",
    "    \n",
    "    A = gpt_set.intersection(bert_set)\n",
    "    B = bert_set.intersection(wiki_set)\n",
    "    C = gpt_set.intersection(wiki_set)\n",
    "\n",
    "    matched = list(A.union(B).union(C))\n",
    "    \n",
    "    for i in matched:\n",
    "        output.append(dict_[i])\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4ff671cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_entities(list_):\n",
    "    \n",
    "    # Define a regular expression pattern to match invalid characters.\n",
    "    pattern = r'\\s*{}\\s*'.format(re.escape(\"’\"))\n",
    "    pattern1 = r'\\s*{}\\s*'.format(re.escape(\"/\"))\n",
    "    output_list = []\n",
    "\n",
    "    for item in list_:\n",
    "        item = re.sub(pattern, \"’\", item)\n",
    "        tem = re.sub(pattern1, \"/\", item)\n",
    "            \n",
    "    return output_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f074fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(index, wiki, bert, gpt, acronym):\n",
    "    checkpoint = {'index': index, 'wiki': wiki, 'bert': bert, 'gpt': gpt, 'acronym': acronym}\n",
    "    with open('checkpoint.pkl', 'wb') as checkpoint_file:\n",
    "        pickle.dump(checkpoint, checkpoint_file)\n",
    "\n",
    "# Function to load the state\n",
    "def load_checkpoint(length):\n",
    "    try:\n",
    "        with open('checkpoint.pkl', 'rb') as checkpoint_file:\n",
    "            checkpoint = pickle.load(checkpoint_file)\n",
    "            return checkpoint['index'], checkpoint['wiki'], checkpoint['bert'], \n",
    "        checkpoint['gpt'], checkpoint['acronym']\n",
    "    except FileNotFoundError:\n",
    "        return 0, [''] * length, [''] * length, [''] * length, {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "49a9cc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_length = len(text_sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "60fb4f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the last checkpoint\n",
    "start_index, wiki_entity_list, bert_entity_list, gpt_entity_list, acronyms = load_checkpoint(text_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9d21a6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_index = 0\n",
    "wiki_entity_list = [''] * text_length\n",
    "bert_entity_list = [''] * text_length\n",
    "gpt_entity_list = [''] * text_length\n",
    "acronyms = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e8df97aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WIKI DONE\n",
      "BERT DONE\n",
      "GPT DONE\n",
      "NUMBER OF PROCESSED SECTIONS: 0\n",
      "WIKI DONE\n",
      "BERT DONE\n",
      "GPT DONE\n",
      "NUMBER OF PROCESSED SECTIONS: 1\n",
      "WIKI DONE\n",
      "BERT DONE\n",
      "GPT DONE\n",
      "NUMBER OF PROCESSED SECTIONS: 2\n",
      "WIKI DONE\n",
      "BERT DONE\n",
      "GPT DONE\n",
      "NUMBER OF PROCESSED SECTIONS: 3\n",
      "WIKI DONE\n",
      "BERT DONE\n",
      "GPT DONE\n",
      "NUMBER OF PROCESSED SECTIONS: 4\n",
      "WIKI DONE\n",
      "BERT DONE\n",
      "GPT DONE\n",
      "NUMBER OF PROCESSED SECTIONS: 5\n",
      "WIKI DONE\n",
      "BERT DONE\n",
      "GPT DONE\n",
      "NUMBER OF PROCESSED SECTIONS: 6\n",
      "WIKI DONE\n",
      "BERT DONE\n",
      "GPT DONE\n",
      "NUMBER OF PROCESSED SECTIONS: 7\n",
      "TIME TAKEN TO EXTRACT ENTITIES from 8 section: 40.026734828948975\n"
     ]
    }
   ],
   "source": [
    "# Continue from the last checkpoint\n",
    "\n",
    "start_time = time.time()\n",
    "for index in range(start_index, text_length):\n",
    "    try:\n",
    "        segment = text_sections[index]\n",
    "        \n",
    "        ## WIKINEURAL BILINGUAL MODEL\n",
    "        wiki_output = query_wiki({\n",
    "            \"inputs\": segment,\n",
    "        })\n",
    "        create_entities(wiki_output)\n",
    "        wiki_words = list(set(apply_threshold(wiki_output, 0.7)))\n",
    "        wiki_entity_list[index] = wiki_words\n",
    "        print (\"WIKI DONE\")\n",
    "\n",
    "        ## BERT BASE MODEL\n",
    "        bert_output = query_bert({\n",
    "            \"inputs\": segment,\n",
    "        })\n",
    "        create_entities(bert_output)\n",
    "        bert_words = list(set(apply_threshold(bert_output, 0.7)))\n",
    "        bert_entity_list[index] = bert_words\n",
    "        print (\"BERT DONE\")\n",
    "\n",
    "\n",
    "        ## GPT PROMPT\n",
    "        gpt_output = query_gpt(segment)\n",
    "        gpt_entity_list[index] = gpt_output['proper_nouns']\n",
    "\n",
    "        print (\"GPT DONE\")\n",
    "\n",
    "        ## Acronyms extraction\n",
    "        acronyms.update(gpt_output['acronyms'])\n",
    "    \n",
    "        \n",
    "        print(f\"NUMBER OF PROCESSED SECTIONS: {index}\")\n",
    "\n",
    "        # Save checkpoint at intervals\n",
    "        #if index % 5 == 0:\n",
    "            #save_checkpoint(index, wiki_entity_list, bert_entity_list, gpt_entity_list, acronyms)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing section {index}: {str(e)}\")\n",
    "        #save_checkpoint(index, wiki_entity_list, bert_entity_list, gpt_entity_list, acronyms)\n",
    "\n",
    "        continue  # Exit the loop in case of an error\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"TIME TAKEN TO EXTRACT ENTITIES from {text_length} section: {elapsed_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6df46581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'UNDP': 'United Nations Development Programme', 'UNFPA': 'United Nations Population Fund', 'UNOPS': 'United Nations Office for Project Services', 'UN': 'United Nations', 'SDG': 'Sustainable Development Goals', 'EU': 'European Union', 'NSDI': 'National Spatial Data Infrastructure', 'UNSDCF': 'United Nations Sustainable Development Cooperation Framework', 'STEM': 'Science, Technology, Engineering, and Mathematics', 'SDGs': 'Sustainable Development Goals'}\n"
     ]
    }
   ],
   "source": [
    "print (acronyms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a46f014",
   "metadata": {},
   "source": [
    "Processing the Entities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "017e28b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get raw version of entities for comparison\n",
    "raw_wiki = get_raw(wiki_entity_list)\n",
    "raw_bert = get_raw(bert_entity_list)\n",
    "raw_gpt = get_raw(gpt_entity_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "349b1d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The number of matching entities in section 1: 14\n",
      "\n",
      "['European Union', 'Human Development Index', 'Sustainable Development Goal', 'New York', 'National Strategy for Development and Integration', 'UNDP', 'United Nations Population Fund', 'United Nations Office for Project Services', 'Albania', 'Agenda for Sustainable Development', 'English', 'Inequality Index', 'United Nations Development Programme', 'United']\n",
      "\n",
      "--------------\n",
      "\n",
      "The number of matching entities in section 2: 9\n",
      "\n",
      "['Albania', 'United Nations', 'Roma', 'European Union', 'UNDP', 'Agenda', 'United Nations Sustainable Development Cooperation Framework', 'Egyptian', 'Sustainable Development Goals']\n",
      "\n",
      "--------------\n",
      "\n",
      "The number of matching entities in section 3: 9\n",
      "\n",
      "['Western Balkan Regional Economic Area', 'Nations', 'Albania', 'United Nations', 'European Union', 'European Union Civil Protection Mechanism', 'UNDP', 'UNSDCF', 'Sustainable Development Goals']\n",
      "\n",
      "--------------\n",
      "\n",
      "The number of matching entities in section 4: 5\n",
      "\n",
      "['Nations', 'Albania', 'United Nations', 'European Union', 'UNDP']\n",
      "\n",
      "--------------\n",
      "\n",
      "The number of matching entities in section 5: 3\n",
      "\n",
      "['Roma', 'Egyptian', 'UNDP']\n",
      "\n",
      "--------------\n",
      "\n",
      "The number of matching entities in section 6: 7\n",
      "\n",
      "['United Nations agencies', 'United Nations', 'Commissioners for Information and Data Protection and Protection from Discrimination', 'Government', 'UNDP', 'People ’ s Advocate', 'National']\n",
      "\n",
      "--------------\n",
      "\n",
      "The number of matching entities in section 7: 7\n",
      "\n",
      "['Albania', 'United Nations', 'European Union', 'UNDP', 'Government – United Nations', 'Executive Board', 'Sustainable Development Goals']\n",
      "\n",
      "--------------\n",
      "\n",
      "The number of matching entities in section 8: 1\n",
      "\n",
      "['National Statistical Office']\n",
      "\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "entity_objects = []\n",
    "merged = []\n",
    "i = 0\n",
    "\n",
    "while i < len(raw_wiki):\n",
    "    merged = merge_extracted_entities(raw_wiki[i], raw_bert[i], raw_gpt[i])\n",
    "    print (f\"\\nThe number of matching entities in section {i+1}: {len(merged)}\\n\")\n",
    "    print (merged)\n",
    "    \n",
    "    print (\"\\n--------------\")\n",
    "\n",
    "    entity_objects.append(merged)\n",
    "    \n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ab5cd1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "## Code to manually compare extarcted entities from 3 models\n",
    "i = 0\n",
    "\n",
    "while i < len(entity_objects):\n",
    "    \n",
    "    print (bert_entity_list[i])\n",
    "    print (\"-----\")\n",
    "    print (wiki_entity_list[i])\n",
    "    print (\"-----\")\n",
    "    print (gpt_entity_list[i])\n",
    "    print (\"-----\")\n",
    "    print (entity_objects[i])\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    i = i+1\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b6c56a",
   "metadata": {},
   "source": [
    "#  Categorize entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e05942",
   "metadata": {},
   "source": [
    "Zero Shot Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "197cdc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_entities(text, entities, categories):\n",
    "    \n",
    "    \n",
    "    categorization_prompt = f\"\"\"\n",
    "\n",
    "    You will be given a >>>>>TEXT<<<<<, an >>>>>EntityList<<<<< and >>>>>Categories<<<<<. \n",
    "    Your task is to assign a sutiable category to each element of >>>>>EntityList<<<<<.\n",
    "    \n",
    "    Return a list of JSON objects of categorized entities. \n",
    "\n",
    "\n",
    "    >>>>>TEXT<<<<<\n",
    "    {text}\n",
    "\n",
    "    >>>>>Categories<<<<<\n",
    "    {categories}\n",
    "\n",
    "    >>>>>EntityList<<<<<\n",
    "    {entities}\n",
    "    \"\"\"\n",
    "\n",
    "    categorized_entities = get_answer(categorization_prompt, 30)\n",
    "    categorized_entities = json.loads(categorized_entities)\n",
    "    \n",
    "    return (categorized_entities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0d1e03",
   "metadata": {},
   "source": [
    "# Relation Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03230ac",
   "metadata": {},
   "source": [
    "Chain of Thought - Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "ebf0d4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_relation_details(text, entities, relation_labels):\n",
    "    relation_extraction_prompt = f\"\"\"\n",
    "    \n",
    "    [Context]\n",
    "    You belong to a team of consultants at UNDP's Sustainable Energy Hub (SEH), working on a project to extract a \n",
    "    Knowledge Graph from the UNDP dataset.\n",
    "    You will be given a >>>>>TEXT<<<<<, an >>>>>EntityList<<<<< and a list of >>>>>RelationLabels<<<<<.\n",
    "\n",
    "   [Task]\n",
    "   \n",
    "   Your task is to perform Relation Extraction on the given >>>>>TEXT<<<<< \n",
    "   to find relations between elements of provided >>>>>EntityList<<<<<.\n",
    "   \n",
    "   Please make sure to read these instructions and constraints carefully.\n",
    "\n",
    "    [Instructions]\n",
    "    1. Carefully read and store the >>>>>RelationLabels<<<<<.\n",
    "    2. Scan the >>>>>TEXT<<<<< to find Named Entites from >>>>>EntityList<<<<< that are related.\n",
    "    3. Scan the >>>>>RelationLabels<<<<< to select a suitable label to\n",
    "    describe the relation between the above selected entities. Mark this label as \"Relation\".\n",
    "    4. Assign \"Subject\" and \"Object\" to entities depending on the selected \"Relation\"\n",
    "    selected in previous step to create a tuple.\n",
    "    5. If available, select a small \"Description\" from the >>>>>TEXT<<<<< for the above relation.\n",
    "    6. Assign a Relevance score between 1 to 10 to the extracted relation, with 10 being the most relevant.\n",
    "    7. Repeat the process to extract remaining relations from >>>>>TEXT<<<<<.\n",
    "    \n",
    "    \n",
    "    [Constraints]\n",
    "    1. Values of 'Relation' key should belong to >>>>>RelationLabels<<<<<.\n",
    "    \n",
    "    [Output Format]\n",
    "    Provide the result as a JSON array.\n",
    "\n",
    "    Perform relation extraction on the below:\n",
    "    \n",
    "    >>>>>TEXT<<<<<\n",
    "    {text}\n",
    "\n",
    "    >>>>>EntityList<<<<<\n",
    "    {entities}\n",
    "\n",
    "    >>>>>RelationLabels<<<<<\n",
    "    {relation_labels}\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "    relations = get_answer(relation_extraction_prompt,60)\n",
    "    relations = json.loads(relations)\n",
    "\n",
    "    return relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "686b9f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Modified relation for testing\n",
    "\n",
    "def extract_relation_details_refined(text, entities, relation_labels):\n",
    "    relation_extraction_prompt = f\"\"\"\n",
    "    \n",
    "    [Context]\n",
    "    You are part of the UNDP's Sustainable Energy Hub team, tasked with extracting a Knowledge Graph from a dataset.\n",
    "\n",
    "    [Task]\n",
    "    Perform Relation Extraction on the provided text to find relations between entities in the given entity list.\n",
    "\n",
    "    [Instructions]\n",
    "    1. Understand the relation labels: {relation_labels}.\n",
    "    2. Identify relationships in {text} between entities in {entities}.\n",
    "    3. Use the relation labels to describe each identified relationship.\n",
    "    4. Label entities as \"Subject\" and \"Object\" based on their roles in the relation.\n",
    "    5. Provide a brief description from the text for each relation.\n",
    "    6. Assign a relevance score (1-10) to each relation, with 10 being the most relevant.\n",
    "\n",
    "    [Example]\n",
    "    Given text: \"Sustainability Framework was deployed by UNDP to support communities and organizations\n",
    "    in achieving sustainability.\"\n",
    "    Entities: [\"Sustainability Framework\", \"UNDP\"]\n",
    "    Relation Labels: {relation_labels}\n",
    "    Output: [{{\"Subject\": \"UNDP\", \"Relation\": \"implements\", \"Object\": \"Sustainability Framework\", \n",
    "    \"Description\": \"Sustainability Framework was deployed by Company X to support communities and organizations\n",
    "    in achieving sustainability.\", \"Relevance\": 8}}]\n",
    "\n",
    "    [Constraints]\n",
    "    1. Only use entities from {entities}.\n",
    "    2. Relation labels must be from {relation_labels}.\n",
    "\n",
    "    [Output Format]\n",
    "    Provide the result as a JSON array.\n",
    "    \n",
    "    Now, perform relation extraction on the following:\n",
    "    \n",
    "    Text:\n",
    "    {text}\n",
    "\n",
    "    Entity List:\n",
    "    {entities}\n",
    "\n",
    "    Relation Labels:\n",
    "    {relation_labels}\n",
    "    \"\"\"\n",
    "    relations = get_answer(relation_extraction_prompt,60)\n",
    "    relations = json.loads(relations)\n",
    "\n",
    "    return relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7c43b8d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'United Nations Development Programme': 'UNDP', 'United Nations Population Fund': 'UNFPA', 'United Nations Office for Project Services': 'UNOPS', 'United Nations': 'UN', 'Sustainable Development Goals': 'SDGs', 'European Union': 'EU', 'United Nations Sustainable Development Cooperation Framework': 'UNSDCF', 'Science, Technology, Engineering, and Mathematics': 'STEM', 'National Spatial Data Infrastructure': 'NSDI', 'Country Programme Document': 'CPD', 'Coronavirus Disease 2019': 'COVID-19', 'National Statistical Office': 'NSO'}\n"
     ]
    }
   ],
   "source": [
    "# invert acronyms dict to ease look up\n",
    "acronyms_dict = {v: k for k, v in acronyms.items()}\n",
    "print (acronyms_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d730afd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_list = []\n",
    "relations_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61fea8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "seen_entities = set()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for index, uncategorized_entities in enumerate(entity_objects):\n",
    "    entities_subset = categorize_entities(text_sections[index], uncategorized_entities, categories)\n",
    "    \n",
    "    # Add 'acronym' key to entity list\n",
    "    for item in entities_subset:\n",
    "        if item[\"entity\"] not in seen_entities:\n",
    "            seen_entities.add(item[\"entity\"])\n",
    "            \n",
    "            if item[\"entity\"] in acronyms_dict.keys():\n",
    "                item[\"acronym\"] = acronyms_dict[item[\"entity\"]]\n",
    "            \n",
    "            entities_list.append(item)\n",
    "    \n",
    "    print (\"CATEGORIZED ENTITIES: \\n\")\n",
    "    print (entities_subset)\n",
    "    \n",
    "    relations_subset = extract_relation_details(text_sections[index], entities_subset, relation_labels)\n",
    "    \n",
    "    print (\"\\n EXTRACTED RELATIONS: \\n\")\n",
    "    print (relations_subset)\n",
    "    \n",
    "    relations_list.extend(relations_subset)\n",
    "    \n",
    "\n",
    "    print (\"\\n-------------------\")\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"TIME TAKEN TO EXTRACT RELATIONS FROM {text_length} SECTIONS: {elapsed_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "93a973dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "90\n"
     ]
    }
   ],
   "source": [
    "print(len(entities_list))\n",
    "print (len(relations_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "21e264dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a list of names, to check for \n",
    "\n",
    "entity_names = set([item['entity'] for item in entities_list])\n",
    "entity_names.update(acronyms.keys())\n",
    "entity_names.update(acronyms.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e409383f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "print (len(acronyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5915e211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n"
     ]
    }
   ],
   "source": [
    "print (len(entity_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "60b0bc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_relations = [i for i in relations_list if i['Subject'] in entity_names and i['Object'] in entity_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "8b46f9b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n"
     ]
    }
   ],
   "source": [
    "print (len(final_relations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "d4e80840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Relation': 'an_instance_of', 'Subject': 'Executive Board', 'Object': 'United Nations Development Programme', 'Description': 'The Executive Board is an instance of the United Nations Development Programme.', 'Relevance': 8}, {'Relation': 'an_instance_of', 'Subject': 'Executive Board', 'Object': 'United Nations Population Fund', 'Description': 'The Executive Board is an instance of the United Nations Population Fund.', 'Relevance': 8}, {'Relation': 'an_instance_of', 'Subject': 'Executive Board', 'Object': 'United Nations Office for Project Services', 'Description': 'The Executive Board is an instance of the United Nations Office for Project Services.', 'Relevance': 8}, {'Relation': 'focuses_on', 'Subject': 'UNDP', 'Object': 'Sustainable Development Goal', 'Description': 'UNDP focuses on the Sustainable Development Goals.', 'Relevance': 9}, {'Relation': 'in', 'Subject': 'Albania', 'Object': 'European Union', 'Description': 'Albania is in the European Union.', 'Relevance': 7}, {'Relation': 'partners_with', 'Subject': 'UNDP', 'Object': 'Government', 'Description': 'UNDP partners with the Government.', 'Relevance': 8}, {'Relation': 'monitors', 'Subject': 'UNDP', 'Object': 'Sustainable Development Goals', 'Description': 'UNDP monitors the progress of the Sustainable Development Goals.', 'Relevance': 8}, {'Relation': 'measures', 'Subject': 'Albania', 'Object': 'Human Development Index', 'Description': 'Albania measures its Human Development Index.', 'Relevance': 7}, {'Relation': 'measures', 'Subject': 'Albania', 'Object': 'Inequality Index', 'Description': 'Albania measures its Inequality Index.', 'Relevance': 7}, {'Relation': 'supports', 'Subject': 'UNDP', 'Object': 'Albania', 'Description': 'UNDP support to development remains highly relevant in Albania', 'Relevance': 8}, {'Relation': 'focuses_on', 'Subject': 'UNDP', 'Object': 'Sustainable Development Goals', 'Description': 'UNDP will focus on local-level actions that inform national strategies and build forward better', 'Relevance': 9}, {'Relation': 'partners_with', 'Subject': 'UNDP', 'Object': 'United Nations agencies', 'Description': 'UNDP will partner with United Nations agencies on joint programmes', 'Relevance': 7}, {'Relation': 'supports', 'Subject': 'UNDP', 'Object': 'European Union', 'Description': 'UNDP supports aspirations to join the European Union', 'Relevance': 8}, {'Relation': 'partners_with', 'Subject': 'UNDP', 'Object': 'United Nations', 'Description': 'Partnering with United Nations agencies', 'Relevance': 8}, {'Relation': 'contributes_to', 'Subject': 'UNDP', 'Object': 'Sustainable Development Goals', 'Description': 'consensus on three highly effective solutions for policy and programming to accelerate progress towards European Union accession and achieve the Sustainable Development Goals', 'Relevance': 9}, {'Relation': 'partners_with', 'Subject': 'UNDP', 'Object': 'Nations', 'Description': 'UNDP partners with Nations to achieve sustainable development goals.', 'Relevance': 8}, {'Relation': 'contributes_to', 'Subject': 'UNDP', 'Object': 'Albania', 'Description': 'UNDP contributes to the development of Albania.', 'Relevance': 9}, {'Relation': 'aligns_with', 'Subject': 'UNDP', 'Object': 'European Union', 'Description': 'UNDP aligns its activities with the European Union.', 'Relevance': 7}, {'Relation': 'supports', 'Subject': 'UNDP', 'Object': 'Albania', 'Description': 'UNDP supports Albania in various areas.', 'Relevance': 8}, {'Relation': 'supports', 'Subject': 'UNDP', 'Object': 'United Nations', 'Description': 'UNDP supports the United Nations in achieving its goals.', 'Relevance': 9}, {'Relation': 'supports', 'Subject': 'UNDP', 'Object': 'Roma', 'Description': 'UNDP will support Roma communities to consolidate reforms, strengthen capacities, address skills mismatch and increase labour force participation especially for women and young people, persons with disabilities, Roma and Egyptian communities and the long-term unemployed.', 'Relevance': 8}, {'Relation': 'supports', 'Subject': 'UNDP', 'Object': 'Egyptian', 'Description': 'UNDP will support Egyptian communities to consolidate reforms, strengthen capacities, address skills mismatch and increase labour force participation especially for women and young people, persons with disabilities, Roma and Egyptian communities and the long-term unemployed.', 'Relevance': 8}, {'Relation': 'supports', 'Subject': 'UNDP', 'Object': 'Government', 'Description': 'UNDP will continue to support the Government to address structural and systemic barriers to gender equality and women’s empowerment in policy formulation and implementation, and to generate quality gender-disaggregated data for targeted, informed investments and quantifying of results.', 'Relevance': 9}, {'Relation': 'partners_with', 'Subject': 'UNDP', 'Object': 'United Nations agencies', 'Description': 'UNDP will continue to work closely with existing donors, vertical funds and United Nations agencies, diversifying funding sources by partnering with new donors including on an integrated national finance framework.', 'Relevance': 7}, {'Relation': 'supports', 'Subject': 'UNDP', 'Object': 'Albania', 'Description': 'UNDP will support Albania in'}, {'Relation': 'partners_with', 'Subject': 'UNDP', 'Object': 'Government – United Nations', 'Description': 'UNDP will participate in joint Government–United Nations coordination mechanisms.'}, {'Relation': 'administers', 'Subject': 'UNDP', 'Object': 'Executive Board', 'Description': 'UNDP will administer the Executive Board.'}, {'Relation': 'aligns_with', 'Subject': 'UNDP', 'Object': 'Sustainable Development Goals', 'Description': \"UNDP's results framework is aligned with indicators related to the Sustainable Development Goals.\"}, {'Relation': 'contributes_to', 'Subject': 'UNDP', 'Object': 'European Union', 'Description': \"UNDP's programme outcomes contribute to European Union accession.\"}]\n"
     ]
    }
   ],
   "source": [
    "print (final_relations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ce9275",
   "metadata": {},
   "source": [
    "# Write the output to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b071ef18",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_relations = json.dumps(final_relations, indent=2)\n",
    "json_entities = json.dumps(combined_entities, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d99a090",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Entities/' + metadata['File Name']+ '.json', \"w\") as output_file:\n",
    "    output_file.write(json_entities)\n",
    "    output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c596dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Relations/' + metadata['File Name']+ '.json', \"w\") as output_file:\n",
    "    output_file.write(json_relations)\n",
    "    output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4862fd69",
   "metadata": {},
   "source": [
    "# Connecting with DBpedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2a53f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "# Define the DBpedia SPARQL endpoint\n",
    "sparql_endpoint = \"http://dbpedia.org/sparql\"\n",
    "\n",
    "# Create a SPARQLWrapper instance\n",
    "sparql = SPARQLWrapper(sparql_endpoint)\n",
    "\n",
    "# Function to search for an entity by label and return its DBpedia URI\n",
    "def search_entity(label):\n",
    "    query = f\"\"\"\n",
    "    SELECT ?entity\n",
    "    WHERE {{\n",
    "      ?entity rdfs:label \"{label}\"@en.\n",
    "    }}\n",
    "    LIMIT 1\n",
    "    \"\"\"\n",
    "\n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "\n",
    "    results = sparql.query().convert()\n",
    "\n",
    "    if \"results\" in results and \"bindings\" in results[\"results\"] and results[\"results\"][\"bindings\"]:\n",
    "        entity_uri = results[\"results\"][\"bindings\"][0][\"entity\"][\"value\"]\n",
    "        return entity_uri\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Function to retrieve and return the abstract or comment of an entity\n",
    "def retrieve_entity_summary(entity_uri):\n",
    "    # Try to retrieve the abstract\n",
    "    abstract_query = f\"\"\"\n",
    "    SELECT ?abstract\n",
    "    WHERE {{\n",
    "      <{entity_uri}> dbo:abstract ?abstract.\n",
    "      FILTER (LANGMATCHES(LANG(?abstract), \"en\"))\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    sparql.setQuery(abstract_query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "\n",
    "    abstract_results = sparql.query().convert()\n",
    "\n",
    "    if \"results\" in abstract_results and \"bindings\" in abstract_results[\"results\"]:\n",
    "        for result in abstract_results[\"results\"][\"bindings\"]:\n",
    "            abstract = result[\"abstract\"][\"value\"]\n",
    "            return abstract\n",
    "\n",
    "    # If abstract is not found, try to retrieve the comment\n",
    "    comment_query = f\"\"\"\n",
    "    SELECT ?comment\n",
    "    WHERE {{\n",
    "      <{entity_uri}> rdfs:comment ?comment.\n",
    "      FILTER (LANGMATCHES(LANG(?comment), \"en\"))\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    sparql.setQuery(comment_query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "\n",
    "    comment_results = sparql.query().convert()\n",
    "\n",
    "    if \"results\" in comment_results and \"bindings\" in comment_results[\"results\"]:\n",
    "        for result in comment_results[\"results\"][\"bindings\"]:\n",
    "            comment = result[\"comment\"][\"value\"]\n",
    "            return comment\n",
    "\n",
    "    # If neither abstract nor comment is found, return None\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c57444",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.error\n",
    "\n",
    "def dbpedia_summary(search_label):\n",
    "    entity_uri = search_entity(search_label)\n",
    "\n",
    "    if entity_uri:\n",
    "        print(f\"Entity found with DBpedia URI: {entity_uri}\")\n",
    "        try:\n",
    "            summary = retrieve_entity_summary(entity_uri)\n",
    "            if summary:\n",
    "                return summary\n",
    "            else:\n",
    "                print(\"No abstract or comment found for this entity.\")\n",
    "        except urllib.error.URLError as e:\n",
    "            print(f\"Error: {e}\")\n",
    "    else:\n",
    "        print(f\"No entity found with the label: {search_label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be620ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_summaries(entities):\n",
    "    summary_list = []\n",
    "    list_ = []\n",
    "\n",
    "    for item in entities:\n",
    "        summary = dbpedia_summary(item['entity'])\n",
    "        item['summary'] = summary\n",
    "\n",
    "    return entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3de5f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_entities = extract_summaries(entities_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae8f2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (final_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a57c4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (entities_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06754a38",
   "metadata": {},
   "source": [
    "# Add Relations to Spreadsheet for Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62e2121",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "\n",
    "# Use the credentials from the service account key JSON file you downloaded\n",
    "scope = [\"https://spreadsheets.google.com/feeds\", \"https://www.googleapis.com/auth/drive\"]\n",
    "creds = ServiceAccountCredentials.from_json_keyfile_name('energy-moonshot-ai-97aa9045e45f.json', scope)\n",
    "client = gspread.authorize(creds)\n",
    "\n",
    "# Open the Google Sheet by its title or URL\n",
    "spreadsheet = client.open_by_url('https://docs.google.com/spreadsheets/d/1yZ-XQQs52kaI5k9MjvV_CdbgWQi-GazjHHGqQUF8gko/edit')\n",
    "\n",
    "\n",
    "# Enter relations in the first sheet\n",
    "sheet = spreadsheet.sheet1\n",
    "\n",
    "# Start row index from 5\n",
    "start_row_index = 5\n",
    "index = 1\n",
    "\n",
    "# Check if there's valid data to insert\n",
    "if final_relations_1:\n",
    "    # Create a list of lists where each inner list represents the values of a row\n",
    "    batch_relations = []\n",
    "    for index, row_data in enumerate(final_relations_1):\n",
    "        row = [index, row_data['Subject'], row_data['Relation'], row_data.get('Object', ''), \n",
    "               row_data.get('Description', ''), row_data.get('Relevance', '')]\n",
    "        \n",
    "        batch_relations.append(row)\n",
    "        index = index + 1\n",
    "\n",
    "    # Insert the data into the Google Sheet starting from row 5\n",
    "    sheet.insert_rows(batch_relations, start_row_index)\n",
    "\n",
    "    print(f\"{len(final_relations_1)} entries added to Google Sheet.\")\n",
    "else:\n",
    "    print(\"No data to insert.\")\n",
    "    \n",
    "    \n",
    "# Enter entities in the second sheet\n",
    "sheet = spreadsheet.sheet2\n",
    "\n",
    "# Start row index from 5\n",
    "start_row_index = 5\n",
    "index = 1\n",
    "\n",
    "if entities_list:\n",
    "    batch_entities = []\n",
    "    for index, row_data in enumerate(entities_list):\n",
    "        row = [i, row_data['entity'], row_data['category'], row_data.get('acronym', ''), row_data.get('summary', '')]\n",
    "        batch_entities.append(row)\n",
    "        \n",
    "        index = index + 1\n",
    "    sheet.insert_rows(batch_entities, start_row_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7975f7",
   "metadata": {},
   "source": [
    "# Creating Graph in Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535855bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph, Node, Relationship\n",
    "graph = Graph(uri = 'bolt://localhost:7687',user='neo4j',password=NEO4JPASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c80244f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document:\n",
    "    def __init__(self, metadata, entities, relations):\n",
    "        self.metadata = metadata\n",
    "        self.entities = entities\n",
    "        self.relations = relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079f0f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "document = Document(metadata, final_entities, final_relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2cd7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create or retrieve a node\n",
    "def get_or_create_node(label, key, value):\n",
    "    # Attempt to find an existing node with the given label and key\n",
    "    existing_node = get_node(label, key, value)\n",
    "    \n",
    "    if existing_node:\n",
    "        return existing_node\n",
    "    else:\n",
    "        new_node = Node(label, **{key: value})\n",
    "        graph.create(new_node)\n",
    "        return new_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85c567e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node(label, key, value):\n",
    "    node = graph.nodes.match(label, **{key:value}).first()\n",
    "    return node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a79443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to insert relations \n",
    "def insert_relations_neo4j(document):\n",
    "    document_node = get_or_create_node(\"Document\", \"name\", document.metadata['Document Title'] )\n",
    "            \n",
    "    for key,value in metadata.items():\n",
    "        document_node[key] = value\n",
    "        \n",
    "    graph.push(document_node)\n",
    "    \n",
    "    for item in document.relations:\n",
    "        subject = get_or_create_node(\"Entity\", \"name\", item[\"Subject\"])\n",
    "        obj = get_or_create_node(\"Entity\", \"name\", item[\"Object\"])\n",
    "        relation = Relationship(subject, item[\"Relation\"], obj)\n",
    "        if 'Description' in item:\n",
    "            relation[\"Description\"] = item[\"Description\"]\n",
    "        \n",
    "        # Merge nodes and create relationships\n",
    "        graph.merge(subject, \"Subject\", \"name\")\n",
    "        graph.merge(obj, \"Object\", \"name\")\n",
    "        graph.create(relation)\n",
    "        \n",
    "        # Link the nodes to the project node\n",
    "        #graph.create(Relationship(subject, \"Belongs To\", document_node))\n",
    "        #graph.create(Relationship(obj, \"Belongs To\", document_node))\n",
    "        \n",
    "    for item in document.entities:\n",
    "        if \"acronym\" in item:\n",
    "            node = get_or_create_node(\"Entity\", \"name\", item[\"name\"])\n",
    "            node['acronym'] = item[\"acronym\"]\n",
    "            node['category'] = item[\"category\"]\n",
    "            graph.push(node)\n",
    "            graph.create(Relationship(node, \"Parent Document\", document_node))\n",
    "\n",
    "        else:\n",
    "            node = get_or_create_node(\"Entity\", \"name\", item[\"name\"])\n",
    "            node['category'] = item[\"category\"]\n",
    "            graph.push(node)\n",
    "            graph.create(Relationship(node, \"Parent Document\", document_node))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c93fb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to insert summaries \n",
    "def insert_summary_neo4j(data):\n",
    "    for item in data:\n",
    "        node = get_node(\"Entity\", \"name\", item.name)\n",
    "        node[\"Summary\"] = item.summary\n",
    "        graph.push(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c007af4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_relations_neo4j(document)\n",
    "insert_summary_neo4j(summary_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35dab65",
   "metadata": {},
   "source": [
    "# Delete the checkpoint file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc098710",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove(\"unwanted-file.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
