{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ff8ab69",
   "metadata": {},
   "source": [
    "<h2> Energy Knowledge Graph Pipeline</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "672cb5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##imports\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe3a350",
   "metadata": {},
   "outputs": [],
   "source": [
    "##set globals and filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5f05ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6996b28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b68eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade spacy pydantic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddf0b75",
   "metadata": {},
   "source": [
    "<h4>extract entities and relations for all docs </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6611daa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "non_nc = spacy.load('en_core_web_md')\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "SUBJECTS = [\"nsubj\", \"nsubjpass\", \"csubj\", \"csubjpass\", \"agent\", \"expl\"]\n",
    "VERBS = ['ROOT', 'advcl']\n",
    "OBJECTS = [\"dobj\", \"dative\", \"attr\", \"oprd\", 'pobj']\n",
    "ENTITY_LABELS = ['PERSON', 'NORP', 'GPE', 'ORG', 'FAC', 'LOC', 'PRODUCT', 'EVENT', 'WORK_OF_ART']\n",
    "\n",
    "def remove_unicode(text):\n",
    "    return text.encode(\"ascii\", \"ignore\").decode(\"utf-8\")\n",
    "\n",
    "def extract_relationship(text, entity1, entity2):\n",
    "    \n",
    "    relationship = None\n",
    "    \n",
    "    # Find start and end indices of entity1 and entity2\n",
    "    start_index_entity1 = text.lower().find(entity1.lower())\n",
    "    end_index_entity1 = start_index_entity1 + len(entity1)\n",
    "    start_index_entity2 = text.lower().find(entity2.lower())\n",
    "    end_index_entity2 = start_index_entity2 + len(entity2)\n",
    "    words_between = \"\"\n",
    "\n",
    "    # Check if both entities exist in the text\n",
    "    if start_index_entity1 != -1 and start_index_entity2 != -1:\n",
    "        # Entity2 occurs after Entity1\n",
    "        if start_index_entity2 > end_index_entity1:\n",
    "            words_between = text[end_index_entity1:start_index_entity2].strip()\n",
    "        # Entity1 occurs after Entity2\n",
    "        elif start_index_entity1 > start_index_entity2:\n",
    "            words_between = text[end_index_entity2:start_index_entity1].strip()\n",
    "            \n",
    "        words_doc = nlp(words_between)\n",
    "        for token in words_doc:\n",
    "            if token.pos_ == \"VERB\":\n",
    "                relationship = token.text\n",
    "                break\n",
    "\n",
    "    return relationship if relationship else \"None\"\n",
    "\n",
    "\n",
    "def extract_entities_relations(text):\n",
    "    # Process the text using spaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Extract unique entities (unique nouns, organizations, persons, and locations)\n",
    "    entities = []\n",
    "    entity_set = set()  # To track unique entities\n",
    "    for entity in doc.ents:\n",
    "        if entity.label_ == 'ORG':\n",
    "            category = 'Organization'\n",
    "        elif entity.label_ == 'LOC':\n",
    "            category = 'Location'\n",
    "        else:\n",
    "            category = ''\n",
    "        \n",
    "        if category and '.' not in entity.text and '\\n' not in entity.text:  # Avoid abbreviations and multi-line entities\n",
    "            entity_text = entity.text.lower()\n",
    "            if entity_text not in entity_set:  # Check if entity is not already encountered\n",
    "                entity_set.add(entity_text)\n",
    "                entities.append({\"entity\": entity.text, \"category\": category})\n",
    "\n",
    "\n",
    "   # Extract relations involving the identified entities\n",
    "    entity_relations = {entity['entity']: [] for entity in entities}\n",
    "\n",
    "    # Generate all possible pairs of entities\n",
    "    entity_pairs = [(entity1['entity'], entity2['entity']) for entity1 in entities for entity2 in entities if entity1['entity'] != entity2['entity']]\n",
    "    \n",
    "    # Process the text using spaCy\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Iterate through sentences\n",
    "    for sent in doc.sents:\n",
    "        # Check for relationships between pairs of entities in the sentence\n",
    "        for entity_pair in entity_pairs:\n",
    "            entity1, entity2 = entity_pair\n",
    "            if entity1.lower() in sent.text.lower() and entity2.lower() in sent.text.lower():\n",
    "                # print(f\"found sentence=== {sent.text.lower()} for entity {entity1.lower()} {entity2.lower()} \\n\\n\")\n",
    "                relatn = extract_relationship(remove_unicode(sent.text.lower()),entity1.lower(),entity2.lower() )\n",
    "                # Check if relation already exists for the entity pair\n",
    "                if relatn == 'None':\n",
    "                   h=0\n",
    "                else: \n",
    "\n",
    "                    relation = {\n",
    "                        \"Relation\": relatn,\n",
    "                        # \"Subject\": entity1.lower(),\n",
    "                        \"Object\": entity2.lower(),\n",
    "                        \"Description\": sent.text,\n",
    "                                }\n",
    "                    if relation not in entity_relations[entity1]:\n",
    "                        entity_relations[entity1].append(relation)\n",
    "                    if relation not in entity_relations[entity2]:\n",
    "                        entity_relations[entity2].append(relation)              \n",
    "\n",
    "    # Remove entities with no relations\n",
    "    entities_with_relations = [entity for entity in entities if entity_relations[entity['entity']]]\n",
    "    \n",
    "    return entities_with_relations, entity_relations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "9bd08c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_json(json_data):\n",
    "    # Remove duplicates in entities\n",
    "    entities = json_data[\"knowledge graph\"][\"entities\"]\n",
    "    unique_entities = {entity[\"entity\"]: entity for entity in entities}.values()\n",
    "    json_data[\"knowledge graph\"][\"entities\"] = list(unique_entities)\n",
    "    \n",
    "    # Remove relations with empty array\n",
    "    relations = json_data[\"knowledge graph\"][\"relations\"]\n",
    "    filtered_relations = {\n",
    "        key: value for key, value in relations.items() if value\n",
    "    }\n",
    "    json_data[\"knowledge graph\"][\"relations\"] = filtered_relations\n",
    "    \n",
    "    return json_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27af333",
   "metadata": {},
   "outputs": [],
   "source": [
    "##export results to '02_Output/00_By-Document/' in 00_Entities and 01_Relations\n",
    "\n",
    "# Replace 'path_to_folder' with the path to your folder containing .txt files\n",
    "\n",
    "\n",
    "import concurrent.futures\n",
    "\n",
    "def process_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        text_content = file.read()\n",
    "\n",
    "        # Extracting entities and relations\n",
    "        entities, relations = extract_entities_relations(text_content)\n",
    "        # Constructing JSON data\n",
    "        json_data = {\n",
    "            \"metadata\": {\n",
    "                # Add your metadata extraction logic here\n",
    "            },\n",
    "            \"knowledge graph\": {\n",
    "                \"entities\": entities,\n",
    "                \"relations\": relations\n",
    "            }\n",
    "        }\n",
    "        output_folder_path = '../02_Output/00_By-Document'\n",
    "\n",
    "        cleanedJSON = clean_json(json_data)\n",
    "        # Save as JSON\n",
    "        output_file_name = os.path.basename(file_path).replace('.txt', '.json')\n",
    "        output_file_path = os.path.join(output_folder_path, output_file_name)\n",
    "        with open(output_file_path, 'w') as output_file:\n",
    "            json.dump(cleanedJSON, output_file, indent=4)\n",
    "        return output_file_path\n",
    "\n",
    "folder_path = '../01_Input/01_Cleaned-Text'\n",
    "file_paths = [os.path.join(folder_path, file_name) for file_name in os.listdir(folder_path) if file_name.endswith(\".txt\")]\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    results = executor.map(process_file, file_paths)\n",
    "\n",
    "for result in results:\n",
    "    print(f\"Processed file saved at: {result}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4a472a",
   "metadata": {},
   "source": [
    "<h4> merge entities and relations across all docs </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "944ea724",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dictionary of all entities and dictionary of relations, with values for the documents they appear in\n",
    "#then iterate through each and create merged versions\n",
    "\n",
    "##export to '02_Output/01_Merged/' as energy-entities.json and energy-entities-datecode.json (and energy-relations)\n",
    "\n",
    "# Folder path containing JSON files\n",
    "# folder_path = '../02_Output/00_By-Document'\n",
    "\n",
    "# # Initialize an empty dictionary to hold merged data\n",
    "# merged_data = {\"metadata\": {}, \"knowledge graph\": {\"entities\": [], \"relations\": []}}\n",
    "\n",
    "# # Loop through JSON files in the folder\n",
    "# for file_name in os.listdir(folder_path):\n",
    "#     if file_name.endswith(\".json\"):\n",
    "#         with open(os.path.join(folder_path, file_name), 'r') as file:\n",
    "#             data = json.load(file)\n",
    "            \n",
    "#             # Merge metadata\n",
    "#             merged_data['metadata'].update(data['metadata'])\n",
    "            \n",
    "#             # Merge entities and relations\n",
    "#             merged_data['knowledge graph']['entities'].extend(data['knowledge graph']['entities'])\n",
    "#             merged_data['knowledge graph']['relations'].extend(data['knowledge graph']['relations'])\n",
    "\n",
    "# # Save merged data into a single JSON file\n",
    "# output_file = '../02_Output/01_Merged/merged-knowledge-graph.json'\n",
    "# with open(output_file, 'w') as outfile:\n",
    "#     json.dump(merged_data, outfile, indent=4)\n",
    "\n",
    "# print(f\"Merged data saved to {output_file}\")\n",
    "\n",
    "def merge_json_files(folder_path):\n",
    "    merged_entities = []\n",
    "    merged_relations = {}\n",
    "\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith(\".json\"):\n",
    "            with open(os.path.join(folder_path, file_name), 'r') as file:\n",
    "                data = json.load(file)\n",
    "                entities = data[\"knowledge graph\"][\"entities\"]\n",
    "                relations = data[\"knowledge graph\"][\"relations\"]\n",
    "\n",
    "                # Merge entities\n",
    "                merged_entities.extend(entities)\n",
    "\n",
    "                # Merge relations\n",
    "                for entity, rel_list in relations.items():\n",
    "                    if entity in merged_relations:\n",
    "                        merged_relations[entity].extend(rel_list)\n",
    "                    else:\n",
    "                        merged_relations[entity] = rel_list\n",
    "\n",
    "    # Construct merged JSON data\n",
    "    merged_json_data = {\n",
    "        \"metadata\": {},  # Add metadata as needed\n",
    "        \"knowledge graph\": {\n",
    "            \"entities\": merged_entities,\n",
    "            \"relations\": merged_relations\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return merged_json_data\n",
    "\n",
    "folder_path = '../02_Output/00_By-Document'\n",
    "merged_json = merge_json_files(folder_path)\n",
    "\n",
    "# Save the merged JSON data to a file\n",
    "output_file_path = '../02_Output/01_Merged/merged-knowledge-graph.json'\n",
    "with open(output_file_path, 'w') as output_file:\n",
    "    json.dump(merged_json, output_file, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3072ea",
   "metadata": {},
   "source": [
    "<h4> extract jsons for each entity </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4800426f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##iterate through keys in entities, and identify all relations for that entity\n",
    "##generate json including all info about that entity, its relations, all related entitites, and document ids\n",
    " \n",
    "\n",
    "# Folder path containing JSON files\n",
    "\n",
    "# Folder path containing JSON files\n",
    "folder_path = '../02_Output/01_Merged'\n",
    "\n",
    "# Dictionary to hold entity-wise relations\n",
    "entity_relations = defaultdict(list)\n",
    "\n",
    "# Loop through JSON files in the folder\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith(\".json\"):\n",
    "        with open(os.path.join(folder_path, file_name), 'r') as file:\n",
    "            data = json.load(file)\n",
    "            \n",
    "            entities = data['knowledge graph']['entities']\n",
    "            relations = data['knowledge graph']['relations']\n",
    "            \n",
    "            for entity_info in entities:\n",
    "                entity = entity_info['entity']\n",
    "                \n",
    "                if entity in relations:\n",
    "                    entity_relations[entity].extend(relations[entity])\n",
    "\n",
    "# Export to '02_Output/02_By-Entity/'\n",
    "output_folder_path = '../02_Output/02_By-Entity'\n",
    "# Create separate JSON files for each entity's relations\n",
    "for entity, relations in entity_relations.items():\n",
    "    entity_file = f'{entity}.json'\n",
    "    entity_data = {\"metadata\": {}, \"knowledge graph\": {\"entities\": [], \"relations\": {entity: relations}}}\n",
    "    \n",
    "    with open(os.path.join(output_folder_path, entity_file), 'w') as outfile:\n",
    "        json.dump(entity_data, outfile, indent=4)\n",
    "\n",
    "print(\"Entity files created.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9718269b",
   "metadata": {},
   "source": [
    "<h4> create json exports for countries and documents </h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0c4aea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##open country and document metadata csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a702e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##use full merged knowledge graph to extract entities and relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92513de2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
