{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ff8ab69",
   "metadata": {},
   "source": [
    "<h2> Energy Knowledge Graph Pipeline</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "672cb5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##imports\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe3a350",
   "metadata": {},
   "outputs": [],
   "source": [
    "##set globals and filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5f05ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6996b28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b68eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade spacy pydantic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddf0b75",
   "metadata": {},
   "source": [
    "<h4>extract entities and relations for all docs </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6611daa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "non_nc = spacy.load('en_core_web_md')\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "SUBJECTS = [\"nsubj\", \"nsubjpass\", \"csubj\", \"csubjpass\", \"agent\", \"expl\"]\n",
    "VERBS = ['ROOT', 'advcl']\n",
    "OBJECTS = [\"dobj\", \"dative\", \"attr\", \"oprd\", 'pobj']\n",
    "ENTITY_LABELS = ['PERSON', 'NORP', 'GPE', 'ORG', 'FAC', 'LOC', 'PRODUCT', 'EVENT', 'WORK_OF_ART']\n",
    "\n",
    "\n",
    "# You might need libraries like spaCy or NLTK for NLP tasks\n",
    "\n",
    "# Function to extract entities and relations (replace with your extraction logic)\n",
    "def extract_entities_relations(text):\n",
    "    # Process the text using spaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Extract entities\n",
    "    # Extract entities (unique nouns, organizations, persons, and locations)\n",
    "    entity_set = set()  # Set to track unique entities\n",
    "    entities = []\n",
    "    for entity in doc.ents:\n",
    "        if entity.label_ == 'ORG':\n",
    "            category = 'Organization'\n",
    "        elif entity.label_ == 'LOC':\n",
    "            category = 'Location'\n",
    "        else:\n",
    "            category = ''\n",
    "        \n",
    "        if category and '.' not in entity.text and '\\n' not in entity.text:  # Avoid abbreviations and multi-line entities\n",
    "            if entity.text not in entity_set:  # Check if entity is not already encountered\n",
    "                entity_set.add(entity.text)\n",
    "                entities.append({\"entity\": entity.text, \"category\": category})\n",
    "   \n",
    "    # Extract relations (based on simple pattern matching)\n",
    "\n",
    "    relations = []\n",
    "    for sent in doc.sents:\n",
    "        for token in sent:\n",
    "            if token.dep_ in VERBS and token.pos_ == 'VERB':\n",
    "                subj = [w for w in token.children if w.dep_ in SUBJECTS and w.text.lower() not in ['it', 'they', 'he', 'she']]\n",
    "                obj = [w for w in token.children if w.dep_ in OBJECTS]\n",
    "                if subj and obj:\n",
    "                    relations.append({\n",
    "                        \"Relation\": token.lemma_,\n",
    "                        \"Subject\": subj[0].text,\n",
    "                        \"Object\": obj[0].text,\n",
    "                        \"Description\": sent.text,\n",
    "                        # \"Relevance\": 8  # Placeholder relevance score\n",
    "                    })\n",
    "    \n",
    "    \n",
    "    return entities, relations\n",
    "    \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e27af333",
   "metadata": {},
   "outputs": [],
   "source": [
    "##export results to '02_Output/00_By-Document/' in 00_Entities and 01_Relations\n",
    "\n",
    "# Replace 'path_to_folder' with the path to your folder containing .txt files\n",
    "folder_path = '../01_Input/01_Cleaned-Text'\n",
    "\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith(\".txt\"):\n",
    "        with open(os.path.join(folder_path, file_name), 'r') as file:\n",
    "            text_content = file.read()\n",
    "\n",
    "            # Extracting entities and relations\n",
    "            entities, relations = extract_entities_relations(text_content)\n",
    "\n",
    "            # Constructing JSON data\n",
    "            json_data = {\n",
    "                \"metadata\": {\n",
    "                    # Add your metadata extraction logic here\n",
    "                },\n",
    "                \"knowledge graph\": {\n",
    "                    \"entities\": entities,\n",
    "                    \"relations\": relations\n",
    "                }\n",
    "            }\n",
    "\n",
    "            output_folder_path = '../02_Output/00_By-Document'\n",
    "\n",
    "            # Save as JSON\n",
    "            output_file_name = file_name.replace('.txt', '.json')\n",
    "            with open(os.path.join(output_folder_path, output_file_name), 'w') as output_file:\n",
    "                json.dump(json_data, output_file, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4a472a",
   "metadata": {},
   "source": [
    "<h4> merge entities and relations across all docs </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "944ea724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged data saved to ../02_Output/01_Merged/merged-knowledge-graph.json\n"
     ]
    }
   ],
   "source": [
    "#create dictionary of all entities and dictionary of relations, with values for the documents they appear in\n",
    "#then iterate through each and create merged versions\n",
    "\n",
    "##export to '02_Output/01_Merged/' as energy-entities.json and energy-entities-datecode.json (and energy-relations)\n",
    "\n",
    "# Folder path containing JSON files\n",
    "folder_path = output_folder_path\n",
    "\n",
    "# Initialize an empty dictionary to hold merged data\n",
    "merged_data = {\"metadata\": {}, \"knowledge graph\": {\"entities\": [], \"relations\": []}}\n",
    "\n",
    "# Loop through JSON files in the folder\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith(\".json\"):\n",
    "        with open(os.path.join(folder_path, file_name), 'r') as file:\n",
    "            data = json.load(file)\n",
    "            \n",
    "            # Merge metadata\n",
    "            merged_data['metadata'].update(data['metadata'])\n",
    "            \n",
    "            # Merge entities and relations\n",
    "            merged_data['knowledge graph']['entities'].extend(data['knowledge graph']['entities'])\n",
    "            merged_data['knowledge graph']['relations'].extend(data['knowledge graph']['relations'])\n",
    "\n",
    "# Save merged data into a single JSON file\n",
    "output_file = '../02_Output/01_Merged/merged-knowledge-graph.json'\n",
    "with open(output_file, 'w') as outfile:\n",
    "    json.dump(merged_data, outfile, indent=4)\n",
    "\n",
    "print(f\"Merged data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3072ea",
   "metadata": {},
   "source": [
    "<h4> extract jsons for each entity </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4800426f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##iterate through keys in entities, and identify all relations for that entity\n",
    "##generate json including all info about that entity, its relations, all related entitites, and document ids\n",
    " \n",
    "\n",
    "# Folder path containing JSON files\n",
    "folder_path = output_folder_path\n",
    "\n",
    "# Dictionary to hold entity-wise relations\n",
    "entity_relations = defaultdict(list)\n",
    "\n",
    "# Loop through JSON files in the folder\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith(\".json\"):\n",
    "        with open(os.path.join(folder_path, file_name), 'r') as file:\n",
    "            data = json.load(file)\n",
    "            \n",
    "            entities = data['knowledge graph']['entities']\n",
    "            relations = data['knowledge graph']['relations']\n",
    "            \n",
    "            for entity_info in entities:\n",
    "                entity = entity_info['entity']\n",
    "                \n",
    "                for relation in relations:\n",
    "                    if (entity in relation['Subject'] or\n",
    "                        entity in relation['Object'] or\n",
    "                        entity in relation['Description']):\n",
    "                        \n",
    "                        entity_relations[entity].append(relation)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f342b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "##export to '02_Output/02_By-Entity/'\n",
    "output_folder_path = '../02_Output/02_By-Entity'\n",
    "# Create separate JSON files for each entity's relations\n",
    "for entity, relations in entity_relations.items():\n",
    "    entity_file = f'{entity}.json'\n",
    "    entity_data = {\"metadata\": {}, \"knowledge graph\": {\"entity\": entity, \"relations\": relations}}\n",
    "    \n",
    "    with open(os.path.join(output_folder_path, entity_file), 'w') as outfile:\n",
    "        json.dump(entity_data, outfile, indent=4)\n",
    "\n",
    "print(\"Entity files created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02c718b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9718269b",
   "metadata": {},
   "source": [
    "<h4> create json exports for countries and documents </h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0c4aea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##open country and document metadata csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a702e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##use full merged knowledge graph to extract entities and relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92513de2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
